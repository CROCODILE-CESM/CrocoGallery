{"version":"1","records":[{"hierarchy":{"lvl1":"Welcome to the in-progress CrocoGallery! üêä‚ú®"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Welcome to the in-progress CrocoGallery! üêä‚ú®"},"content":"Yes, it‚Äôs still a bit rough around the edges. Think of it like a baby crocodile: adorable, slightly scary, and learning to swim.\n\nCrocoGallery is a collection of notebook tutorials, features, and use cases for CROCODILE packages, hosted via GitHub Pages with Jupyter Book.\n\nWhether you‚Äôre just dipping your toes or diving headfirst into the deep end, these guided notebooks show off what CROCODILE can do for regional ocean modeling ‚Äî and how to make it do the heavy lifting for you.\n\nStart with the tutorials on the sidebar and explore at your own pace!","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":" CUPiD Diagnostics Tutorial "},"type":"lvl1","url":"/notebooks/diagnostics/cupid-for-regional-mom6","position":0},{"hierarchy":{"lvl1":" CUPiD Diagnostics Tutorial "},"content":"\nGoal: In this tutorial, you‚Äôll learn how to make the above animation (and other diagnostic plots).\n\n","type":"content","url":"/notebooks/diagnostics/cupid-for-regional-mom6","position":1},{"hierarchy":{"lvl1":" CUPiD Diagnostics Tutorial ","lvl4":"¬† Warm up the Engine ‚Äì¬†Start a JupyterHub Server"},"type":"lvl4","url":"/notebooks/diagnostics/cupid-for-regional-mom6#warm-up-the-engine-start-a-jupyterhub-server","position":2},{"hierarchy":{"lvl1":" CUPiD Diagnostics Tutorial ","lvl4":"¬† Warm up the Engine ‚Äì¬†Start a JupyterHub Server"},"content":"If you are not already logged into \n\njupter.hub.ucar.edu , please navigate there and start a new server. Required Settings\n\nResource Selection: Casper PBS Batch\n\nQueue or Reservation: tutorial\n\nProject Account: CESM0030\n\n...\n\nSpecify Memory per Node in GB: 20Leave everything else the same!\n\nCESM3 includes the CESM Unified Postprocessing and Diagnostics (\n\nCUPiD) python framework.\nDevelopment is guided by its vision statement:\n\nCUPiD is a ‚Äúone stop shop‚Äù that enables and integrates timeseries file generation, data standardization, diagnostics, and metrics from all CESM components.\n\nThis collaborative effort aims to simplify the user experience of running diagnostics by calling post-processing tools directly from CUPiD,\nrunning all component diagnostics from the same tool as either part of the CIME workflow or independently,\nand sharing python code and a standard conda environment across components.","type":"content","url":"/notebooks/diagnostics/cupid-for-regional-mom6#warm-up-the-engine-start-a-jupyterhub-server","position":3},{"hierarchy":{"lvl1":" CUPiD Diagnostics Tutorial ","lvl2":"CUPiD for Regional MOM6"},"type":"lvl2","url":"/notebooks/diagnostics/cupid-for-regional-mom6#cupid-for-regional-mom6","position":4},{"hierarchy":{"lvl1":" CUPiD Diagnostics Tutorial ","lvl2":"CUPiD for Regional MOM6"},"content":"CUPiD is a broad package designed to pull many independent tasks together and standardize common parts of the CESM postprocessing workflow.\nFor postprocessing, CUPiD is automating the process of converting CESM output to the standard format required for the Coupled Model Intercomparison Project (\n\nCMIP).\n\nThis tutorial is going to focus on the diagnostic side of CUPiD and tools for accessing and manipulating MOM6 output.\nCUPiD is designed to make it easy for users to run existing notebooks to look at output from all components of CESM,\nbut we will focus on a set of MOM6 notebooks meant to analyze regional runs.\nParticipants will install CUPiD and then familiarize themselves with it by looking at diagnostics from an existing regional MOM6 run in stand-alone mode.\nDuring project worktime there are also instructions for\n\nrunning the same diagnostics on a user-generated regional MOM6 run (with CESM integration), and\n\ncontributing more diagnostics to the CUPiD project.","type":"content","url":"/notebooks/diagnostics/cupid-for-regional-mom6#cupid-for-regional-mom6","position":5},{"hierarchy":{"lvl1":" CUPiD Diagnostics Tutorial ","lvl2":"Your Turn!"},"type":"lvl2","url":"/notebooks/diagnostics/cupid-for-regional-mom6#your-turn","position":6},{"hierarchy":{"lvl1":" CUPiD Diagnostics Tutorial ","lvl2":"Your Turn!"},"content":"As we progress through this tutorial, think about what kinds of diagnostics might be helpful for general regions and your specific regions of interest.","type":"content","url":"/notebooks/diagnostics/cupid-for-regional-mom6#your-turn","position":7},{"hierarchy":{"lvl1":" CUPiD Diagnostics Tutorial ","lvl3":"Ideas for New Notebooks","lvl2":"Your Turn!"},"type":"lvl3","url":"/notebooks/diagnostics/cupid-for-regional-mom6#ideas-for-new-notebooks","position":8},{"hierarchy":{"lvl1":" CUPiD Diagnostics Tutorial ","lvl3":"Ideas for New Notebooks","lvl2":"Your Turn!"},"content":"This is a brief list of some possible topics/notebooks to contribute from a regional ocean perspective. Add your own!\n\nComparison to Other Model Output/Reanalysis Products: compare model output to other model output or a reanalysis product (\n\nGlobal Atmspheric Observations Example). CUPiD is already setup with this in mind with the base_case global parameters.\n\nTransport through Passages: MOM6 lets you define transects to calculate transport through different passages. These transects are defined in the diag_table so a notebook would need to chek if any transects are defined and then visualize/analyze them appropriately. Comparing to observations could be tricky and very helpful!\n\nRegion Aware Diagnostics: Different regions might benefit from a different standard sets of diagnostics. Even simple adjustments like choosing a different projection for plotting or looking at sea ice in the arctic.","type":"content","url":"/notebooks/diagnostics/cupid-for-regional-mom6#ideas-for-new-notebooks","position":9},{"hierarchy":{"lvl1":"Running CUPiD within CESM"},"type":"lvl1","url":"/notebooks/diagnostics/cupid-in-cesm-workflow","position":0},{"hierarchy":{"lvl1":"Running CUPiD within CESM"},"content":"CUPiD has been included in the CESM workflow,\nwhich means you can run CUPiD from a CESM case directory with the familiar case.submit command.\nFor this project, we will run the notebooks Aidan put together to analyze a case you ran yourself.","type":"content","url":"/notebooks/diagnostics/cupid-in-cesm-workflow","position":1},{"hierarchy":{"lvl1":"Running CUPiD within CESM","lvl2":"Background"},"type":"lvl2","url":"/notebooks/diagnostics/cupid-in-cesm-workflow#background","position":2},{"hierarchy":{"lvl1":"Running CUPiD within CESM","lvl2":"Background"},"content":"This notebook originated as a third task in the \n\nCUPiD Diagnostics Tutorial.\nIt still uses some of the same structure as the tutorial pages.","type":"content","url":"/notebooks/diagnostics/cupid-in-cesm-workflow#background","position":3},{"hierarchy":{"lvl1":"Running CUPiD within CESM","lvl3":"Step 1: Log on to Derecho and Navigate to your CESM case","lvl2":"Background"},"type":"lvl3","url":"/notebooks/diagnostics/cupid-in-cesm-workflow#step-1-log-on-to-derecho-and-navigate-to-your-cesm-case","position":4},{"hierarchy":{"lvl1":"Running CUPiD within CESM","lvl3":"Step 1: Log on to Derecho and Navigate to your CESM case","lvl2":"Background"},"content":"For the tutorial, we ran CUPiD on NCAR‚Äôs Casper machine, which is very useful for diagnostics, data analysis, and visualization.\nThe CESM case that we made in Monday‚Äôs practicum, Using CrocoDash, was set up to run on Derecho.\nCESM is picky about what machine it runs on because it tailors the configuration and run accordingly.\nEven though Derecho and Casper share a file system, they have very different hardware (and provide users with different modules).\n\nYou can either log in to Derecho directly from JupyterHub,\nor log in to Casper through the JupyterHub and then ssh to Derecho from a terminal.\n\nOnce you are on derecho, change into the directory for the case you want to analyze.\nThe path to this directory might vary, but it will likely be located under the same crocodile_2025 directory we have been working in so far.","type":"content","url":"/notebooks/diagnostics/cupid-in-cesm-workflow#step-1-log-on-to-derecho-and-navigate-to-your-cesm-case","position":5},{"hierarchy":{"lvl1":"Running CUPiD within CESM","lvl3":"Step 2: Explore how CUPiD is Incorporated in a CESM Case","lvl2":"Background"},"type":"lvl3","url":"/notebooks/diagnostics/cupid-in-cesm-workflow#step-2-explore-how-cupid-is-incorporated-in-a-cesm-case","position":6},{"hierarchy":{"lvl1":"Running CUPiD within CESM","lvl3":"Step 2: Explore how CUPiD is Incorporated in a CESM Case","lvl2":"Background"},"content":"As you are likely familiar with at this point,\nCESM uses XML files to manage the environment in which your case is run.\nEach file contains variables pertaining to a different phase of the case generation process:env_archive.xml\nenv_batch.xml\nenv_build.xml\nenv_case.xml\nenv_mach_pes.xml\nenv_mach_specific.xml\nenv_postprocessing.xml\nenv_run.xml\nenv_workflow.xml\n\nWe are interested in the CUPiD variables,\nwhich are all defined in env_postprocessing.xml.\nXML files are plain-text so we can inspect them directly,\nbut we‚Äôll use CIME‚Äôs xmlquery tool instead.\n\nüö® POP QUIZ! üö®\n\nHow do you use xmlquery to see all the variables that contain CUPID in their name (along with their values)?\n\nSolution$ ./xmlquery -p CUPID\nResults in group cupid_analysis\n\tCUPID_BASELINE_CASE: b.e23_alpha17f.BLT1850.ne30_t232.092\n\tCUPID_BASELINE_ROOT: /glade/derecho/scratch/mlevy/archive/CAcurrent.002/..\n\tCUPID_BASE_NYEARS: 100\n\tCUPID_BASE_STARTDATE: 0001-01-01\n\tCUPID_EXAMPLE: key_metrics\n\tCUPID_NYEARS: 1\n\tCUPID_STARTDATE: 0001-01-01\n\tCUPID_TS_DIR: /glade/derecho/scratch/mlevy/archive/CAcurrent.002/..\n\nResults in group cupid_config\n\tCUPID_GEN_DIAGNOSTICS: TRUE\n\tCUPID_GEN_HTML: TRUE\n\tCUPID_GEN_TIMESERIES: TRUE\n\tCUPID_ROOT: /glade/work/mlevy/codes/CESM/cesm3_0_alpha07c_CROCO/tools/CUPiD\n\nResults in group cupid_environments\n\tCUPID_ANALYSIS_ENV: cupid-analysis\n\tCUPID_INFRASTRUCTURE_ENV: cupid-infrastructure\n\nResults in group cupid_run_components\n\tCUPID_MEM_PER_TASK: 10\n\tCUPID_NTASKS: 1\n\tCUPID_RUN_ADF: FALSE\n\tCUPID_RUN_ALL: TRUE\n\tCUPID_RUN_ATM: FALSE\n\tCUPID_RUN_GLC: FALSE\n\tCUPID_RUN_ICE: FALSE\n\tCUPID_RUN_LND: FALSE\n\tCUPID_RUN_OCN: FALSE\n\tCUPID_RUN_ROF: FALSE\n\tCUPID_TASKS_PER_NODE: 128","type":"content","url":"/notebooks/diagnostics/cupid-in-cesm-workflow#step-2-explore-how-cupid-is-incorporated-in-a-cesm-case","position":7},{"hierarchy":{"lvl1":"Running CUPiD within CESM","lvl3":"Step 3: Configure and Run CUPiD","lvl2":"Background"},"type":"lvl3","url":"/notebooks/diagnostics/cupid-in-cesm-workflow#step-3-configure-and-run-cupid","position":8},{"hierarchy":{"lvl1":"Running CUPiD within CESM","lvl3":"Step 3: Configure and Run CUPiD","lvl2":"Background"},"content":"The first variable to talk about is CUPID_ROOT.\nCUPiD is distributed with CESM in the tools/CUPiD subdirectory,\nbut you may wish to use a more recent version of CUPiD.\nThis could be handy if, for example,\nyou are attending a workshop where you took a CUPiD diagnostic and the first exercises were cloning CUPiD,\ninstalling environments from this clone and then running a few examples.\nIn that case, you want to run./xmlchange CUPID_ROOT=</path/to/CUPiD>\n\nWe also want to run the regional_ocean example,\nwhich is set by the CUPID_EXAMPLE variable../xmlchange CUPID_EXAMPLE=regional_ocean\n\nLastly, we don‚Äôt need to generate time series files or build a webpage./xmlchange CUPID_GEN_TIMESERIES=FALSE,CUPID_GEN_HTML=FALSE\n\nWhen you are ready to run CUPiD, you can tell case.submit that you want to run the case.cupid job in the workflow.\nMake sure you are logged in to Derecho (not Casper) and then run:./case.submit --job case.cupid\n\nThis will add a job to the development queue on derecho,\nand it hopefully will start running quickly.\nWhen it finishes, output will be in the computed_notebooks subdirectory of your case directory.","type":"content","url":"/notebooks/diagnostics/cupid-in-cesm-workflow#step-3-configure-and-run-cupid","position":9},{"hierarchy":{"lvl1":"Running CUPiD within CESM","lvl2":"Side Quest: How Does CUPiD Tie in to CESM?"},"type":"lvl2","url":"/notebooks/diagnostics/cupid-in-cesm-workflow#side-quest-how-does-cupid-tie-in-to-cesm","position":10},{"hierarchy":{"lvl1":"Running CUPiD within CESM","lvl2":"Side Quest: How Does CUPiD Tie in to CESM?"},"content":"The details look complicated, but it‚Äôs pretty simple from the user‚Äôs perspective\n(we‚Äôre going to look at several files to understand what CESM is doing,\nbut in practice you won‚Äôt need to modify any of them if you just want to run CUPiD).\nThe CESM workflow is defined in the \n\nccs_config_cesm repository,\nand the piece relevant to CUPiD is the \n\nmachines/template.cupid file:#!/bin/bash -e\n\n# Batch system directives\n{{ batchdirectives }}\n\n# Set environment for CESM\nsource .env_mach_specific.sh\n\n# Run shell script in CUPiD external\nCUPID_ROOT=`./xmlquery --value CUPID_ROOT`\n(. ${CUPID_ROOT}/helper_scripts/cesm_postprocessing.sh)\n\nCIME parses everything in {{ }} and replaces it with machine-specific code;\nthe final file is case.cupid in your case directory.\nOn derecho, this looks like:#!/bin/bash -e\n\n# Batch system directives\n#PBS -N cupid.{CASE}\n#PBS  -r n\n#PBS  -j oe\n#PBS  -S /bin/bash\n#PBS  -l select=1:ncpus=1:mpiprocs=1:ompthreads=1:mem=10GB\n\n# Set environment for CESM\nsource .env_mach_specific.sh\n\n# Run shell script in CUPiD external\nCUPID_ROOT=`./xmlquery --value CUPID_ROOT`\n(. ${CUPID_ROOT}/helper_scripts/cesm_postprocessing.sh)\n\nIn an attempt to keep all necessary code in the CUPiD repository,\nyou‚Äôll note that the last two lines are then calling \n\nhelper_scripts/cesm_postprocessing.sh out of CUPID_ROOT.\nCUPiD‚Äôs \n\nhelper_scripts/ directory contains several scripts used to create configuration files,\nand cesm_postprocessing.sh is the driver that ties everything together.\nWe won‚Äôt walk through the entire script here,\nbut I want to highlight the comments in the script that list the processes:# Set variables that come from environment or CESM XML files\n\n# Create directory for running CUPiD\nmkdir -p cupid-postprocessing\ncd cupid-postprocessing\n\n# Use cupid-infrastructure environment for running these scripts\nconda activate ${CUPID_INFRASTRUCTURE_ENV}\n\n# 1. Generate CUPiD config file\n${CUPID_ROOT}/helper_scripts/generate_cupid_config_for_cesm_case.py\n\n# 2. Generate ADF config file\nif [ \"${CUPID_RUN_ADF}\" == \"TRUE\" ]; then\n\n# 3. Generate ILAMB config file\nif [ \"${CUPID_RUN_ILAMB}\" == \"TRUE\" ]; then\n\n# 4. Generate LDF config file\nif [ \"${CUPID_RUN_LDF}\" == \"TRUE\" ]; then\n\n# 5. Generate timeseries files\nif [ \"${CUPID_GEN_TIMESERIES}\" == \"TRUE\" ]; then\n\n# 6. Run ADF\nif [ \"${CUPID_RUN_ADF}\" == \"TRUE\" ]; then\n\n# 7. Run ILAMB\nif [ \"${CUPID_RUN_ILAMB}\" == \"TRUE\" ]; then\n\n# 8. Run LDF\nif [ \"${CUPID_RUN_LDF}\" == \"TRUE\" ]; then\n\n# 9. Run CUPiD and build webpage\nconda deactivate\nconda activate ${CUPID_INFRASTRUCTURE_ENV}\nif [ \"${CUPID_GEN_DIAGNOSTICS}\" == \"TRUE\" ]; then\n  ${CUPID_ROOT}/cupid/run_diagnostics.py ${CUPID_FLAG_STRING}\nfi\nif [ \"${CUPID_GEN_HTML}\" == \"TRUE\" ]; then\n  ${CUPID_ROOT}/cupid/generate_webpage.py\nfi\n\nYou can see how the variables defined in env_postprocessing.xml impact what parts of CUPiD are run.\nIn the next task we will make sure these XML variables are set correctly and then ask CESM to run case.cupid.","type":"content","url":"/notebooks/diagnostics/cupid-in-cesm-workflow#side-quest-how-does-cupid-tie-in-to-cesm","position":11},{"hierarchy":{"lvl1":"Installing CUPiD"},"type":"lvl1","url":"/notebooks/diagnostics/cupid-intro","position":0},{"hierarchy":{"lvl1":"Installing CUPiD"},"content":"This page follows the \n\nCUPiD Installation Documentation,\nwith some tweaks specific to the NCAR super computer.\n\n","type":"content","url":"/notebooks/diagnostics/cupid-intro","position":1},{"hierarchy":{"lvl1":"Installing CUPiD","lvl4":"Checkpoint #1"},"type":"lvl4","url":"/notebooks/diagnostics/cupid-intro#checkpoint-1","position":2},{"hierarchy":{"lvl1":"Installing CUPiD","lvl4":"Checkpoint #1"},"content":"At this point you should have a running JupyterHub instance.","type":"content","url":"/notebooks/diagnostics/cupid-intro#checkpoint-1","position":3},{"hierarchy":{"lvl1":"Installing CUPiD","lvl2":"Task 0: Open a Terminal in JupyterHub for this Activity"},"type":"lvl2","url":"/notebooks/diagnostics/cupid-intro#task-0-open-a-terminal-in-jupyterhub-for-this-activity","position":4},{"hierarchy":{"lvl1":"Installing CUPiD","lvl2":"Task 0: Open a Terminal in JupyterHub for this Activity"},"content":"","type":"content","url":"/notebooks/diagnostics/cupid-intro#task-0-open-a-terminal-in-jupyterhub-for-this-activity","position":5},{"hierarchy":{"lvl1":"Installing CUPiD","lvl3":"0.1 Navigate to your crocodile_2025 directory","lvl2":"Task 0: Open a Terminal in JupyterHub for this Activity"},"type":"lvl3","url":"/notebooks/diagnostics/cupid-intro#id-0-1-navigate-to-your-crocodile-2025-directory","position":6},{"hierarchy":{"lvl1":"Installing CUPiD","lvl3":"0.1 Navigate to your crocodile_2025 directory","lvl2":"Task 0: Open a Terminal in JupyterHub for this Activity"},"content":"We all have a directory that we have been using all week for the practical exercises.\nThe full path to this should be: /glade/work/${USER}/crocodile_2025.\nTo access this directory:\n\nClick File in the top left and select Open from Path...\n\nIn the pop-up field enter: /work/${USER}/crocodile_2025\n\nNote that JupyterHub references the filesystem relative to /glade,\nso we don‚Äôt need include that part of the path in step 2 above.","type":"content","url":"/notebooks/diagnostics/cupid-intro#id-0-1-navigate-to-your-crocodile-2025-directory","position":7},{"hierarchy":{"lvl1":"Installing CUPiD","lvl3":"0.2 Open a terminal","lvl2":"Task 0: Open a Terminal in JupyterHub for this Activity"},"type":"lvl3","url":"/notebooks/diagnostics/cupid-intro#id-0-2-open-a-terminal","position":8},{"hierarchy":{"lvl1":"Installing CUPiD","lvl3":"0.2 Open a terminal","lvl2":"Task 0: Open a Terminal in JupyterHub for this Activity"},"content":"Open a Terminal instance by scrolling all the way down in the launcher tab and selecting Terminal.\n\nThis will open a terminal with the following prompt:\nUSERNAME@crhtcXX:/glade/work/USERNAME/crocodile_2025>","type":"content","url":"/notebooks/diagnostics/cupid-intro#id-0-2-open-a-terminal","position":9},{"hierarchy":{"lvl1":"Installing CUPiD","lvl2":"Task 1: Clone CUPiD and Install Environments"},"type":"lvl2","url":"/notebooks/diagnostics/cupid-intro#task-1-clone-cupid-and-install-environments","position":10},{"hierarchy":{"lvl1":"Installing CUPiD","lvl2":"Task 1: Clone CUPiD and Install Environments"},"content":"CUPiD is available from the NCAR organization on \n\ngithub.com.\nIt requires conda to manage a few different python environments.\nThe NCAR system administrators provide conda through a module,\nwhich you can access by runningmodule load conda","type":"content","url":"/notebooks/diagnostics/cupid-intro#task-1-clone-cupid-and-install-environments","position":11},{"hierarchy":{"lvl1":"Installing CUPiD","lvl3":"1.1: Clone the repository from github","lvl2":"Task 1: Clone CUPiD and Install Environments"},"type":"lvl3","url":"/notebooks/diagnostics/cupid-intro#id-1-1-clone-the-repository-from-github","position":12},{"hierarchy":{"lvl1":"Installing CUPiD","lvl3":"1.1: Clone the repository from github","lvl2":"Task 1: Clone CUPiD and Install Environments"},"content":"Running the following command will create a subdirectory named CUPiD in your current working directory:git clone --recurse-submodules https://github.com/NCAR/CUPiD.git\n\n","type":"content","url":"/notebooks/diagnostics/cupid-intro#id-1-1-clone-the-repository-from-github","position":13},{"hierarchy":{"lvl1":"Installing CUPiD","lvl4":"Checkpoint #2","lvl3":"1.1: Clone the repository from github","lvl2":"Task 1: Clone CUPiD and Install Environments"},"type":"lvl4","url":"/notebooks/diagnostics/cupid-intro#checkpoint-2","position":14},{"hierarchy":{"lvl1":"Installing CUPiD","lvl4":"Checkpoint #2","lvl3":"1.1: Clone the repository from github","lvl2":"Task 1: Clone CUPiD and Install Environments"},"content":"At this point you should have an open terminal in JupyterHub, the current working directory should be your crocodile_2025 workshop directory, and you should have successfully cloned the CUPiD repository.\n\nThe rest of this tutorial will refer to the location you installed CUPiD as ${CUPID_ROOT}.\nTo be able to copy and paste blocks of commands directly, we need to add the variable to your environment.\n\nMost of you are using bash or zsh,\nwhich have been the default shell on Linux and Unix computers for a while now.\nTo set the environment variable runcd CUPiD\nexport CUPID_ROOT=`pwd -P`\n\nNote: This environment variable only exists in this specific terminal.\nIt will need to exported again if you open a separate terminal,\nunless you modify your start files\n(see the \n\nNCAR documentation for this process for more information).\n\nUsing a different shell?\n\nIf you have changed your shell to csh or tcsh, you will want to run the following instead:cd CUPiD\nsetenv CUPID_ROOT `pwd -P`\n\nIf you do not know for a fact that you changed your shell, you can probably assume you are using bash. To know for sure, though, runecho $SHELL","type":"content","url":"/notebooks/diagnostics/cupid-intro#checkpoint-2","position":15},{"hierarchy":{"lvl1":"Installing CUPiD","lvl3":"1.2: Install two conda environments","lvl2":"Task 1: Clone CUPiD and Install Environments"},"type":"lvl3","url":"/notebooks/diagnostics/cupid-intro#id-1-2-install-two-conda-environments","position":16},{"hierarchy":{"lvl1":"Installing CUPiD","lvl3":"1.2: Install two conda environments","lvl2":"Task 1: Clone CUPiD and Install Environments"},"content":"CUPiD needs a python environment with specific packages installed to run the CUPiD tools,\nwhile the diagnostic notebooks provided with CUPiD need a different set of packages.\n\nTo run CUPiD itself we use the (cupid-infrastructure) environment, then CUPiD runs the diagnostics notebooks with (cupid-analysis).\nWe will use mamba to install these environments and the --yes option to avoid mamba waiting for approval before continuing with the install:","type":"content","url":"/notebooks/diagnostics/cupid-intro#id-1-2-install-two-conda-environments","position":17},{"hierarchy":{"lvl1":"Installing CUPiD","lvl4":"Install (cupid-infrastructure) and (cupid-analysis)","lvl3":"1.2: Install two conda environments","lvl2":"Task 1: Clone CUPiD and Install Environments"},"type":"lvl4","url":"/notebooks/diagnostics/cupid-intro#install-cupid-infrastructure-and-cupid-analysis","position":18},{"hierarchy":{"lvl1":"Installing CUPiD","lvl4":"Install (cupid-infrastructure) and (cupid-analysis)","lvl3":"1.2: Install two conda environments","lvl2":"Task 1: Clone CUPiD and Install Environments"},"content":"cd ${CUPID_ROOT}\nmamba env create -f environments/cupid-infrastructure.yml --yes && \\\nmamba env create -f environments/cupid-analysis.yml --yes\n\nNotes:\n\nThis is two separate commands separated by &&;\nthe second command (installing (cupid-analysis)) will only run if the first command (installing (cupid-infrastructure) finishes successfully).\nAlso, the \\ is a line continuation operator -\nwithout it, the command would be on a single line and very long.\n\nIf you remove the --yes flag, mamba will ask you to confirm the installation after it determines what packages will be installed.","type":"content","url":"/notebooks/diagnostics/cupid-intro#install-cupid-infrastructure-and-cupid-analysis","position":19},{"hierarchy":{"lvl1":"Installing CUPiD","lvl2":"Basic Walkthrough of CUPiD (cupid-diagnostics script and directory structure)"},"type":"lvl2","url":"/notebooks/diagnostics/cupid-intro#basic-walkthrough-of-cupid-cupid-diagnostics-script-and-directory-structure","position":20},{"hierarchy":{"lvl1":"Installing CUPiD","lvl2":"Basic Walkthrough of CUPiD (cupid-diagnostics script and directory structure)"},"content":"While the CUPiD environments install, let‚Äôs talk about how CUPiD works.\nCUPiD can provide diagnostics for a single run,\nor compare a single run to a single baseline run.\nFuture development will enable comparison of more than two different runs,\nbut the next exercise in this tutorial will be using CUPiD to create diagnostics for a single CESM case that has already been run.\nThis will be done by running the cupid-diagnostics command from the (cupid-infrastructure) environment.(cupid-infrastructure) $ cupid-diagnostics --help\nUsage: cupid-diagnostics [OPTIONS] [CONFIG_PATH]\n\n  Main engine to set up running all the notebooks.\n\n  Args:     CONFIG_PATH: str, path to configuration file (default config.yml)\n\n  Returns:     None\n\n  Called by ``cupid-diagnostics``.\n\nOptions:\n  -s, --serial          Do not use LocalCluster objects\n  -atm, --atmosphere    Run atmosphere component diagnostics\n  -ocn, --ocean         Run ocean component diagnostics\n  -lnd, --land          Run land component diagnostics\n  -ice, --seaice        Run sea ice component diagnostics\n  -glc, --landice       Run land ice component diagnostics\n  -rof, --river-runoff  Run river runoff component diagnostics\n  -h, --help            Show this message and exit.\n\nNotice that cupid-diagnostics has a few options to allow you to run a subset of the default diagnostics,\nbut the CONFIG_PATH argument (which defaults to config.yml) is the important argument.\nCUPiD provides a set of examples (found in the examples/ directory),\neach with its own config.yml file.\n\nThe \n\nConfiguration File page of the CUPiD website goes into more detail looking at the key_metrics example,\nbut in the \n\nRunning a CUPiD Example we will look at the \n\nconfig.yml file associated with the regional_ocean example.","type":"content","url":"/notebooks/diagnostics/cupid-intro#basic-walkthrough-of-cupid-cupid-diagnostics-script-and-directory-structure","position":21},{"hierarchy":{"lvl1":"Installing CUPiD","lvl3":"Directory Structure of CUPiD","lvl2":"Basic Walkthrough of CUPiD (cupid-diagnostics script and directory structure)"},"type":"lvl3","url":"/notebooks/diagnostics/cupid-intro#directory-structure-of-cupid","position":22},{"hierarchy":{"lvl1":"Installing CUPiD","lvl3":"Directory Structure of CUPiD","lvl2":"Basic Walkthrough of CUPiD (cupid-diagnostics script and directory structure)"},"content":"Before we dive into the details of how CUPiD works,\nlet‚Äôs take a broader view of the CUPiD directory structure.\nAs you can see in the \n\nCUPiD github repository,\nCUPiD has a bunch of subdirectories.\nFor this walkthrough, we‚Äôll talk about on just four of them:","type":"content","url":"/notebooks/diagnostics/cupid-intro#directory-structure-of-cupid","position":23},{"hierarchy":{"lvl1":"Installing CUPiD","lvl4":"cupid/","lvl3":"Directory Structure of CUPiD","lvl2":"Basic Walkthrough of CUPiD (cupid-diagnostics script and directory structure)"},"type":"lvl4","url":"/notebooks/diagnostics/cupid-intro#cupid","position":24},{"hierarchy":{"lvl1":"Installing CUPiD","lvl4":"cupid/","lvl3":"Directory Structure of CUPiD","lvl2":"Basic Walkthrough of CUPiD (cupid-diagnostics script and directory structure)"},"content":"This is the guts of the CUPiD python package that is installed  in (cupid-infrastructure)\n(e.g. cupid-diagnostics runs cupid/run_diagnostics.py).\nSome of the more complex scripts are broken across multiple files:\ncupid-timeseries runs cupid/run_timeseries.py,\nwhich imports functions from cupid/timeseries.py.","type":"content","url":"/notebooks/diagnostics/cupid-intro#cupid","position":25},{"hierarchy":{"lvl1":"Installing CUPiD","lvl4":"environments/","lvl3":"Directory Structure of CUPiD","lvl2":"Basic Walkthrough of CUPiD (cupid-diagnostics script and directory structure)"},"type":"lvl4","url":"/notebooks/diagnostics/cupid-intro#environments","position":26},{"hierarchy":{"lvl1":"Installing CUPiD","lvl4":"environments/","lvl3":"Directory Structure of CUPiD","lvl2":"Basic Walkthrough of CUPiD (cupid-diagnostics script and directory structure)"},"content":"The environments/ directory contains conda environment files for three different environments.\nYou are currently installing two of them:\n\nenvironments/cupid-infrastructure.yml: builds (cupid-infrastructure),\nwhich provides the actual CUPiD functions, and\n\nenvironments/cupid-analysis.yml: builds (cupid-analysis),\nwhich provides an environment for CUPiD to use to run diagnostics notebooks.\n\nThere is also\n\nenvironments/docs.yml, which builds (cupid-docs)\n(used to create the CUPiD website).\nIt is not needed for most CUPiD users.","type":"content","url":"/notebooks/diagnostics/cupid-intro#environments","position":27},{"hierarchy":{"lvl1":"Installing CUPiD","lvl4":"examples/","lvl3":"Directory Structure of CUPiD","lvl2":"Basic Walkthrough of CUPiD (cupid-diagnostics script and directory structure)"},"type":"lvl4","url":"/notebooks/diagnostics/cupid-intro#examples","position":28},{"hierarchy":{"lvl1":"Installing CUPiD","lvl4":"examples/","lvl3":"Directory Structure of CUPiD","lvl2":"Basic Walkthrough of CUPiD (cupid-diagnostics script and directory structure)"},"content":"The examples/ directory contains subdirectories,\neach with its own config.yml file:\n\nThe directory names give a hint as to what the configuration file will provide:\n\nkey_metrics/ is the workhorse example;\nit aims to give a brief summary of a fully coupled CESM run\n(and comparisons to a baseline case).\n\nadditional_metrics/ goes into more detail than key_metrics.\nAs you might imagine, it takes longer to run.\n\nexternal_diag_packages/ pulls some key tables and figures from the output of the AMWG Diagnostic Framework (ADF),\nthe land equivalent (LDF),\nand the International Land Model Benchmarking (ILAMB) project,\nand also links to the full output of those packages.\nNote that cupid-diagnostics does not run those packages directly,\nit expects the user to do so manually.\n\nregional_ocean/ contains four notebooks that look at output from a regional MOM6 CESM case.\n\nSpoiler alert: you‚Äôll learn a lot more about regional_ocean on the next page.","type":"content","url":"/notebooks/diagnostics/cupid-intro#examples","position":29},{"hierarchy":{"lvl1":"Installing CUPiD","lvl4":"nblibrary/","lvl3":"Directory Structure of CUPiD","lvl2":"Basic Walkthrough of CUPiD (cupid-diagnostics script and directory structure)"},"type":"lvl4","url":"/notebooks/diagnostics/cupid-intro#nblibrary","position":30},{"hierarchy":{"lvl1":"Installing CUPiD","lvl4":"nblibrary/","lvl3":"Directory Structure of CUPiD","lvl2":"Basic Walkthrough of CUPiD (cupid-diagnostics script and directory structure)"},"content":"CUPiD curates a library of notebooks that can be run by cupid-diagnostics if the config.yml file requests it.\nIt contains subdirectories for each component of CESM:\n\nAnd then each subdirectory contains one or more notebooks.\nFor example, the ocn/ directory has:\n\nSpoiler alert: you‚Äôre going to run the four notebooks starting with Regional_Ocean_.\n\nIf you want to create your own diagnostic notebook\n(we‚Äôll talk about how to do that later in the afternoon),\nyou would add it to the repository in the appropriate subdirectory.","type":"content","url":"/notebooks/diagnostics/cupid-intro#nblibrary","position":31},{"hierarchy":{"lvl1":"Output from CUPiD Example"},"type":"lvl1","url":"/notebooks/diagnostics/cupid-output","position":0},{"hierarchy":{"lvl1":"Output from CUPiD Example"},"content":"After cupid-diagnostics runs successfully from ${CUPID_ROOT}/examples/regional_ocean,\nyou will have four notebooks in the computed_notebooks/ocn/ subdirectory.\nWe have included the expected output in this webpage as a reference,\nbut ideally you will be able to look at the notebooks you have run inside JupyterHub.","type":"content","url":"/notebooks/diagnostics/cupid-output","position":1},{"hierarchy":{"lvl1":"Output from CUPiD Example","lvl2":"Notebook Output"},"type":"lvl2","url":"/notebooks/diagnostics/cupid-output#notebook-output","position":2},{"hierarchy":{"lvl1":"Output from CUPiD Example","lvl2":"Notebook Output"},"content":"Regional Ocean Report Card with CUPiD\n\nRegional Ocean Animations with CUPiD\n\nRegional Ocean Atmospheric Forcing with CUPiD\n\nRegional Ocean OBC with CUPiD","type":"content","url":"/notebooks/diagnostics/cupid-output#notebook-output","position":3},{"hierarchy":{"lvl1":" Advanced CUPiD Tutorial "},"type":"lvl1","url":"/notebooks/diagnostics/cupid-projects","position":0},{"hierarchy":{"lvl1":" Advanced CUPiD Tutorial "},"content":"We walked through CUPiD basics and how to use it to analyze specific curated examples.\nIt can also be used to \n\nanalyze CESM runs directly from the case directory,\nand you can create your own notebooks to run via CUPiD.\nIf you do that, you can also \n\nshare those notebooks with the community.","type":"content","url":"/notebooks/diagnostics/cupid-projects","position":1},{"hierarchy":{"lvl1":"Contributing to CUPiD"},"type":"lvl1","url":"/notebooks/diagnostics/contributing-to-cupid","position":0},{"hierarchy":{"lvl1":"Contributing to CUPiD"},"content":"CUPiD is a tool for the CESM community, driven by community. If you have an idea for diagnostics, use cases, or new features, your contribution will be incredibly valuable!\n\nThe CUPiD docs have resources for developing new diagnostics. Please consult these if you would like to create new notebooks or examples:\n\nCUPiD Contributors Guide\n\nHow to Add New Notebooks to CUPiD\n\nThere are a few techinical steps in the contributor‚Äôs guide (see link above) that we will not discuss here; let us know if you run into any issues with pre-commit or GitHub.\nNew diagnostics notebooks can become part of an existing diagnostics workflow (see \n\nCUPiD/examples) or you can create your own example with a new config file.","type":"content","url":"/notebooks/diagnostics/contributing-to-cupid","position":1},{"hierarchy":{"lvl1":"Contributing to CUPiD","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/diagnostics/contributing-to-cupid#overview","position":2},{"hierarchy":{"lvl1":"Contributing to CUPiD","lvl2":"Overview"},"content":"The structure of CUPiD can be confusing, but contributing is easy! Here are the basic steps of adding notebooks:\n\nHave an idea for diagnostics or analysis of a component of CESM output\n\nDesign a Jupyter notebook that operates on CESM history (one time step, multiple variables) or timeseries (one variable, multiple time steps) files\n\nAdd a cell for parameters that CUPiD will inject (see \n\nhow to add new notebooks). These are flexible, but variables can be tricky to add within the CUPiD workflow so be mindful.\n\nCreate/modify a config.yml file to populate and run your new notebook(s)!","type":"content","url":"/notebooks/diagnostics/contributing-to-cupid#overview","position":3},{"hierarchy":{"lvl1":"Contributing to CUPiD","lvl2":"Making a (Sub)-Project for the 2025 CrocoDash Workshop"},"type":"lvl2","url":"/notebooks/diagnostics/contributing-to-cupid#making-a-sub-project-for-the-2025-crocodash-workshop","position":4},{"hierarchy":{"lvl1":"Contributing to CUPiD","lvl2":"Making a (Sub)-Project for the 2025 CrocoDash Workshop"},"content":"If you want to work with CUPiD and diagnostics as part of your 2025 Workshop project, let us know! You‚Äôll be making so many cool regional models, and you might need new cool metrics and visualizations to look at them! Here are a couple resources that might help get you started on ideas:\n\nMOM6-Tools - this python package is designed and maintained by Gustavo Marques at NCAR (extra points if you talk to him this week)! It primarily hosts tools for global ocean models; maybe they could also work for regional models?\n\nMOM6 Analysis Cookbook - from COSIMA, our collaborators in Australia! This notebook provides some more specialized tools/techniques for working with MOM6 data like xgcm and budgets.\n\nEKE Calculation - from Giovanni Seijo-Ellis (look for him at the workshop!). This notebook calculates Eddy Kinetic Energy, specifically for a region in the Caribbean. How could we generalize this? What other metrics could we calculate?","type":"content","url":"/notebooks/diagnostics/contributing-to-cupid#making-a-sub-project-for-the-2025-crocodash-workshop","position":5},{"hierarchy":{"lvl1":"Contributing to CUPiD","lvl2":"CUPiD Structure"},"type":"lvl2","url":"/notebooks/diagnostics/contributing-to-cupid#cupid-structure","position":6},{"hierarchy":{"lvl1":"Contributing to CUPiD","lvl2":"CUPiD Structure"},"content":"nblibrary: where new notebooks will go. They are categorized by component (e.g. ocean - ocn and atmosphere - atm). Notebooks are run here by CUPiD then copied to the path from run_dir in the config file.\n\nexamples: config files for different workflows. The config files determine which notebooks are run and what parameters are passed in.\n\nhelper_scripts: infrastructure that translates CESM variables/settings to CUPiD config information and parameters. You will need to edit this if your workflow/notebook is designed to run immediately after a CESM run.","type":"content","url":"/notebooks/diagnostics/contributing-to-cupid#cupid-structure","position":7},{"hierarchy":{"lvl1":"Contributing to CUPiD","lvl2":"Advice for Contributors"},"type":"lvl2","url":"/notebooks/diagnostics/contributing-to-cupid#advice-for-contributors","position":8},{"hierarchy":{"lvl1":"Contributing to CUPiD","lvl2":"Advice for Contributors"},"content":"For CrocoDash, we created a new example, regional_ocean_report_card, with a new set of notebooks for regional ocean diagnostics; based on this experience these are some guidelines and notes for development.","type":"content","url":"/notebooks/diagnostics/contributing-to-cupid#advice-for-contributors","position":9},{"hierarchy":{"lvl1":"Contributing to CUPiD","lvl3":"Miscellanous Notes:","lvl2":"Advice for Contributors"},"type":"lvl3","url":"/notebooks/diagnostics/contributing-to-cupid#miscellanous-notes","position":10},{"hierarchy":{"lvl1":"Contributing to CUPiD","lvl3":"Miscellanous Notes:","lvl2":"Advice for Contributors"},"content":"Try to run all notebooks with the cupid-analysis conda env. The conda environment can be changed in the config file, but a universal environment is powerful.\n\nIf there are functions common across multiple notebooks, consider adding a utils.py in the same directory as the notebook, this can house light and reusable functions.\n\nBeware! CUPiD works by executing the specified notebooks in the nblibrary folder and then copying them to the output directory. Additional files and folders (like utils.py) are not copied over (this might change in future versions). This means users would need to manually copy over any dependencies if they want to rerun the notebook.\n\nWhen saving images or other output, consider adding a an output path parameter in the notebook or you can cheat and save them in the timeseries or CESM output directory.\n\nThink about how flexible your notebook is between different timescales, regions, and data outputs.\n\nIf something is missing in the CUPiD workflow, make an issue or a pull request!","type":"content","url":"/notebooks/diagnostics/contributing-to-cupid#miscellanous-notes","position":11},{"hierarchy":{"lvl1":"Contributing to CUPiD","lvl3":"Adding Notebook Specific Parameters","lvl2":"Advice for Contributors"},"type":"lvl3","url":"/notebooks/diagnostics/contributing-to-cupid#adding-notebook-specific-parameters","position":12},{"hierarchy":{"lvl1":"Contributing to CUPiD","lvl3":"Adding Notebook Specific Parameters","lvl2":"Advice for Contributors"},"content":"You can add as many notebook specific parameters as you want, but they can be difficult to populate automatically in the CESM workflow.\n\nWhen you add custom variables that will depend on the specific CESM case (e.g. location of data or region specific information),\nyou will also need to update cesm_postprocessing.sh and generate_cupid_config_for_cesm_case.py scripts in CUPiD/helper_scripts.\nThese handle the translation of information from the CESM case to CUPiD config settings.\n\nExample:\n\nFor the open boundary conditions notebook Regional_Ocean_OBC.ipynb, the boundary conditions are only accessible through the MOM6 input directory.\nThe input directory is not available through the CESM output directory, but the path to the input directory is accessible through the CESM case directory.\n\nWe added the path to the case directory as a parameter in the notebook. Then we needed to add the following to generate_cupid_config_for_cesm_case.py:if \"Regional_Ocean_OBC\" in my_dict[\"compute_notebooks\"].get(\"ocn\", {}):\n      my_dict[\"compute_notebooks\"][\"ocn\"][\"Regional_Ocean_OBC\"][\"parameter_groups\"][\"none\"][\"case_root\"] = case_root","type":"content","url":"/notebooks/diagnostics/contributing-to-cupid#adding-notebook-specific-parameters","position":13},{"hierarchy":{"lvl1":"Regional Ocean: Animations of Surface Fields"},"type":"lvl1","url":"/notebooks/diagnostics/cupid-output/regional-ocean-animations","position":0},{"hierarchy":{"lvl1":"Regional Ocean: Animations of Surface Fields"},"content":"","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-animations","position":1},{"hierarchy":{"lvl1":"Regional Ocean: Animations of Surface Fields","lvl2":"Regional Ocean: Animations of Surface Fields"},"type":"lvl2","url":"/notebooks/diagnostics/cupid-output/regional-ocean-animations#regional-ocean-animations-of-surface-fields","position":2},{"hierarchy":{"lvl1":"Regional Ocean: Animations of Surface Fields","lvl2":"Regional Ocean: Animations of Surface Fields"},"content":"\n\n%load_ext autoreload\n%autoreload 2\nimport xarray as xr\nimport os\nfrom IPython.display import HTML\n\nimport regional_utils as utils\n\n\n\ncase_name = \"\"  # \"/glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/Diagnostics/CESM_Output/\"\nCESM_output_dir = \"\"  # \"CROCODILE_tutorial_nwa12_MARBL\"\n\n# As regional domains vary so much in purpose, simulation length, and extent, we don't want to assume a minimum duration\n## Thus, we ignore start and end dates and simply reduce/output over the whole time frame for all of the examples given.\nstart_date = None  # \"0001-01-01\"\nend_date = None  # \"0101-01-01\n\nsave_figs = False\nfig_output_dir = None\n\nlc_kwargs = {}\nserial = False\n\nsfc_variables = []  # ['SSH', 'tos', 'sos']\nmax_frames = None  # 60\n\n\n\n# Parameters\ncase_name = \"CROCODILE_tutorial_nwa12_MARBL\"\nCESM_output_dir = (\n    \"/glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/Diagnostics/CESM_Output/\"\n)\nstart_date = \"\"\nend_date = \"\"\nsave_figs = True\nfig_output_dir = None\nlc_kwargs = {\"threads_per_worker\": 1}\nserial = True\nsfc_variables = [\"SSH\", \"tos\", \"sos\"]\nmax_frames = 60\nsubset_kwargs = {}\nproduct = \"/glade/work/ajanney/crocodile_2025/CUPiD/examples/regional_ocean/computed_notebooks//ocn/Regional_Ocean_Animations.ipynb\"\n\n\n\n\nOUTDIR = f\"{CESM_output_dir}/{case_name}/ocn/hist/\"\nprint(\"Output directory is:\", OUTDIR)\n\n\n\n","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-animations#regional-ocean-animations-of-surface-fields","position":3},{"hierarchy":{"lvl1":"Regional Ocean: Animations of Surface Fields","lvl2":"Open sfc_datas and Define Paths"},"type":"lvl2","url":"/notebooks/diagnostics/cupid-output/regional-ocean-animations#open-sfc-datas-and-define-paths","position":4},{"hierarchy":{"lvl1":"Regional Ocean: Animations of Surface Fields","lvl2":"Open sfc_datas and Define Paths"},"content":"\n\ncase_output_dir = os.path.join(CESM_output_dir, case_name, \"ocn\", \"hist\")\n\n# Xarray time decoding things\ntime_coder = xr.coders.CFDatetimeCoder(use_cftime=True)\n\n## Static data includes hgrid, vgrid, bathymetry, land/sea mask\nstatic_data = xr.open_mfdataset(\n    os.path.join(case_output_dir, f\"*static.nc\"),\n    decode_timedelta=True,\n    decode_times=time_coder,\n)\n\n## Surface Data\nsfc_data = xr.open_mfdataset(\n    os.path.join(case_output_dir, f\"*sfc*.nc\"),\n    decode_timedelta=True,\n    decode_times=time_coder,\n)\n\n## Monthly Domain Data\n# monthly_data = xr.open_mfdataset(\n#     os.path.join(case_output_dir, f\"*z*.nc\"),\n#     decode_timedelta=True,\n#     decode_times=time_coder,\n# )\n\n## Image/Gif Output Directory\nif fig_output_dir is None:\n    image_output_dir = os.path.join(\n        \"/glade/derecho/scratch/\",\n        os.environ[\"USER\"],\n        \"archive\",\n        case_name,\n        \"ocn\",\n        \"cupid_images\",\n    )\nelse:\n    image_output_dir = os.path.join(fig_output_dir, case_name, \"ocn\", \"cupid_images\")\nif not os.path.exists(image_output_dir):\n    os.makedirs(image_output_dir)\nprint(\"Image output directory is:\", image_output_dir)\n\n\n\n## Select for only the variables we want to analyze\nif len(sfc_variables) > 0:\n    print(\"Selecting only the following surface variables:\", sfc_variables)\n    sfc_data = sfc_data[sfc_variables]\n\n## Apply time boundaries\n## if they are the right format\nif len(start_date.split(\"-\")) == 3 and len(end_date.split(\"-\")) == 3:\n    import cftime\n\n    calendar = sfc_data.time.encoding.get(\"calendar\", \"standard\")\n\n    calendar_map = {\n        \"gregorian\": cftime.DatetimeProlepticGregorian,\n        \"noleap\": cftime.DatetimeNoLeap,\n    }\n\n    CFTime = calendar_map.get(calendar, cftime.DatetimeGregorian)\n    y, m, d = [int(i) for i in start_date.split(\"-\")]\n    start_date_time = CFTime(y, m, d)\n    y, m, d = [int(i) for i in end_date.split(\"-\")]\n    end_date_time = CFTime(y, m, d)\n\n    print(\n        f\"Applying time range from start_date: {start_date_time} and end_date: {end_date_time}.\"\n    )\n\n    sfc_data = sfc_data.sel(time=slice(start_date_time, end_date_time))\n\nsfc_time_bounds = [\n    sfc_data[\"time\"].isel(time=0).compute().item(),\n    sfc_data[\"time\"].isel(time=-1).compute().item(),\n]\n\nprint(f\"Surface Data Time Bounds: {sfc_time_bounds[0]} to {sfc_time_bounds[-1]}\")\n\n\n\n","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-animations#open-sfc-datas-and-define-paths","position":5},{"hierarchy":{"lvl1":"The GIF!"},"type":"lvl1","url":"/notebooks/diagnostics/cupid-output/regional-ocean-animations#the-gif","position":6},{"hierarchy":{"lvl1":"The GIF!"},"content":"\n\nif sfc_data[\"time\"].size > max_frames:\n    sfc_data = sfc_data.isel(time=slice(0, max_frames))\n\nfor gif_variable in sfc_variables:\n\n    field = sfc_data[gif_variable]\n\n    coords = utils.chooseGeoCoords(field.dims)\n    areacello = utils.chooseAreacello(field.dims)\n\n    anim = utils.create2DFieldAnimation(\n        field,\n        latitude=static_data[coords[\"latitude\"]],\n        longitude=static_data[coords[\"longitude\"]],\n        iter_dim=\"time\",\n        interval=150,\n        save=save_figs,\n        save_path=image_output_dir,\n    )\n\n\n\nfrom matplotlib import rcParams\n\nrcParams[\"animation.embed_limit\"] = (\n    200 * 1024 * 1024\n)  # constrains max size of HTML to be displayed in notebook\n\nHTML(anim.to_jshtml())\n\n\n\n","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-animations#the-gif","position":7},{"hierarchy":{"lvl1":"The GIF!","lvl2":"Not rendered here!"},"type":"lvl2","url":"/notebooks/diagnostics/cupid-output/regional-ocean-animations#not-rendered-here","position":8},{"hierarchy":{"lvl1":"The GIF!","lvl2":"Not rendered here!"},"content":"","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-animations#not-rendered-here","position":9},{"hierarchy":{"lvl1":"Regional Ocean: Atmospheric Forcing"},"type":"lvl1","url":"/notebooks/diagnostics/cupid-output/regional-ocean-atmospheric-forcing","position":0},{"hierarchy":{"lvl1":"Regional Ocean: Atmospheric Forcing"},"content":"","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-atmospheric-forcing","position":1},{"hierarchy":{"lvl1":"Regional Ocean: Atmospheric Forcing"},"type":"lvl1","url":"/notebooks/diagnostics/cupid-output/regional-ocean-atmospheric-forcing#regional-ocean-atmospheric-forcing","position":2},{"hierarchy":{"lvl1":"Regional Ocean: Atmospheric Forcing"},"content":"\n\n%load_ext autoreload\n%autoreload 2\n\nimport xarray as xr\nimport os\nimport cartopy.crs as ccrs\nimport cartopy\nimport matplotlib.pyplot as plt\n\nimport regional_utils as utils\n\n\n\ncase_name = \"\"  # \"/glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/Diagnostics/CESM_Output/\"\nCESM_output_dir = \"\"  # \"CROCODILE_tutorial_nwa12_MARBL\"\n\n# As regional domains vary so much in purpose, simulation length, and extent, we don't want to assume a minimum duration\n## Thus, we ignore start and end dates and simply reduce/output over the whole time frame for all of the examples given.\nstart_date = None  # \"0001-01-01\"\nend_date = None  # \"0101-01-01\n\nsave_figs = False\nfig_output_dir = None\n\nlc_kwargs = {}\nserial = False\n\natm_variables = []  # ['tauuo', 'tauvo', 'hfds']\nocn_variables = []  # ['uo', 'vo', 'thetao']\n\n\n\n# Parameters\ncase_name = \"CROCODILE_tutorial_nwa12_MARBL\"\nCESM_output_dir = (\n    \"/glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/Diagnostics/CESM_Output/\"\n)\nstart_date = \"\"\nend_date = \"\"\nsave_figs = True\nfig_output_dir = None\nlc_kwargs = {\"threads_per_worker\": 1}\nserial = True\natm_variables = [\"tauuo\", \"tauvo\", \"hfds\"]\nocn_variables = [\"uo\", \"vo\", \"thetao\"]\nsubset_kwargs = {}\nproduct = \"/glade/work/ajanney/crocodile_2025/CUPiD/examples/regional_ocean/computed_notebooks//ocn/Regional_Ocean_Atmospheric_Forcing.ipynb\"\n\n\n\n\nOUTDIR = f\"{CESM_output_dir}/{case_name}/ocn/hist/\"\nprint(\"Output directory is:\", OUTDIR)\n\n\n\ncase_output_dir = os.path.join(CESM_output_dir, case_name, \"ocn\", \"hist\")\n\n# Xarray time decoding things\ntime_coder = xr.coders.CFDatetimeCoder(use_cftime=True)\n\n## Static data includes hgrid, vgrid, bathymetry, land/sea mask\nstatic_data = xr.open_mfdataset(\n    os.path.join(case_output_dir, f\"*static.nc\"),\n    decode_timedelta=True,\n    decode_times=time_coder,\n    engine=\"netcdf4\",\n)\n\n# ## Surface Data\n# sfc_data = xr.open_mfdataset(\n#     os.path.join(case_output_dir, f\"*sfc*.nc\"),\n#     decode_timedelta=True,\n#     decode_times=time_coder,\n#     engine=\"netcdf4\",\n# )\n\n## Native Monthly Domain Data (and atm forcing)\nmonthly_data = xr.open_mfdataset(\n    os.path.join(case_output_dir, f\"*h.z*.nc\"),\n    decode_timedelta=True,\n    decode_times=time_coder,\n    engine=\"netcdf4\",\n)\n\n## Native Monthly Domain Data (and atm forcing)\nnative_data = xr.open_mfdataset(\n    os.path.join(case_output_dir, f\"*native*.nc\"),\n    decode_timedelta=True,\n    decode_times=time_coder,\n    engine=\"netcdf4\",\n)\n\n## Image/Gif Output Directory\nif fig_output_dir is None:\n    image_output_dir = os.path.join(\n        \"/glade/derecho/scratch/\",\n        os.environ[\"USER\"],\n        \"archive\",\n        case_name,\n        \"ocn\",\n        \"cupid_images\",\n    )\nelse:\n    image_output_dir = os.path.join(fig_output_dir, case_name, \"ocn\", \"cupid_images\")\nif not os.path.exists(image_output_dir):\n    os.makedirs(image_output_dir)\nprint(\"Image output directory is:\", image_output_dir)\n\n\n\n## Select for only the variables we want to analyze\nif len(atm_variables) > 0:\n    print(\"Selecting only the following surface variables:\", atm_variables)\n    native_data = native_data[atm_variables]\nif len(ocn_variables) > 0:\n    print(\"Selecting only the following monthly variables:\", ocn_variables)\n    monthly_data = monthly_data[ocn_variables]\n\n## Apply time boundaries\n## if they are the right format\nif len(start_date.split(\"-\")) == 3 and len(end_date.split(\"-\")) == 3:\n    import cftime\n\n    calendar = monthly_data.time.encoding.get(\"calendar\", \"standard\")\n\n    calendar_map = {\n        \"gregorian\": cftime.DatetimeProlepticGregorian,\n        \"noleap\": cftime.DatetimeNoLeap,\n    }\n\n    CFTime = calendar_map.get(calendar, cftime.DatetimeGregorian)\n    y, m, d = [int(i) for i in start_date.split(\"-\")]\n    start_date_time = CFTime(y, m, d)\n    y, m, d = [int(i) for i in end_date.split(\"-\")]\n    end_date_time = CFTime(y, m, d)\n\n    print(\n        f\"Applying time range from start_date: {start_date_time} and end_date: {end_date_time}.\"\n    )\n\n    monthly_data = monthly_data.sel(time=slice(start_date_time, end_date_time))\n    native_data = native_data.sel(time=slice(start_date_time, end_date_time))\n\nnative_time_bounds = [\n    native_data[\"time\"].isel(time=0).compute().item(),\n    native_data[\"time\"].isel(time=-1).compute().item(),\n]\nmonthly_time_bounds = [\n    monthly_data[\"time\"].isel(time=0).compute().item(),\n    monthly_data[\"time\"].isel(time=-1).compute().item(),\n]\n\nprint(f\"Surface Data Time Bounds: {native_time_bounds[0]} to {native_time_bounds[-1]}\")\nprint(\n    f\"Monthly Data Time Bounds: {monthly_time_bounds[0]} to {monthly_time_bounds[-1]}\"\n)\n\n\n\n","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-atmospheric-forcing#regional-ocean-atmospheric-forcing","position":3},{"hierarchy":{"lvl1":"Regional Ocean: Atmospheric Forcing","lvl2":"Simple Visualization of Atmospheric Forcing vs Ocean Variables"},"type":"lvl2","url":"/notebooks/diagnostics/cupid-output/regional-ocean-atmospheric-forcing#simple-visualization-of-atmospheric-forcing-vs-ocean-variables","position":4},{"hierarchy":{"lvl1":"Regional Ocean: Atmospheric Forcing","lvl2":"Simple Visualization of Atmospheric Forcing vs Ocean Variables"},"content":"This notebook is short and sweet, designed to just show how we might access the atmospheric forcing variables. Pay particular attention to the meta data for the different variables!\n\nNote: Native output can be weird, be wary and ask questions!\n\nfor coord in list(static_data.variables):\n    if \"geolon\" in coord or \"geolat\" in coord:\n        native_data = native_data.assign_coords({coord: static_data[coord]})\n        monthly_data = monthly_data.assign_coords({coord: static_data[coord]})\n\nfor i, var in enumerate(atm_variables):\n    field = native_data[var]\n    sfc_var = ocn_variables[i]  # assuming order matches, it may not\n    sfc_field = monthly_data[sfc_var].isel(z_l=0)\n\n    geocoords = utils.chooseGeoCoords(field.dims)\n    lon = geocoords[\"longitude\"]\n    lat = geocoords[\"latitude\"]\n\n    cmap = utils.chooseColorMap(var)\n\n    transform = ccrs.PlateCarree()\n    subplot_kwargs = {\n        \"projection\": ccrs.Mercator(),\n    }\n\n    map_extent = [\n        float(field[lon].min()),\n        float(field[lon].max()),\n        float(field[lat].min()),\n        float(field[lat].max()),\n    ]\n\n    # Plot atmospheric variable for each month\n    g1 = field.plot(\n        x=lon,\n        y=lat,\n        col=\"time\",\n        col_wrap=4,\n        robust=True,\n        cmap=cmap,\n        transform=transform,\n        subplot_kws=subplot_kwargs,\n    )\n    plt.suptitle(f\"Atmosphere: {var} monthly fields\", fontsize=16, y=1.02)\n\n    for ax in g1.axs.flat:\n        ax.set_extent(map_extent, crs=ccrs.PlateCarree())\n        ax.coastlines(resolution=\"50m\", color=\"black\", linewidth=0.3)\n\n    if save_figs:\n        plt.savefig(os.path.join(image_output_dir, f\"{var}_monthly_grid.png\"))\n    plt.show()\n\n    # Plot ocean surface variable for each month\n    g2 = sfc_field.plot(\n        x=lon,\n        y=lat,\n        col=\"time\",\n        col_wrap=4,\n        robust=True,\n        cmap=cmap,\n        transform=transform,\n        subplot_kws=subplot_kwargs,\n    )\n    plt.suptitle(f\"Ocean: {sfc_var} monthly fields\", fontsize=16, y=1.02)\n\n    for ax in g2.axs.flat:\n        ax.set_extent(map_extent, crs=ccrs.PlateCarree())\n        ax.coastlines(resolution=\"50m\", color=\"black\", linewidth=0.3)\n\n    if save_figs:\n        plt.savefig(os.path.join(image_output_dir, f\"{sfc_var}_monthly_grid.png\"))\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-atmospheric-forcing#simple-visualization-of-atmospheric-forcing-vs-ocean-variables","position":5},{"hierarchy":{"lvl1":"Regional Ocean: Checking Open Boundary Conditions"},"type":"lvl1","url":"/notebooks/diagnostics/cupid-output/regional-ocean-obc","position":0},{"hierarchy":{"lvl1":"Regional Ocean: Checking Open Boundary Conditions"},"content":"An Exception was encountered at ‚Äò \n\nIn [7] ‚Äô.\n\n","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-obc","position":1},{"hierarchy":{"lvl1":"Regional Ocean: Checking Open Boundary Conditions"},"type":"lvl1","url":"/notebooks/diagnostics/cupid-output/regional-ocean-obc#regional-ocean-checking-open-boundary-conditions","position":2},{"hierarchy":{"lvl1":"Regional Ocean: Checking Open Boundary Conditions"},"content":"\n\nimport xarray as xr\nimport numpy as np\nimport os\nfrom pathlib import Path\nimport glob\nimport regional_utils as utils\n\n\n\ncase_name = \"\"  # \"/glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/Diagnostics/CESM_Output/\"\nCESM_output_dir = \"\"  # \"CROCODILE_tutorial_nwa12_MARBL\"\n\n# As regional domains vary so much in purpose, simulation length, and extent, we don't want to assume a minimum duration\n## Thus, we ignore start and end dates and simply reduce/output over the whole time frame for all of the examples given.\nstart_date = None  # \"0001-01-01\"\nend_date = None  # \"0101-01-01\n\nsave_figs = False\nfig_output_dir = None\n\nlc_kwargs = {}\nserial = False\n\n## Notebook Specific Arguments\n# case_root is automatically populated in a CESM CUPiD workflow\n# If case_root is specified, and mom6_input_dir is not, INPUTDIR will be inferred from user_nl_mom in case_root\n# If mom6_input_dir is specified, case_root will be ignored\n# NOTE: If case_root and mom6_input_dir are both None, the notebook will not run\n# If obc_file_str is unspecified, defaults to \"forcing_obc_segment_00*.nc\"\ncase_root = None\nmom6_input_dir = None  # \"/glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/Diagnostics/Input_Dir/CROCODILE_tutorial_nwa12_MARBL_ocnice\"\nobc_file_str = None  # Pattern for OBC file names: \"forcing_obc_segment_00*.nc\"\n\n\n\n# Parameters\ncase_name = \"CROCODILE_tutorial_nwa12_MARBL\"\nCESM_output_dir = (\n    \"/glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/Diagnostics/CESM_Output/\"\n)\nstart_date = \"\"\nend_date = \"\"\nsave_figs = True\nfig_output_dir = None\nlc_kwargs = {\"threads_per_worker\": 1}\nserial = True\ncase_root = None\nmom6_input_dir = \"/glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/Diagnostics/Input_Dir/CROCODILE_tutorial_nwa12_MARBL_ocnice\"\nobc_file_str = \"forcing_obc_segment_00*.nc\"\nsubset_kwargs = {}\nproduct = \"/glade/work/ajanney/crocodile_2025/CUPiD/examples/regional_ocean/computed_notebooks//ocn/Regional_Ocean_OBC.ipynb\"\n\n\n\n\nOUTDIR = f\"{CESM_output_dir}/{case_name}/ocn/hist/\"\nprint(\"Output directory is:\", OUTDIR)\n\n\n\ncase_output_dir = os.path.join(CESM_output_dir, case_name, \"ocn\", \"hist\")\n\n# Xarray time decoding things\ntime_coder = xr.coders.CFDatetimeCoder(use_cftime=True)\n\n## Static data includes hgrid, vgrid, bathymetry, land/sea mask\nstatic_data = xr.open_mfdataset(\n    os.path.join(case_output_dir, f\"*static.nc\"),\n    decode_timedelta=True,\n    decode_times=time_coder,\n)\n\n## Surface Data\nsfc_data = xr.open_mfdataset(\n    os.path.join(case_output_dir, f\"*sfc*.nc\"),\n    decode_timedelta=True,\n    decode_times=time_coder,\n)\n\n# ## Monthly Domain Data\n# monthly_data = xr.open_mfdataset(\n#     os.path.join(case_output_dir, f\"*z*.nc\"),\n#     decode_timedelta=True,\n#     decode_times=time_coder,\n# )\n\n# ## Native Monthly Domain Data\n# native_data = xr.open_mfdataset(\n#     os.path.join(case_output_dir, f\"*native*.nc\"),\n#     decode_timedelta=True,\n#     decode_times=time_coder,\n# )\n\n## Image/Gif Output Directory\nif fig_output_dir is None:\n    image_output_dir = os.path.join(\n        \"/glade/derecho/scratch/\",\n        os.environ[\"USER\"],\n        \"archive\",\n        case_name,\n        \"ocn\",\n        \"cupid_images\",\n    )\nelse:\n    image_output_dir = os.path.join(fig_output_dir, case_name, \"ocn\", \"cupid_images\")\nif not os.path.exists(image_output_dir):\n    os.makedirs(image_output_dir)\nprint(\"Image output directory is:\", image_output_dir)\n\n\n\n## Apply time boundaries\nif len(start_date) > 0 and len(end_date) > 0:\n    import cftime\n\n    calendar = sfc_data.time.encoding.get(\"calendar\", \"standard\")\n\n    calendar_map = {\n        \"gregorian\": cftime.DatetimeProlepticGregorian,\n        \"noleap\": cftime.DatetimeNoLeap,\n    }\n\n    CFTime = calendar_map.get(calendar, cftime.DatetimeGregorian)\n    y, m, d = [int(i) for i in start_date.split(\"-\")]\n    start_date_time = CFTime(y, m, d)\n    y, m, d = [int(i) for i in end_date.split(\"-\")]\n    end_date_time = CFTime(y, m, d)\n\n    sfc_data = sfc_data.sel(time=slice(start_date_time, end_date_time))\n\n\n\n","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-obc#regional-ocean-checking-open-boundary-conditions","position":3},{"hierarchy":{"lvl1":"Accessing and Processing Open Boundary Conditions"},"type":"lvl1","url":"/notebooks/diagnostics/cupid-output/regional-ocean-obc#accessing-and-processing-open-boundary-conditions","position":4},{"hierarchy":{"lvl1":"Accessing and Processing Open Boundary Conditions"},"content":"\n\n","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-obc#accessing-and-processing-open-boundary-conditions","position":5},{"hierarchy":{"lvl1":"Accessing and Processing Open Boundary Conditions","lvl2":"Regional MOM6 Input Directory"},"type":"lvl2","url":"/notebooks/diagnostics/cupid-output/regional-ocean-obc#regional-mom6-input-directory","position":6},{"hierarchy":{"lvl1":"Accessing and Processing Open Boundary Conditions","lvl2":"Regional MOM6 Input Directory"},"content":"Regional MOM6 configurations use a separate input directory containing the data for the initial state, tides, and boundary conditions. This notebook needs to access the boundary conditions. There are a couple options here:\n\nWe instead look for the input directory path in two locations in the case_root directory:\n\nuser_nl_mom\n\nBuildconf/momconf/MOM_input\n\nIn both files, the input directory path should have the name INPUTDIR. If this path cannot be found, the rest of this notebook will fail to run, but the user can manually set the input directory path below.\n\nExecution using papermill encountered an exception here and stopped:\n\ninput_dir = None\n\nfor file in [\"user_nl_mom\", \"BuildConf/momconf/MOM_input\"]:\n    filepath = Path(os.path.join(case_root, file))\n\n    if filepath.is_file():\n        with open(filepath, \"r\") as f:\n            for line in f:\n                if line.strip().startswith(\"INPUTDIR\"):\n                    key, value = line.split(\"=\", 1)\n\n                    input_dir = value.strip()\n                    break\n\n    if Path(input_dir).is_dir():\n        print(f\"Found INPUTDIR in {file}\")\n        break\n\n# Override if autmatic method doesn't work:\n# input_dir = user/specified/path\n\nif input_dir is None:\n    print(\n        \"INPUTDIR not found. Try Setting it manually in the `Regional_Ocean_OBC` notebook.\"\n    )\n    raise FileNotFoundError(\n        \"INPUTDIR not found. Try Setting it manually in the `Regional_Ocean_OBC` notebook.\"\n    )\n\nprint(f\"INPUTDIR: {input_dir}\")\n\n\n\n","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-obc#regional-mom6-input-directory","position":7},{"hierarchy":{"lvl1":"Accessing and Processing Open Boundary Conditions","lvl2":"Check for Boundary Conditions, Verify Horizontal Grid"},"type":"lvl2","url":"/notebooks/diagnostics/cupid-output/regional-ocean-obc#check-for-boundary-conditions-verify-horizontal-grid","position":8},{"hierarchy":{"lvl1":"Accessing and Processing Open Boundary Conditions","lvl2":"Check for Boundary Conditions, Verify Horizontal Grid"},"content":"\n\nThe input directory should contain a few different files:\n\ninit*: initial fields\n\nocean_hgrid.nc, ocean_topo.nc, vgrid*.nc: grids and bathymetry\n\ntu* and tz*: tidal forcing\n\nWhat we care about:\n\nforcing_obc_segment*.nc: open boundary conditions to force the model\n\nThe numbers at the end of the file each correspond to a different edge of the domain.\n\n001: south\n\n002: north\n\n003: west\n\n004: east\n\nboundary_conds = glob.glob(os.path.join(input_dir, \"forcing_obc_segment_*.nc\"))\nprint(f\"Found {len(boundary_conds)} OBC files:\", boundary_conds)\n\nif len(boundary_conds) == 0:\n    raise FileNotFoundError(f\"No boundary condition files found at {input_dir}\")\n\n\n\n","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-obc#check-for-boundary-conditions-verify-horizontal-grid","position":9},{"hierarchy":{"lvl1":"Accessing and Processing Open Boundary Conditions","lvl2":"Opening and Checking Open Boundary Conditions"},"type":"lvl2","url":"/notebooks/diagnostics/cupid-output/regional-ocean-obc#opening-and-checking-open-boundary-conditions","position":10},{"hierarchy":{"lvl1":"Accessing and Processing Open Boundary Conditions","lvl2":"Opening and Checking Open Boundary Conditions"},"content":"\n\nsegment_num_to_dir = {\n    \"001\": \"south\",\n    \"002\": \"north\",\n    \"003\": \"west\",\n    \"004\": \"east\",\n}\nsegment_dir_to_num = {v: k for k, v in segment_num_to_dir.items()}\n\n\n\nlist_boundaries = []\ntemp_time_slice = slice(0, 5)\n\nfor boundary_file in boundary_conds:\n\n    boundary = xr.open_mfdataset(boundary_file)\n\n    # Temp time slice\n    boundary = boundary.isel(time=temp_time_slice)\n\n    code = (boundary_file.split(\"_\")[-1]).split(\".\")[0]\n    bound = segment_num_to_dir[code]\n\n    # Variables on the U/V grid (corners)\n    uv_vars = [f\"u_segment_{code}\", f\"v_segment_{code}\"]\n\n    # Variables on the T grid (cell center)\n    tracer_vars = [\n        f\"salt_segment_{code}\",\n        f\"temp_segment_{code}\",\n        f\"eta_segment_{code}\",\n    ]\n\n    # Create separate datasets for each grid type\n    boundary_uv = boundary[uv_vars]\n    boundary_t = boundary[tracer_vars]\n\n    # Rename coordinates for both datasets\n    boundary_uv = boundary_uv.rename(\n        {\n            f\"ny_segment_{code}\": \"yq\",\n            f\"nx_segment_{code}\": \"xq\",\n            f\"lon_segment_{code}\": \"longitude_q\",\n            f\"lat_segment_{code}\": \"latitude_q\",\n        }\n    )\n    boundary_t = boundary_t.rename(\n        {\n            f\"ny_segment_{code}\": \"yh\",\n            f\"nx_segment_{code}\": \"xh\",\n            f\"lon_segment_{code}\": \"longitude_h\",\n            f\"lat_segment_{code}\": \"latitude_h\",\n        }\n    )\n\n    # Drop variables and handle squeeze for both\n    boundary_uv = boundary_uv.drop_vars([\"nxp\", \"nyp\"])\n    boundary_t = boundary_t.drop_vars([\"nxp\", \"nyp\"])\n\n    if bound in [\"north\", \"south\"]:\n        # Slicing for a C-grid\n        boundary_uv = boundary_uv.isel(xq=slice(0, None, 2))\n        boundary_t = boundary_t.isel(xh=slice(1, None, 2))\n\n        # Assign new coordinates\n        boundary_uv = boundary_uv.assign_coords(\n            xq=(\"xq\", boundary_uv[\"longitude_q\"].values)\n        )\n        boundary_t = boundary_t.assign_coords(\n            xh=(\"xh\", boundary_t[\"longitude_h\"].values)\n        )\n\n        # boundary_uv = boundary_uv.squeeze('yq', drop = True)\n        # boundary_t = boundary_t.squeeze('yh', drop = True)\n    else:\n        # Slicing for a C-grid\n        boundary_uv = boundary_uv.isel(yq=slice(0, None, 2))\n        boundary_t = boundary_t.isel(yh=slice(1, None, 2))\n\n        # Assign new coordinates\n        boundary_uv = boundary_uv.assign_coords(\n            yq=(\"yq\", boundary_uv[\"latitude_q\"].values)\n        )\n        boundary_t = boundary_t.assign_coords(\n            yh=(\"yh\", boundary_t[\"latitude_h\"].values)\n        )\n\n        # boundary_uv = boundary_uv.squeeze('xq', drop = True)\n        # boundary_t = boundary_t.squeeze('xh', drop = True)\n\n    # Combine the processed datasets\n    boundary = xr.merge([boundary_uv, boundary_t])\n\n    # Remove longitude and latitude variables from the datasets\n    for coord in [\"longitude_q\", \"latitude_q\"]:\n        if coord in boundary:\n            boundary = boundary.drop_vars(coord)\n    for coord in [\"longitude_h\", \"latitude_h\"]:\n        if coord in boundary:\n            boundary = boundary.drop_vars(coord)\n\n    # Dumb check to make sure OBCs are on the same grid as model output\n    model_xh = static_data[\"xh\"]\n    model_yh = static_data[\"yh\"]\n    model_xq = static_data[\"xq\"]\n    model_yq = static_data[\"yq\"]\n\n    # if boundary['xh'].size > 1: assert np.allclose(boundary['xh'].values, model_xh.values, atol=1e-6), f\"Boundary {bound} x-coordinates do not match model grid\"\n    # if boundary['yh'].size > 1: assert np.allclose(boundary['yh'].values, model_yh.values, atol=1e-6), f\"Boundary {bound} y-coordinates do not match model grid\"\n    # if boundary['xq'].size > 1: assert np.allclose(boundary['xq'].values, model_xq.values, atol=1e-6), f\"Boundary {bound} corner x-coordinates do not match model grid\"\n    # if boundary['yq'].size > 1: assert np.allclose(boundary['yq'].values, model_yq.values, atol=1e-6), f\"Boundary {bound} corner y-coordinates do not match model grid\"\n\n    z_map = [\n        f\"nz_segment_{code}_u\",\n        f\"nz_segment_{code}_v\",\n        f\"nz_segment_{code}_temp\",\n        f\"nz_segment_{code}_salt\",\n    ]\n    new_boundary = {}\n    for var in boundary.data_vars:\n        da = boundary[var]\n        da.load()\n\n        zdim = [str(d) for d in da.dims if str(d) in z_map]\n        if len(zdim) > 0:\n            da = da.rename({zdim[0]: \"z_l\"})\n        new_boundary[var.split(\"_\")[0]] = da\n\n    new_boundary = xr.Dataset(new_boundary).expand_dims(boundary=[bound])\n    list_boundaries.append(new_boundary)\n\n# boundaries = xr.concat(list_boundaries, dim = 'boundary')\n\n\n\nlist_boundaries[1]\n\n\n\nstatic_data\n\n","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-obc#opening-and-checking-open-boundary-conditions","position":11},{"hierarchy":{"lvl1":"Regional Ocean: Basic Region and Surface Field Visualization"},"type":"lvl1","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card","position":0},{"hierarchy":{"lvl1":"Regional Ocean: Basic Region and Surface Field Visualization"},"content":"","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card","position":1},{"hierarchy":{"lvl1":"Regional Ocean: Basic Region and Surface Field Visualization"},"type":"lvl1","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#regional-ocean-basic-region-and-surface-field-visualization","position":2},{"hierarchy":{"lvl1":"Regional Ocean: Basic Region and Surface Field Visualization"},"content":"\n\nNote: This notebook is meant to be run with the cupid-analysis kernel (see \n\nCUPiD Installation). This notebook is often run by default as part of \n\nCESM post-processing steps, but you can also run it manually.\n\n","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#regional-ocean-basic-region-and-surface-field-visualization","position":3},{"hierarchy":{"lvl1":"Regional Ocean: Basic Region and Surface Field Visualization","lvl2":"Diagnostics and Plotting Resources"},"type":"lvl2","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#diagnostics-and-plotting-resources","position":4},{"hierarchy":{"lvl1":"Regional Ocean: Basic Region and Surface Field Visualization","lvl2":"Diagnostics and Plotting Resources"},"content":"","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#diagnostics-and-plotting-resources","position":5},{"hierarchy":{"lvl1":"Regional Ocean: Basic Region and Surface Field Visualization","lvl3":"Highly Recommended:","lvl2":"Diagnostics and Plotting Resources"},"type":"lvl3","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#highly-recommended","position":6},{"hierarchy":{"lvl1":"Regional Ocean: Basic Region and Surface Field Visualization","lvl3":"Highly Recommended:","lvl2":"Diagnostics and Plotting Resources"},"content":"Xarray Fundamentals - Earth Environmental Data Science Course (recommend the entire course!)\n\nMOM6 Analysis Cookbook","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#highly-recommended","position":7},{"hierarchy":{"lvl1":"Regional Ocean: Basic Region and Surface Field Visualization","lvl3":"Great! Focused on Global Output Diagnostics","lvl2":"Diagnostics and Plotting Resources"},"type":"lvl3","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#great-focused-on-global-output-diagnostics","position":8},{"hierarchy":{"lvl1":"Regional Ocean: Basic Region and Surface Field Visualization","lvl3":"Great! Focused on Global Output Diagnostics","lvl2":"Diagnostics and Plotting Resources"},"content":"MOM6 Tools (parimarily for global metrics)\n\nMOM6 Diagnostics - CESM Tutorial","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#great-focused-on-global-output-diagnostics","position":9},{"hierarchy":{"lvl1":"Regional Ocean: Basic Region and Surface Field Visualization","lvl3":"Misc.","lvl2":"Diagnostics and Plotting Resources"},"type":"lvl3","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#misc","position":10},{"hierarchy":{"lvl1":"Regional Ocean: Basic Region and Surface Field Visualization","lvl3":"Misc.","lvl2":"Diagnostics and Plotting Resources"},"content":"MOM6 File Structure\n\nxGCM - python package for staggered grids\n\n%load_ext autoreload\n%autoreload 2\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport regional_utils as utils\nimport xarray as xr\nfrom cartopy import crs as ccrs\n\n\n\ncase_name = \"\"  # \"/glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/Diagnostics/CESM_Output/\"\nCESM_output_dir = \"\"  # \"CROCODILE_tutorial_nwa12_MARBL\"\n\n# As regional domains vary so much in purpose, simulation length, and extent, we don't want to assume a minimum duration\n## Thus, we ignore start and end dates and simply reduce/output over the whole time frame for all of the examples given.\nstart_date = None  # \"0001-01-01\"\nend_date = None  # \"0101-01-01\n\nsave_figs = False\nfig_output_dir = None\n\nlc_kwargs = {}\nserial = False\n\nsfc_variables = []  # ['SSH', 'tos', 'sos', 'speed', 'SSV', 'SSU']\nmonthly_variables = []  # ['thetao', 'so', 'uo', 'vo']\n\n\n\n# Parameters\ncase_name = \"CROCODILE_tutorial_nwa12_MARBL\"\nCESM_output_dir = (\n    \"/glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/Diagnostics/CESM_Output/\"\n)\nstart_date = \"\"\nend_date = \"\"\nsave_figs = True\nfig_output_dir = None\nlc_kwargs = {\"threads_per_worker\": 1}\nserial = True\nsfc_variables = [\"tos\"]\nmonthly_variables = [\"thetao\"]\nsubset_kwargs = {}\nproduct = \"/glade/work/ajanney/crocodile_2025/CUPiD/examples/regional_ocean/computed_notebooks//ocn/Regional_Ocean_Report_Card.ipynb\"\n\n\n\n\nOUTDIR = os.path.join(CESM_output_dir, case_name, \"ocn\", \"hist\")\nprint(\"Output directory is:\", OUTDIR)\n\n\n\n","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#misc","position":11},{"hierarchy":{"lvl1":"Load in Model Output and Look at Variables/Meta Data"},"type":"lvl1","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#load-in-model-output-and-look-at-variables-meta-data","position":12},{"hierarchy":{"lvl1":"Load in Model Output and Look at Variables/Meta Data"},"content":"","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#load-in-model-output-and-look-at-variables-meta-data","position":13},{"hierarchy":{"lvl1":"Load in Model Output and Look at Variables/Meta Data","lvl3":"Default File Structure in MOM6"},"type":"lvl3","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#default-file-structure-in-mom6","position":14},{"hierarchy":{"lvl1":"Load in Model Output and Look at Variables/Meta Data","lvl3":"Default File Structure in MOM6"},"content":"This file structure will be different if you modify the diag_table.\n\nstatic data: contains horizontal grid, vertical grid, land/sea mask, bathymetry, lat/lon information\n\nsfc data: daily output of 2D surface fields (salinity, temp, SSH, velocities)\n\nmonthly data: averaged monthly output of the full 3D domain, regridded to a predefined grid (MOM6 default is WOA, see more below)\n\nnative data: averaged monthly output of ocean state and atmospheric fluxes on the native MOM6 grid\n\n# Xarray time decoding things\ntime_coder = xr.coders.CFDatetimeCoder(use_cftime=True)\n\n## Static data includes hgrid, vgrid, bathymetry, land/sea mask\nstatic_data = xr.open_mfdataset(\n    os.path.join(OUTDIR, \"*h.static.nc\"),\n    decode_timedelta=True,\n    decode_times=time_coder,\n    engine=\"netcdf4\",\n)\n\n## Surface Data\nsfc_data = xr.open_mfdataset(\n    os.path.join(OUTDIR, \"*h.sfc*.nc\"),\n    decode_timedelta=True,\n    decode_times=time_coder,\n    engine=\"netcdf4\",\n)\n\n## Monthly Full Domain Data\n## Not used in this notebook by default\nmonthly_data = xr.open_mfdataset(\n    os.path.join(OUTDIR, \"*h.z*.nc\"),\n    decode_timedelta=True,\n    decode_times=time_coder,\n    engine=\"netcdf4\",\n)\n\n## Monthly Full Domain Data, on native MOM6 grid\n## Not used in this notebook by default\nnative_data = xr.open_mfdataset(\n    os.path.join(OUTDIR, \"*h.native*.nc\"),\n    decode_timedelta=True,\n    decode_times=time_coder,\n    engine=\"netcdf4\",\n)\n\n## Image/Gif Output Directory\nif fig_output_dir is None:\n    image_output_dir = os.path.join(\n        \"/glade/derecho/scratch/\",\n        os.environ[\"USER\"],\n        \"archive\",\n        case_name,\n        \"ocn\",\n        \"cupid_images\",\n    )\nelse:\n    image_output_dir = os.path.join(fig_output_dir, case_name, \"ocn\", \"cupid_images\")\nif not os.path.exists(image_output_dir):\n    os.makedirs(image_output_dir)\nprint(\"Image output directory is:\", image_output_dir)\n\n\n\n## Select for only the variables we want to analyze\nif len(sfc_variables) > 0:\n    print(\"Selecting only the following surface variables:\", sfc_variables)\n    sfc_data = sfc_data[sfc_variables]\nif len(monthly_variables) > 0:\n    print(\"Selecting only the following monthly variables:\", monthly_variables)\n    monthly_data = monthly_data[monthly_variables]\n\n## Apply time boundaries\n## if they are the right format\nif len(start_date.split(\"-\")) == 3 and len(end_date.split(\"-\")) == 3:\n    import cftime\n\n    calendar = sfc_data.time.encoding.get(\"calendar\", \"standard\")\n\n    calendar_map = {\n        \"gregorian\": cftime.DatetimeProlepticGregorian,\n        \"noleap\": cftime.DatetimeNoLeap,\n    }\n\n    CFTime = calendar_map.get(calendar, cftime.DatetimeGregorian)\n    y, m, d = [int(i) for i in start_date.split(\"-\")]\n    start_date_time = CFTime(y, m, d)\n    y, m, d = [int(i) for i in end_date.split(\"-\")]\n    end_date_time = CFTime(y, m, d)\n\n    print(\n        f\"Applying time range from start_date: {start_date_time} and end_date: {end_date_time}.\"\n    )\n\n    sfc_data = sfc_data.sel(time=slice(start_date_time, end_date_time))\n    monthly_data = monthly_data.sel(time=slice(start_date_time, end_date_time))\n    native_data = native_data.sel(time=slice(start_date_time, end_date_time))\n\nsfc_time_bounds = [\n    sfc_data[\"time\"].isel(time=0).compute().item(),\n    sfc_data[\"time\"].isel(time=-1).compute().item(),\n]\nmonthly_time_bounds = [\n    monthly_data[\"time\"].isel(time=0).compute().item(),\n    monthly_data[\"time\"].isel(time=-1).compute().item(),\n]\n\nprint(f\"Surface Data Time Bounds: {sfc_time_bounds[0]} to {sfc_time_bounds[-1]}\")\nprint(\n    f\"Monthly Data Time Bounds: {monthly_time_bounds[0]} to {monthly_time_bounds[-1]}\"\n)\n\n\n\n","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#default-file-structure-in-mom6","position":15},{"hierarchy":{"lvl1":"Load in Model Output and Look at Variables/Meta Data","lvl3":"*mom6.h.static*.nc: static information about the domain"},"type":"lvl3","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#id-mom6-h-static-nc-static-information-about-the-domain","position":16},{"hierarchy":{"lvl1":"Load in Model Output and Look at Variables/Meta Data","lvl3":"*mom6.h.static*.nc: static information about the domain"},"content":"The MOM6 grid uses an Arakawa C grid which staggers velocities and tracers (temp, salinity, SSH, etc.). See \n\nthis MOM6 documentation for more information.","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#id-mom6-h-static-nc-static-information-about-the-domain","position":17},{"hierarchy":{"lvl1":"Load in Model Output and Look at Variables/Meta Data","lvl4":"Some variables of interest:","lvl3":"*mom6.h.static*.nc: static information about the domain"},"type":"lvl4","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#some-variables-of-interest","position":18},{"hierarchy":{"lvl1":"Load in Model Output and Look at Variables/Meta Data","lvl4":"Some variables of interest:","lvl3":"*mom6.h.static*.nc: static information about the domain"},"content":"geolon/geolat (and c/u/v variants): specifies the true lat/lon of each cell. We use these variables for plotting and placing the data geographically.\n\nwet (and c/u/v variants): the land-sea mask that specifies if a given point is on land or sea.\n\nareacello (and bu,cu,cv variants): area of grid cell (important for taking area-weighted averages)\n\ndeptho: depth of ocean floor - bathymetry\n\nstatic_data\n\n\n\nIndexing and the Horizontal C-grid in MOM6\n\nWhen accessing the output, we need to pay particular attention to which variables we are accessing and which coordinates correspond to their position on the grid. This also affects plotting and spatial averages (as we will see in this notebook and others).\n\nxh/yh: index the center of the cell in x and y respectively\n\nxq/yq: index the corner of the cell in x and y respectively\n\nCoordinates:\n\n(xh,yh): center of cell, where tracers are. Plot with geolon,geolat.\n\n(xh,yq): middle of horizontal interface, where meridional (v) velocity is. Plot with geolon_v,geolat_v.\n\n(xq,yh): middle of vertical interface, where zonal (u) velocity is. Plot with geolon_u,geolat_u.\n\n(xq,yq): corners between cells, where vorticity is.\n\n","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#some-variables-of-interest","position":19},{"hierarchy":{"lvl1":"Load in Model Output and Look at Variables/Meta Data","lvl3":"*h.sfc*.nc: daily surface fields"},"type":"lvl3","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#id-h-sfc-nc-daily-surface-fields","position":20},{"hierarchy":{"lvl1":"Load in Model Output and Look at Variables/Meta Data","lvl3":"*h.sfc*.nc: daily surface fields"},"content":"The surface fields are especially useful for diagnosing short runs. This is not only the most dynamic field for short runs, but it also is the only file that stores daily results (by default). A lot of the diagnostics in this notebook use surface fields because we are able to take time averages and look at time series for any run longer than a couple of days (unlike the full 3D domain fields which are averaged over each month).","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#id-h-sfc-nc-daily-surface-fields","position":21},{"hierarchy":{"lvl1":"Load in Model Output and Look at Variables/Meta Data","lvl4":"Some variables of interest:","lvl3":"*h.sfc*.nc: daily surface fields"},"type":"lvl4","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#some-variables-of-interest-1","position":22},{"hierarchy":{"lvl1":"Load in Model Output and Look at Variables/Meta Data","lvl4":"Some variables of interest:","lvl3":"*h.sfc*.nc: daily surface fields"},"content":"SSH: sea surface height\n\ntos: temperature of ocean surface\n\nsos: salinity of ocean surface\n\nspeed: magnitude of speed (considered a tracer, at the center of a cell)\n\nSSU and SSV: zonal and meridional velocity at the surface\n\nsfc_data\n\n\n\n","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#some-variables-of-interest-1","position":23},{"hierarchy":{"lvl1":"Load in Model Output and Look at Variables/Meta Data","lvl3":"*h.z*.nc: fields for the full 3D domain, averaged monthly, regridded vertically"},"type":"lvl3","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#id-h-z-nc-fields-for-the-full-3d-domain-averaged-monthly-regridded-vertically","position":24},{"hierarchy":{"lvl1":"Load in Model Output and Look at Variables/Meta Data","lvl3":"*h.z*.nc: fields for the full 3D domain, averaged monthly, regridded vertically"},"content":"These are vertically remapped diagnostics files that capture the full 3D domain, but they only output monthly averages. For runs less than a month, they will average over the full length of the run.\n\nBy default, this diagnostics MOM6 regrids output to the vertical grid from the 2009 World Ocean Atlas (35 layers down to 6750 m).","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#id-h-z-nc-fields-for-the-full-3d-domain-averaged-monthly-regridded-vertically","position":25},{"hierarchy":{"lvl1":"Load in Model Output and Look at Variables/Meta Data","lvl4":"Some variables of interest:","lvl3":"*h.z*.nc: fields for the full 3D domain, averaged monthly, regridded vertically"},"type":"lvl4","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#some-variables-of-interest-2","position":26},{"hierarchy":{"lvl1":"Load in Model Output and Look at Variables/Meta Data","lvl4":"Some variables of interest:","lvl3":"*h.z*.nc: fields for the full 3D domain, averaged monthly, regridded vertically"},"content":"uo and vo: zonal and meridional velocity\n\nthetao: potential temperature\n\nso: salinity\n\nvmo and umo: zonal and meridional mass transport\n\nvolcello: volume of each cell (important for averaging over a volume of the domain)\n\nNote: now there are two z coordinates. z_i is the vertical interface between cells and z_l identifies the depth of the cell centers.\n\nmonthly_data\n\n\n\nNote on Vertically Remapped Diagnostics in MOM6\n\nThe output found in *mom6.h.z.*.nc files is vertically remapped by MOM6. By default, this output is on the 2009 World Ocean Atlas grid. You can regrid the output after runtime (see packages like \n\nxgcm and and \n\nxESMF), but if you know a vertical grid that you need output on, MOM6 can handle the interpolation automatically.\n\nThe vertical grid settings for a particular CESM run can be found in MOM_parameter_doc.all in the CESM case run directory. See \n\nthis MOM6 documentation for more information.\n\n","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#some-variables-of-interest-2","position":27},{"hierarchy":{"lvl1":"Load in Model Output and Look at Variables/Meta Data","lvl3":"*h.native*.nc: fields for the full 3D domain, averaged monthly, on native MOM6 grid."},"type":"lvl3","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#id-h-native-nc-fields-for-the-full-3d-domain-averaged-monthly-on-native-mom6-grid","position":28},{"hierarchy":{"lvl1":"Load in Model Output and Look at Variables/Meta Data","lvl3":"*h.native*.nc: fields for the full 3D domain, averaged monthly, on native MOM6 grid."},"content":"These outputs are on the MOM6 native grid. From the \n\nMOM6 Documentation:\n\nSince the model can be run in arbitrary coordinates, say in hybrid-coordinate mode, then native-space diagnostics can be potentially confusing. Native diagnostics are useful when examining exactly what the model is doing\n\nThe default native file (as configured by CESM) also outputs useful global averages and atmospheric variables.","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#id-h-native-nc-fields-for-the-full-3d-domain-averaged-monthly-on-native-mom6-grid","position":29},{"hierarchy":{"lvl1":"Load in Model Output and Look at Variables/Meta Data","lvl4":"Some variables of interest:","lvl3":"*h.native*.nc: fields for the full 3D domain, averaged monthly, on native MOM6 grid."},"type":"lvl4","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#some-variables-of-interest-3","position":30},{"hierarchy":{"lvl1":"Load in Model Output and Look at Variables/Meta Data","lvl4":"Some variables of interest:","lvl3":"*h.native*.nc: fields for the full 3D domain, averaged monthly, on native MOM6 grid."},"content":"soga: global mean ocean salinity\n\nthetaoga: global mean ocean potential temperature\n\ntauuo and tauvo: zonal and meridional downward stress from the atmospheric forcing\n\nhfds: net downward surface heat flux\n\nhf*: various individual heat fluxes into the ocean\n\nfriver: freshwater flux from rivers\n\nnative_data\n\n\n\n","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#some-variables-of-interest-3","position":31},{"hierarchy":{"lvl1":"Look at Regional Domain"},"type":"lvl1","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#look-at-regional-domain","position":32},{"hierarchy":{"lvl1":"Look at Regional Domain"},"content":"\n\n%matplotlib inline\n\n\n\nNote: you may see some numpy divide by zero errors below. I don‚Äôt why, but they aren‚Äôt an issue!\n\nutils.visualizeRegionalDomain(static_data)\n\n\n\n","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#look-at-regional-domain","position":33},{"hierarchy":{"lvl1":"Plotting Regional MOM6 Output"},"type":"lvl1","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#plotting-regional-mom6-output","position":34},{"hierarchy":{"lvl1":"Plotting Regional MOM6 Output"},"content":"We will primarily look at the surface fields and some of the monthly full domain fields in this notebook.\n\nXarray comes with great default plotting wrappers that can do a lot! If you take an Xarray DataArray (one variable data structure) and callxr.DataArray.plot()\n\nit will usually do a pretty good job! Here we‚Äôll walk through some powerful ways to use this plot wrapper, and we provide some additional functions to help the plotting along!\n\n","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#plotting-regional-mom6-output","position":35},{"hierarchy":{"lvl1":"Plotting Regional MOM6 Output","lvl2":"Simple plotting with Xarray"},"type":"lvl2","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#simple-plotting-with-xarray","position":36},{"hierarchy":{"lvl1":"Plotting Regional MOM6 Output","lvl2":"Simple plotting with Xarray"},"content":"By default, these files are all loaded in as Datasets which store a large set of variables with a variety of shared coordinates/dimensions.\n\nWhen plotting, we need to reduce this space to a single variable. If we want to plot a spatial field we need one time step, and if we want to plot a timeseries, we need to average over the spatial dimensions (this is a bit more complicated than it might seem because the grid cells are not all the same size).\n\nLet‚Äôs use some basic Xarray plotting to look at a surface field:\n\n## Select a single variable at a single time\nsst = sfc_data[\"tos\"].isel(time=0)\nsst.plot(cmap=\"viridis\", vmin=-1.9, vmax=30)  # approx. color bar bounds for temp\n# Note that it automatically labels with coordinate names and attributes.\n\n\n\nLooks good! But from above we know we want to plot with geolon and geolat for an accurate map. We can also choose some different color maps and projections. Let‚Äôs use some regional_utils to get this done.\n\n## Select a single variable at a single time\nsst = sfc_data[\"tos\"].isel(time=0)\n\ncoords = utils.chooseGeoCoords(sst.dims)\nlat = static_data[coords[\"latitude\"]]\nlon = static_data[coords[\"longitude\"]]\n\nsst = sst.assign_coords({\"lon\": lon, \"lat\": lat})\n\ncmap = utils.chooseColorMap(sst.name)\n\ncbar_levels = utils.chooseColorLevels(\n    sst.min().compute().item(),\n    sst.max().compute().item(),\n)\n\n# Create the plot with projection\nfig = plt.figure(dpi=200)\nax = plt.axes(projection=ccrs.Mercator())\n\n# Plot the data\np = sst.plot(\n    x=\"lon\",\n    y=\"lat\",\n    cmap=cmap,\n    levels=cbar_levels,\n    transform=ccrs.PlateCarree(),\n    ax=ax,\n    add_colorbar=True,\n)\n\n# Add coastlines - now this should work\nax.coastlines(resolution=\"50m\", color=\"black\", linewidth=0.3)\n\nplt.show()\n\n\n\nLooks good! Doing all this setup everytime would be cumbersome, so we wrapped it in a function utils.plotLatLonField.\n\nPlease go check it out and see what‚Äôs going on behind the scenes. It also can calculate statistics for the field taking into account area/volume weights.\n\n","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#simple-plotting-with-xarray","position":37},{"hierarchy":{"lvl1":"Plotting Regional MOM6 Output","lvl2":"MOM6 Output - Surface Fields"},"type":"lvl2","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#mom6-output-surface-fields","position":38},{"hierarchy":{"lvl1":"Plotting Regional MOM6 Output","lvl2":"MOM6 Output - Surface Fields"},"content":"\n\nThe cells below visualize the mean state and the std of the fields over the full time given.\n\n## Ploting basic surface state variables\nadd_stats = True\ntime_bounds = (\n    sfc_data[\"time\"][0].compute().item(),\n    sfc_data[\"time\"][-1].compute().item(),\n)\n\nfor var in sfc_variables:\n    if var not in list(sfc_data.variables):\n        print(f\"Variable '{var}' not in given sfc_data. It will not be plotted.\")\n        sfc_variables.remove(var)\n\n\nprint(f\"Taking mean and std dev from {time_bounds[0]} to {time_bounds[-1]}\")\nfor var in sfc_variables:\n    field = sfc_data[var]\n\n    coords = utils.chooseGeoCoords(field.dims)\n    areacello = utils.chooseAreacello(field.dims)\n\n    mean = field.mean(dim=\"time\", skipna=True).compute()\n    std = field.std(dim=\"time\", skipna=True).compute()\n    mean.attrs = field.attrs\n    std.attrs = field.attrs\n    mean.attrs[\"long_name\"] = f\"Mean {field.long_name}\"\n    std.attrs[\"long_name\"] = f\"Std Dev {field.long_name}\"\n\n    utils.plotLatLonField(\n        mean,\n        latitude=static_data[coords[\"latitude\"]],\n        longitude=static_data[coords[\"longitude\"]],\n        stats=add_stats,\n        area_weights=static_data[areacello],\n        save=save_figs,\n        save_path=image_output_dir,\n    )\n    plt.show()\n\n    utils.plotLatLonField(\n        std,\n        latitude=static_data[coords[\"latitude\"]],\n        longitude=static_data[coords[\"longitude\"]],\n        stats=add_stats,\n        area_weights=static_data[areacello],\n        save=save_figs,\n        save_path=image_output_dir,\n    )\n    plt.show()\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#mom6-output-surface-fields","position":39},{"hierarchy":{"lvl1":"Area Weighted Averages and Timeseries"},"type":"lvl1","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#area-weighted-averages-and-timeseries","position":40},{"hierarchy":{"lvl1":"Area Weighted Averages and Timeseries"},"content":"\n\nCalculating and plotting timeseries can also be complicated. We need to be careful when taking an average of the fields that we take weighted statistics because the grid cells are not guaranteed to be constant area or volume.\n\nThe information we need for these weighted averages is contained in the areacello* variables (in static_data) and the volcello variable (in native_data and monthly_data).\n\nWe have to pay attention to coordinates and dimensions because of MOM6‚Äôs staggered grid (see how we choose the geolon/lat coords above).\n\nNote: These timeseries may be less helpful for shorter runs, but see what they reveal!\n\nfor var in sfc_variables:\n    field = sfc_data[var]\n    areacello_var = utils.chooseAreacello(field.dims)\n\n    utils.plotAvgTimeseries(\n        sfc_data[var],\n        static_data[areacello_var],\n        save=save_figs,\n        save_path=image_output_dir,\n    )\n\n\n\n","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#area-weighted-averages-and-timeseries","position":41},{"hierarchy":{"lvl1":"Viewing Full Domain Monthly Output"},"type":"lvl1","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#viewing-full-domain-monthly-output","position":42},{"hierarchy":{"lvl1":"Viewing Full Domain Monthly Output"},"content":"Let‚Äôs look at the monthly output. When using longer runs (years or decades). These hold a lot more information about how your model is evolving and performing over time. For shorter runs (like many regional models often are), we may only have a handful of monthly output timesteps.\n\nXarray has some very useful methods for visualizing multiple levels and easily plotting them using the col and col_wrap keywords.\n\n**Note: ** none of the following plots are saved. If you want to save them, rerun the first few cells of the notebook and manually save these.\n\nfor var in monthly_variables:\n    if var not in list(monthly_data.variables):\n        print(f\"Variable '{var}' not in given monthly_data. It will not be plotted.\")\n        monthly_variables.remove(var)\n\n## Let's only look at one variable for now (this plotting can be slow)\nfor var in monthly_variables[0:1]:\n    # Look at only the first time step for now\n    field = monthly_data[var].isel(time=0)\n\n    cmap = utils.chooseColorMap(var)\n    levels = utils.chooseColorLevels(\n        field.min().compute().item(),\n        field.max().compute().item(),\n    )\n\n    subplot_kwargs = {\n        \"projection\": ccrs.Mercator(),\n    }\n\n    p = field.plot(\n        col=\"z_l\",\n        col_wrap=5,\n        cmap=cmap,\n        levels=levels,\n        transform=ccrs.PlateCarree(),\n        subplot_kws=subplot_kwargs,\n    )\n    plt.suptitle(f\"{var} at depths\", y=1.0, fontsize=20)\n\n    for ax in p.axs.flat:\n        ax.coastlines(resolution=\"50m\", color=\"black\", linewidth=0.3)\n\n\n\n","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#viewing-full-domain-monthly-output","position":43},{"hierarchy":{"lvl1":"Viewing Full Domain Monthly Output","lvl2":"Vertical Profiles"},"type":"lvl2","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#vertical-profiles","position":44},{"hierarchy":{"lvl1":"Viewing Full Domain Monthly Output","lvl2":"Vertical Profiles"},"content":"Another useful tool, sometimes we want to look at the vertical profile of something like salinity or temperature. Be careful to use area weighted averages here. (This could be a tool in regional_utils, but it‚Äôs not too much code, you decide!)\n\nfor var in monthly_variables[0:1]:  # only one variable again, make it more!\n    # Look at only the first time step for now\n    field = monthly_data[var].isel(time=0)\n\n    area_weights = utils.chooseAreacello(field.dims)\n    field_weighted = field.weighted(static_data[area_weights])\n\n    mean_vars = [dim for dim in field.dims if dim != \"z_l\"]\n    field_mean = field_weighted.mean(mean_vars)\n\n    field_mean.plot(y=\"z_l\", yincrease=False)\n\n\n\n","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#vertical-profiles","position":45},{"hierarchy":{"lvl1":"Viewing Full Domain Monthly Output","lvl2":"Bonus: Interpolating Levels and Slices"},"type":"lvl2","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#bonus-interpolating-levels-and-slices","position":46},{"hierarchy":{"lvl1":"Viewing Full Domain Monthly Output","lvl2":"Bonus: Interpolating Levels and Slices"},"content":"We successfully plotted different levels above, we can also plot vertical slices. In both cases, we might run into an issue where we want to interpolate to a depth or lat/lon that is not explicitly resolved in the model. We can write our own tailored methods for this, but for simple visual inspection/comparison Xarray tools work well!\n\nmonthly_data[\"z_l\"]\n\n\n\n## Plot a specific depth\nfield = monthly_data[\"thetao\"].isel(time=0)\nfield = field.interp(\n    z_l=550, method=\"linear\"\n)  # you may choose a different depth if you want to test the interpolation\n\ncoords = utils.chooseGeoCoords(field.dims)\nareacello = utils.chooseAreacello(field.dims)\n\nutils.plotLatLonField(\n    field,\n    latitude=static_data[coords[\"latitude\"]],\n    longitude=static_data[coords[\"longitude\"]],\n    stats=True,\n    area_weights=static_data[areacello],\n    save=save_figs,\n    save_path=image_output_dir,\n)\nplt.show()\n\n\n\n","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#bonus-interpolating-levels-and-slices","position":47},{"hierarchy":{"lvl1":"Viewing Full Domain Monthly Output","lvl3":"Let‚Äôs try it with a vertical slice now too!","lvl2":"Bonus: Interpolating Levels and Slices"},"type":"lvl3","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#lets-try-it-with-a-vertical-slice-now-too","position":48},{"hierarchy":{"lvl1":"Viewing Full Domain Monthly Output","lvl3":"Let‚Äôs try it with a vertical slice now too!","lvl2":"Bonus: Interpolating Levels and Slices"},"content":"\n\nfield = monthly_data[\"thetao\"].isel(time=0)\nx_var = \"xh\" if \"xh\" in field.dims else \"xq\"\nlongitude = np.mean(field[x_var].to_numpy())\nvertical = field.interp({x_var: longitude})\nlevels = np.linspace(\n    vertical.min().compute().item(), vertical.max().compute().item(), 35\n)\nvertical.plot(\n    yincrease=False,\n    levels=levels,\n)\n\n","type":"content","url":"/notebooks/diagnostics/cupid-output/regional-ocean-report-card#lets-try-it-with-a-vertical-slice-now-too","position":49},{"hierarchy":{"lvl1":"Running a CUPiD Example"},"type":"lvl1","url":"/notebooks/diagnostics/standalone-cupid","position":0},{"hierarchy":{"lvl1":"Running a CUPiD Example"},"content":"CUPiD provides examples of configuration files to allow users to look at a variety of diagnostics in CUPiD/examples.\nThese examples are designed to be run on the NCAR supercomputers,\nusing output curated from CESM development runs and stored in /glade/campaign/cesm/development/cross-wg/diagnostic_framework/CESM_output_for_testing/.\nThe provided configuration files also act as templates for the CESM workflow, as discussed in the CUPiD project pages.\n\nNote: we actually access the example output through the CROCODILE workshop directory for simplicity: /glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/Diagnostics/CESM_Output/.\nIt just links back to CESM_output_for_testing.\n\nThere is a regional_ocean example in CUPiD that provides diagnostics for a 10-month CESM run created with CrocoDash.\nThis run uses a 1/12¬∞ grid over the northwest Atlantic domain, and includes ocean biogeochemistry tracers from the Marine Biogeochemistry library (MARBL).\n(The compset used was 1850_DATM%JRA_SLND_SICE_MOM6%MARBL-BIO_SROF_SGLC_SWAV_SESP.)\n\nFor this exercise, we will run the regional_ocean example in CUPiD.\nThis includes four notebooks:\n\nRegional_Ocean_Report_Card.ipynb: basic plotting and analysis utilities, primarily focused on surface fields.\n\nRegional_Ocean_Animations.ipynb: create animations.\n\nRegional_Ocean_Atmospheric_Forcing.ipynb: look at atmospheric forcing at the surface.\n\nRegional_Ocean_OBC.ipynb: visualize surface fields and open boundary conditions.","type":"content","url":"/notebooks/diagnostics/standalone-cupid","position":1},{"hierarchy":{"lvl1":"Running a CUPiD Example","lvl2":"Task 2: Let‚Äôs Run CUPiD"},"type":"lvl2","url":"/notebooks/diagnostics/standalone-cupid#task-2-lets-run-cupid","position":2},{"hierarchy":{"lvl1":"Running a CUPiD Example","lvl2":"Task 2: Let‚Äôs Run CUPiD"},"content":"Navigate to the examples subdirectory of your installation of CUPiD and look at what‚Äôs inside:cd ${CUPID_ROOT}/examples\nls\n\nOutputadditional_metrics  external_diag_packages  key_metrics  regional_ocean\n\nWe will be using the regional_ocean example for this demo and the workshop:cd regional_ocean","type":"content","url":"/notebooks/diagnostics/standalone-cupid#task-2-lets-run-cupid","position":3},{"hierarchy":{"lvl1":"Running a CUPiD Example","lvl3":"2.1 Confirm the Conda Environments Installed Correctly","lvl2":"Task 2: Let‚Äôs Run CUPiD"},"type":"lvl3","url":"/notebooks/diagnostics/standalone-cupid#id-2-1-confirm-the-conda-environments-installed-correctly","position":4},{"hierarchy":{"lvl1":"Running a CUPiD Example","lvl3":"2.1 Confirm the Conda Environments Installed Correctly","lvl2":"Task 2: Let‚Äôs Run CUPiD"},"content":"For a standalone CUPiD run (as opposed to running it from a CESM case),\nrun the cupid-diagnostics command from the same directory as a config.yml file.\nCUPiD will create a computed_notebooks directory for output.\nThis command will only be recognized in the (cupid-infrastructure) environment,\nso let‚Äôs make sure the (cupid-infrastructure) and (cupid-analysis) conda environments are fully installed for everyone.conda activate cupid-infrastructure\nwhich cupid-diagnostics\n\nIf the environment installed correctly,\nwhich will return a path to cupid-diagnostics.\nIf the environment did not install correctly,\nwhich will return an error like which: no cupid-diagnostics in [long list of directories].\n\n","type":"content","url":"/notebooks/diagnostics/standalone-cupid#id-2-1-confirm-the-conda-environments-installed-correctly","position":5},{"hierarchy":{"lvl1":"Running a CUPiD Example","lvl4":"Checkpoint #3","lvl3":"2.1 Confirm the Conda Environments Installed Correctly","lvl2":"Task 2: Let‚Äôs Run CUPiD"},"type":"lvl4","url":"/notebooks/diagnostics/standalone-cupid#checkpoint-3","position":6},{"hierarchy":{"lvl1":"Running a CUPiD Example","lvl4":"Checkpoint #3","lvl3":"2.1 Confirm the Conda Environments Installed Correctly","lvl2":"Task 2: Let‚Äôs Run CUPiD"},"content":"At this point the following should all be true:\n\nyour terminal is in CUPiD‚Äôs examples/regional_ocean directory,\n\nthe (cupid-infrastructure) conda environment is active, and\n\nthe which cupid-diagnostics command found the cupid-diagnostics script.","type":"content","url":"/notebooks/diagnostics/standalone-cupid#checkpoint-3","position":7},{"hierarchy":{"lvl1":"Running a CUPiD Example","lvl3":"2.2 Time to Run CUPiD!","lvl2":"Task 2: Let‚Äôs Run CUPiD"},"type":"lvl3","url":"/notebooks/diagnostics/standalone-cupid#id-2-2-time-to-run-cupid","position":8},{"hierarchy":{"lvl1":"Running a CUPiD Example","lvl3":"2.2 Time to Run CUPiD!","lvl2":"Task 2: Let‚Äôs Run CUPiD"},"content":"üö® Conda and Jupyter might be Butting Heads üö®\n\nBefore moving on, we want to make sure Jupyter can recognize the cupid-analysis environment that will run all of the notebooks. This may not be an issue for you, but just to be safe, follow the steps below.conda activate cupid-analysis\npython -m ipykernel install --user --name=cupid-analysis\nconda deactivate  # or `conda activate cupid-infrastructure`\n\nNow you should be back in the cupid-infrastructure environment and ready to rumble.\n\nIt‚Äôs finally time to run CUPiD!\nFrom the same ${CUPID_ROOT}/examples/regional_ocean directory, run on one processor with:cupid-diagnostics --serial\n\nThis step might take some time, and we can track the progress with the output to terminal.\nIf you notice an error about CUPiD not being able to find the (cupid-analysis) environment, make sure to check the red alert box above and install the ipykernel.\n\nThe notebooks will be run in nblibrary and then copied to computed_notebooks/ocn under the example directory.\nIf you want to rerun the notebooks, make sure to manually copy the regional_utils.py file with:cp ../../nblibrary/ocn/regional_utils.py computed_notebooks/ocn/\n\nmore on this and cupid-webpage below.","type":"content","url":"/notebooks/diagnostics/standalone-cupid#id-2-2-time-to-run-cupid","position":9},{"hierarchy":{"lvl1":"Running a CUPiD Example","lvl2":"CUPiD Configuration"},"type":"lvl2","url":"/notebooks/diagnostics/standalone-cupid#cupid-configuration","position":10},{"hierarchy":{"lvl1":"Running a CUPiD Example","lvl2":"CUPiD Configuration"},"content":"While we‚Äôre waiting for CUPiD, let‚Äôs revisit the famous config.yml file. It‚Äôs delineated","type":"content","url":"/notebooks/diagnostics/standalone-cupid#cupid-configuration","position":11},{"hierarchy":{"lvl1":"Running a CUPiD Example","lvl3":"data_sources Section","lvl2":"CUPiD Configuration"},"type":"lvl3","url":"/notebooks/diagnostics/standalone-cupid#data-sources-section","position":12},{"hierarchy":{"lvl1":"Running a CUPiD Example","lvl3":"data_sources Section","lvl2":"CUPiD Configuration"},"content":"The first section in the config.yml file is data_sources:\n\nThis typically does not need to be edited by the user,\nand may be removed in favor of command-line arguments to the cupid-diagnostics script.\nIt points CUPiD to the notebook library and also tells CUPiD where to execute the notebooks\n(we want the notebooks to be run in the output directory rather than the nblibrary directory).","type":"content","url":"/notebooks/diagnostics/standalone-cupid#data-sources-section","position":13},{"hierarchy":{"lvl1":"Running a CUPiD Example","lvl3":"computation_config Section","lvl2":"CUPiD Configuration"},"type":"lvl3","url":"/notebooks/diagnostics/standalone-cupid#computation-config-section","position":14},{"hierarchy":{"lvl1":"Running a CUPiD Example","lvl3":"computation_config Section","lvl2":"CUPiD Configuration"},"content":"Much like the data_sources section,\nthis section typically does not need to be modified by users and may turn into command-line arguments.\nIt provides the name of the conda environment to run notebooks in by default (users can specify different environments for individual notebooks),\nand it also sets logging information:","type":"content","url":"/notebooks/diagnostics/standalone-cupid#computation-config-section","position":15},{"hierarchy":{"lvl1":"Running a CUPiD Example","lvl3":"global_params Section","lvl2":"CUPiD Configuration"},"type":"lvl3","url":"/notebooks/diagnostics/standalone-cupid#global-params-section","position":16},{"hierarchy":{"lvl1":"Running a CUPiD Example","lvl3":"global_params Section","lvl2":"CUPiD Configuration"},"content":"There are some parameters that are passed to every notebook.\nThese are typically variables associated with the runs being compared\n(things like CESM case names, location of data, length of the run, and so on).\nFor regional_ocean, there are some parameters we want to pass to every ocean notebook and they are included here as well:","type":"content","url":"/notebooks/diagnostics/standalone-cupid#global-params-section","position":17},{"hierarchy":{"lvl1":"Running a CUPiD Example","lvl3":"time_series Section","lvl2":"CUPiD Configuration"},"type":"lvl3","url":"/notebooks/diagnostics/standalone-cupid#time-series-section","position":18},{"hierarchy":{"lvl1":"Running a CUPiD Example","lvl3":"time_series Section","lvl2":"CUPiD Configuration"},"content":"One of the data standardization tasks CUPiD does is converting CESM history files to time series files\n(rather than have many variables at a single time level, these files are a single variable at many time levels).\nThe notebooks provided for this tutorial read history files,\nand the interface for this section is still under development,\nso we won‚Äôt spend much time discussing it.\n\nWant to generate a timeseries?\n\nNote that the timeseries output directory ts_dir is not instantiated in this example. You are able to create timeseries files, but you are not able to save them to the CESM_output_dir as you normally would because we only have read permissions there.\n\nIf you want to run the timeseries tool, set ts_dir: /glade/derecho/scratch/${USER}/archive (or another directory you have write access to) and then runcupid-timeseries","type":"content","url":"/notebooks/diagnostics/standalone-cupid#time-series-section","position":19},{"hierarchy":{"lvl1":"Running a CUPiD Example","lvl3":"compute_notebooks Section","lvl2":"CUPiD Configuration"},"type":"lvl3","url":"/notebooks/diagnostics/standalone-cupid#compute-notebooks-section","position":20},{"hierarchy":{"lvl1":"Running a CUPiD Example","lvl3":"compute_notebooks Section","lvl2":"CUPiD Configuration"},"content":"This section tells CUPiD what notebooks to run,\nand what parameters should be passed to that notebook in addition to the ones listed in global_params.\nCUPiD will always run the infrastructure section,\nand the user can specify what components (atm, ocn, lnd, etc) should also be run.\nBy default, CUPiD will run all the notebooks in this section.\n\nThe first key under each component (e.g. Regional_Ocean_Report_Card) is the name of a notebook,\nand CUPiD will look in nblibrary/{component} for that file.\nIn this example, CUPiD will run nblibrary/atm/Regional_Ocean_Report_Card.ipynb.\nYou can provide more than one notebook per component.","type":"content","url":"/notebooks/diagnostics/standalone-cupid#compute-notebooks-section","position":21},{"hierarchy":{"lvl1":"Running a CUPiD Example","lvl3":"book_toc Section","lvl2":"CUPiD Configuration"},"type":"lvl3","url":"/notebooks/diagnostics/standalone-cupid#book-toc-section","position":22},{"hierarchy":{"lvl1":"Running a CUPiD Example","lvl3":"book_toc Section","lvl2":"CUPiD Configuration"},"content":"After running all the notebooks specified in compute_notebooks,\nCUPiD can use \n\nJupyter Book to create a website.\nUnfortunately there is not a great way to view HTML files that are stored on the NCAR super computers,\nso for this tutorial we will look at the notebooks that have been executed.\n\nTo build the website, however, the book_toc section lays out how to organize the notebooks into different chapters.\nOur examples organize the pages by component,\nbut in other cases it may make sense to group notebooks differently\n(e.g. global surface plots in one section, time series plots of global means in another).","type":"content","url":"/notebooks/diagnostics/standalone-cupid#book-toc-section","position":23},{"hierarchy":{"lvl1":"Running a CUPiD Example","lvl3":"book_config_keys Section","lvl2":"CUPiD Configuration"},"type":"lvl3","url":"/notebooks/diagnostics/standalone-cupid#book-config-keys-section","position":24},{"hierarchy":{"lvl1":"Running a CUPiD Example","lvl3":"book_config_keys Section","lvl2":"CUPiD Configuration"},"content":"This section is used to set the title of the Jupyter Book webpage.\nIt should probable be combined with the book_toc section,\nor maybe it should be a command line argument instead.","type":"content","url":"/notebooks/diagnostics/standalone-cupid#book-config-keys-section","position":25},{"hierarchy":{"lvl1":"Running a CUPiD Example","lvl3":"Where are my notebooks?","lvl2":"CUPiD Configuration"},"type":"lvl3","url":"/notebooks/diagnostics/standalone-cupid#where-are-my-notebooks","position":26},{"hierarchy":{"lvl1":"Running a CUPiD Example","lvl3":"Where are my notebooks?","lvl2":"CUPiD Configuration"},"content":"Remember that the notebooks all run in nblibrary, but after cupid-diagnostics has finished, the notebooks are all copied over the the run_dir in the config file (in this case the same directory as the config). This primarily matters for re-running notebooks when we would need to copy over any dependencies (e.g. nblibrary/regional_ocean.utils.py) and be aware of relative paths.","type":"content","url":"/notebooks/diagnostics/standalone-cupid#where-are-my-notebooks","position":27},{"hierarchy":{"lvl1":"Running a CUPiD Example","lvl2":"Brief overview of the notebooks"},"type":"lvl2","url":"/notebooks/diagnostics/standalone-cupid#brief-overview-of-the-notebooks","position":28},{"hierarchy":{"lvl1":"Running a CUPiD Example","lvl2":"Brief overview of the notebooks"},"content":"To the JupyterHub! Navigate to nblibrary to follow along and we‚Äôll take a look at what our output should look like.\n\nWe can also take a sneak peak at the CrocoGallery \n\nCUPiD_output page before your output is done generating.","type":"content","url":"/notebooks/diagnostics/standalone-cupid#brief-overview-of-the-notebooks","position":29},{"hierarchy":{"lvl1":"Running a CUPiD Example","lvl2":"Final Task: Now it‚Äôs your turn!"},"type":"lvl2","url":"/notebooks/diagnostics/standalone-cupid#final-task-now-its-your-turn","position":30},{"hierarchy":{"lvl1":"Running a CUPiD Example","lvl2":"Final Task: Now it‚Äôs your turn!"},"content":"The power of CUPiD now lies in your hands (don‚Äôt spend it all in one place, or do, it‚Äôs reusable)!\n\nPlease mess with this config file, change variables, adjust paths! Feel free to go in an modify any code you like.\n\nIf you want to run the CUPiD notebooks on your CESM output from the tutorials earlier in the week, it can be done in a few simple steps. In the same examples/regional_ocean directory:\n\nmv computed_notebooks computed_notebooks.example - try not to override diagnostic output, your future self will thank you!\n\nModify config.yml paths/variables:\n\nGlobal Params:case_name and CESM_output_dir\n\nNotebook Params: mom6_input_dir - look for ocnice from CrocoDash!\n\nRun cupid-diagnostics --serial - some of the output may look a little different becaue we‚Äôre working with days/weeks not months.\n\nLet us know if you run into any issues!","type":"content","url":"/notebooks/diagnostics/standalone-cupid#final-task-now-its-your-turn","position":31},{"hierarchy":{"lvl1":"Running a CUPiD Example","lvl3":"Want to generate a CUPiD webpage of your output?","lvl2":"Final Task: Now it‚Äôs your turn!"},"type":"lvl3","url":"/notebooks/diagnostics/standalone-cupid#want-to-generate-a-cupid-webpage-of-your-output","position":32},{"hierarchy":{"lvl1":"Running a CUPiD Example","lvl3":"Want to generate a CUPiD webpage of your output?","lvl2":"Final Task: Now it‚Äôs your turn!"},"content":"We recommend viewing the completed notebooks in JupyterHub.\nThis is the easiest way to see CUPiD output on the NCAR super computer,\nand also makes it easy to re-run the notebooks manually if you want to play with the output.\n\nNote: make sure to select the cupid-analysis kernel in the top right if you want to rerun the notebooks.\n\nIf you want to use CUPiD‚Äôs webpage feature, however, runcupid-webpage\n\nafter cupid-diagnostics completes.\nLike cupid-diagnostics, this command is part of the (cupid-infrastructure) environment and should be run from the directory containing config.yml.\n\nUnfortunately, it is not easy to view webpages on the NCAR super computer.\nYour best bet is probably copying the entire computed_notebooks/_build/html directory to your local computer.\nMore options are discussed in the CUPiD documentation for \n\nLooking at Output.\n\nYou can transfer the files to your personal computer using a tool like scp or rsync:scp USERNAME@casper.hpc.ucar.edu:/glade/work/USERNAME/crocodile_2025/CUPiD/examples/computed_notebooks/_build /path/on/personal/computer\n\nAlternatively, make sure to attend Sam Rabin‚Äôs lecture on VSCode tools for another fancy way of viewing HTML files!","type":"content","url":"/notebooks/diagnostics/standalone-cupid#want-to-generate-a-cupid-webpage-of-your-output","position":33},{"hierarchy":{"lvl1":"Run a coupled biogeochemistry (MARBL) & ocean (MOM6) model"},"type":"lvl1","url":"/notebooks/features/add-bgc","position":0},{"hierarchy":{"lvl1":"Run a coupled biogeochemistry (MARBL) & ocean (MOM6) model"},"content":"MARBL is part of the CESM\n\nAdd MARBL to the compset\n\nAdd relevant information to configure_forcingss\n\nSee below for an example.\n\nAdding runoff to a BGC example and getting river nutrients requires an offline mapping. Add the GLOFAS runoff mesh as well as the river nutrients (which is on the GLOFAS grid)\n\nfrom CrocoDash.case import Case\ncase = Case(\n    cesmroot = cesmroot,\n    caseroot = caseroot,\n    inputdir = inputdir,\n    ocn_grid = grid,\n    ocn_vgrid = vgrid,\n    ocn_topo = topo,\n    project = 'NCGD0011',\n    override = True,\n    machine = \"derecho\",\n    compset = \"CR1850MARBL_JRA\"\n)\n\n\n\ncase.configure_forcings(\n    date_range = [\"2000-01-01 00:00:00\", \"2000-02-01 00:00:00\"],\n    data_input_path = \"\", # Path to CESM OUTPUT data\n    product_name = \"CESM_OUTPUT\",\n    marbl_ic_filepath = \"\" # Path to MARBL Global IC File\n)\n\n# For River Nutrients, which should be used when runoff is also on, add\n#    runoff_esmf_mesh_filepath = \"\", # For river nutrient mapping \n#    global_river_nutrients_filepath = \"\", # Path to global river nutrients mapping\n \n\n\n\n\nNote\n\nIf you‚Äôre working on Derecho or Casper, you can use the following paths:data_input_path = \"/glade/campaign/collections/cmip/CMIP6/CESM-HR/FOSI_BGC/HR/g.e22.TL319_t13.G1850ECOIAF_JRA_HR.4p2z.001/ocn/proc/tseries/month_1\"\nproduct_name = \"CESM_OUTPUT\"\nrunoff_esmf_mesh_filepath = \"/glade/campaign/cesm/cesmdata/cseg/inputdata/ocn/mom/croc/rof/glofas/dis24/GLOFAS_esmf_mesh_v4.nc\"\nglobal_river_nutrients_filepath = \"/glade/campaign/cesm/cesmdata/cseg/inputdata/ocn/mom/croc/rof/river_nutrients/river_nutrients.GNEWS_GNM.glofas.20250916.64bit.nc\"\nmarbl_ic_filepath = \"/glade/campaign/collections/gdex/data/d651077/cesmdata/inputdata/ocn/mom/tx0.66v1/ecosys_jan_IC_omip_latlon_1x1_180W_c231221.nc\"","type":"content","url":"/notebooks/features/add-bgc","position":1},{"hierarchy":{"lvl1":"Add Chlorophyll Files to the Run"},"type":"lvl1","url":"/notebooks/features/add-chl","position":0},{"hierarchy":{"lvl1":"Add Chlorophyll Files to the Run"},"content":"MOM6 can take chlorophyll data as a file in the regional domain. It impacts shortwave penetration. MOM6 parameters that are impacted by it CHL_FROM_FILE, CHL_FILE, VAR_PEN_SW, and PEN_SW_NBANDS.\nIn our workflow, we take raw data from SeaWIFS, process it globally, and subset to our regional domain.\n\nChlorophyll can be added into a CrocoDash case, using functions from mom6_bathy and called from configure_forcings. There is one parameter in configure forcings and three parameters in MOM6\n\nCaution\n\nThis method does not do a great job of resolving chlorophyll in estuaries and similar features. If possible, the generated chlorophyll file should be replaced with a better product (which can be done by replacing the file in CHL_FILE).","type":"content","url":"/notebooks/features/add-chl","position":1},{"hierarchy":{"lvl1":"Add Chlorophyll Files to the Run","lvl2":"CrocoDash Parameters"},"type":"lvl2","url":"/notebooks/features/add-chl#crocodash-parameters","position":2},{"hierarchy":{"lvl1":"Add Chlorophyll Files to the Run","lvl2":"CrocoDash Parameters"},"content":"In case.configure_forcings(), the argument chl_processed_filepath takes in a processed global chlorophyll file. The global processed chlorophyll file is hosted on the CESM inputdata svn server under ocn/mom/croc/chl/data and can be accessed through the CrocoDash raw_data_access module like below:\n\nfrom CrocoDash.raw_data_access.datasets import seawifs as sw\nsw.get_processed_global_seawifs_script_for_cli(\n    output_dir=\"<insert_dir>\",\n    output_file=\"get_seawifs_data.sh\"\n)\n\n\n\nThe file path of the global file (after running the script from the code block) can be passed into configure forcings as shown in this demo\n\ncase.configure_forcings(\n    date_range = [\"2020-01-01 00:00:00\", \"2020-01-09 00:00:00\"],\n    chl_file = \"<CHL_PROCESSED>\",\n)\n\n\n\nNote\n\nIf you‚Äôre working on Derecho or Casper, you can use the following paths:chl_file: /glade/campaign/cesm/cesmdata/cseg/inputdata/ocn/mom/croc/chl/data/SeaWIFS.L3m.MC.CHL.chlor_a.0.25deg.nc","type":"content","url":"/notebooks/features/add-chl#crocodash-parameters","position":3},{"hierarchy":{"lvl1":"Run a coupled sea ice (CICE) & ocean (MOM6) model"},"type":"lvl1","url":"/notebooks/features/add-cice","position":0},{"hierarchy":{"lvl1":"Run a coupled sea ice (CICE) & ocean (MOM6) model"},"content":"Change the compset to include CICE!\n\n# For example...\nfrom CrocoDash.case import Case\ncase = Case(\n    cesmroot = cesmroot,\n    caseroot = caseroot,\n    inputdir = inputdir,\n    ocn_grid = grid,\n    ocn_vgrid = vgrid,\n    ocn_topo = topo,\n    project = 'NCGD0011',\n    override = True,\n    machine = \"derecho\",\n    compset = \"GR_JRA\"\n)\n\n\n\n","type":"content","url":"/notebooks/features/add-cice","position":1},{"hierarchy":{"lvl1":"Run a coupled sea ice (CICE) & ocean (MOM6) model","lvl2":"Optional: After running once, use restart files as initial condition for your run (for realiastic initial conditions)"},"type":"lvl2","url":"/notebooks/features/add-cice#optional-after-running-once-use-restart-files-as-initial-condition-for-your-run-for-realiastic-initial-conditions","position":2},{"hierarchy":{"lvl1":"Run a coupled sea ice (CICE) & ocean (MOM6) model","lvl2":"Optional: After running once, use restart files as initial condition for your run (for realiastic initial conditions)"},"content":"\n\nIn case you want to start the model with a restart file instead of using the generated initial condition, follow the below steps. Note that you have to have finished a run beforehand for the restart files to appear.\n\nLocate your restart file - they are usually in your previous case‚Äôs /archive/rest/<year> folder with a .r infix, e.g., cice.test.cice.r.1994-01-01-00000.nc.\nIf you do not know your /archive folder location, run ./xmlquery DOUT_S_ROOT in your (previous) case folder, which will return a path similar to DOUT_S_ROOT: <PATH TO ARCHIVE>.\n\nUse the cp command to copy the file to your current case /run directory, e.g., cp cice.test.cice.r.1994-01-01-00000.nc <your_run_dir>.\n\nOpen user_nl_cice in your case directory again and change the ice_ic variable from \"UNSET\" to your file name, e.g., ice_ic=cice.test.cice.r.1994-01-01-00000.nc\n\nYour file will now be used automatically as the ice initial condition for your next run. Note that .h and .h1 files (i.e. history files) currently cannot be used as initial conditions for CICE.","type":"content","url":"/notebooks/features/add-cice#optional-after-running-once-use-restart-files-as-initial-condition-for-your-run-for-realiastic-initial-conditions","position":3},{"hierarchy":{"lvl1":"Use different data products w/ the CrocoDash Data Access Module"},"type":"lvl1","url":"/notebooks/features/add-data-products","position":0},{"hierarchy":{"lvl1":"Use different data products w/ the CrocoDash Data Access Module"},"content":"This module can be used with case.configure_forcings to find different ways to access raw data from data sources. Just supply data product name and function name in the following manner:\n\ncase.configure_forcing(...other_arguments..., \nproduct_name = \"product_name\", \nfunction_name = \"function_name\")\n\n\n\nAvailable products and functions can be found in the \n\ndocumentation and in the raw_data_access helper functions shown below. A full list of available functions & products can be accessed in the same documentation.\n\nfrom CrocoDash.raw_data_access import driver\nfrom CrocoDash.raw_data_access import config as tb\n\n\n\npfr_obj = driver.ProductFunctionRegistry()\npfr_obj.load_functions()\npfr_obj.functions\n\n\n\ntb.list_products()\n\n\n\ntb.list_functions(\"GLORYS\")\n\n\n\ntb.type_of_function(\"GLORYS\",\"get_glorys_data_from_cds_api\")\n\n\n\ncase.configure_forcing(date_range = [\"2020-01-01 00:00:00\", \"2020-01-09 00:00:00\"], \n                       product_name = \"product_name\", \n                       function_name = \"function_name\")\n\n\n\n","type":"content","url":"/notebooks/features/add-data-products","position":1},{"hierarchy":{"lvl1":"Use different data products w/ the CrocoDash Data Access Module","lvl2":"Accessing non-forcing raw data"},"type":"lvl2","url":"/notebooks/features/add-data-products#accessing-non-forcing-raw-data","position":2},{"hierarchy":{"lvl1":"Use different data products w/ the CrocoDash Data Access Module","lvl2":"Accessing non-forcing raw data"},"content":"Apart from accessing forcing products through case.configure_forcings(), we can access products like GEBCO, SEAWIFS, GLOFAS, etc.. through importing the raw data access module, like below.\n\n# Import the specific module (which can be found by looking at the API Documentation: https://crocodile-cesm.github.io/CrocoDash/api-docs/CrocoDash.raw_data_access.datasets.html)\nfrom CrocoDash.raw_data_access.datasets import glofas as gl\n\n# Then call the function\ngl.get_processed_global_glofas_script_for_cli(output_dir=tmp_path, output_file=\"glofas_processed_data.nc\")\n\n","type":"content","url":"/notebooks/features/add-data-products#accessing-non-forcing-raw-data","position":3},{"hierarchy":{"lvl1":"Use Custom Pre-Generated Grids"},"type":"lvl1","url":"/notebooks/features/add-grids","position":0},{"hierarchy":{"lvl1":"Use Custom Pre-Generated Grids"},"content":"","type":"content","url":"/notebooks/features/add-grids","position":1},{"hierarchy":{"lvl1":"Use Custom Pre-Generated Grids","lvl2":"Step 1.1: Horizontal Grid"},"type":"lvl2","url":"/notebooks/features/add-grids#step-1-1-horizontal-grid","position":2},{"hierarchy":{"lvl1":"Use Custom Pre-Generated Grids","lvl2":"Step 1.1: Horizontal Grid"},"content":"\n\nfrom CrocoDash.grid import Gridx\n\ngrid = Grid.from_supergrid(\"<HGRID_TRIMMED>\")\n\n\n\n","type":"content","url":"/notebooks/features/add-grids#step-1-1-horizontal-grid","position":3},{"hierarchy":{"lvl1":"Use Custom Pre-Generated Grids","lvl2":"Step 1.2: Topography"},"type":"lvl2","url":"/notebooks/features/add-grids#step-1-2-topography","position":4},{"hierarchy":{"lvl1":"Use Custom Pre-Generated Grids","lvl2":"Step 1.2: Topography"},"content":"\n\nfrom CrocoDash.topo import Topo\n\nbathymetry_path='<BATHY_FILE>'\n\ntopo = Topo.from_topo_file(\n    grid = grid,\n    topo_file_path=bathymetry_path,\n    min_depth = 5,\n)\n\n\n\ntopo.depth.plot()\n\n\n\n","type":"content","url":"/notebooks/features/add-grids#step-1-2-topography","position":5},{"hierarchy":{"lvl1":"Use Custom Pre-Generated Grids","lvl2":"Step 1.3: Vertical Grid"},"type":"lvl2","url":"/notebooks/features/add-grids#step-1-3-vertical-grid","position":6},{"hierarchy":{"lvl1":"Use Custom Pre-Generated Grids","lvl2":"Step 1.3: Vertical Grid"},"content":"\n\nfrom CrocoDash.vgrid import VGrid\n\nvgrid_path='<YOUR_VGRID_PATH>'\n\nvgrid  = VGrid.from_file(vgrid_path)\n\n\n\nimport matplotlib.pyplot as plt\n# Create the plot\nfor depth in vgrid.z:\n    plt.axhline(y=depth, linestyle='-')  # Horizontal lines\n\nplt.ylim(max(vgrid.z) + 10, min(vgrid.z) - 10)  # Invert y-axis so deeper values go down\nplt.ylabel(\"Depth\")\nplt.title(\"Vertical Grid\")\nplt.show()\n\n","type":"content","url":"/notebooks/features/add-grids#step-1-3-vertical-grid","position":7},{"hierarchy":{"lvl1":"Add data runoff from GLOFAS or JRA to the Run"},"type":"lvl1","url":"/notebooks/features/add-rof","position":0},{"hierarchy":{"lvl1":"Add data runoff from GLOFAS or JRA to the Run"},"content":"GLOFAS & JRA ships as part of the CESM\n\nAdd DROF%GLOFAS or DROF%JRA to the compset\n\nPass in the ESMF Mesh File for the dataset to configure_forcings to get the mapping from the GLOFAS data to your domain (will take time and resources)\n\nSee below for an example\n\nfrom CrocoDash.case import Case\n\ncase = Case(\n    cesmroot = cesmroot,\n    caseroot = caseroot,\n    inputdir = inputdir,\n    ocn_grid = grid,\n    ocn_vgrid = vgrid,\n    ocn_topo = topo,\n    project = 'NCGD0011',\n    override = True,\n    machine = \"derecho\",\n    compset = \"CR_JRA_GLOFAS\" # If you'd like JRA, use the compset longname!\n)\n\n\n\ncase.configure_forcings(\n    date_range = [\"2000-01-01 00:00:00\", \"2000-02-01 00:00:00\"],\n    runoff_esmf_mesh_filepath = \"<ESMF_MESH_PATH>\" # For runoff mapping, the esmf mesh file of the input dataset\n)\n\n\n\n\nNote\n\nIf you‚Äôre working on Derecho or Casper, you can use the following paths:GLOFAS ESMF Mesh: \"/glade/campaign/cesm/cesmdata/cseg/inputdata/ocn/mom/croc/rof/glofas/dis24/GLOFAS_esmf_mesh_v4.nc\"\nJRA ESMF Mesh: \"/glade/campaign/cesm/cesmdata/inputdata/lnd/dlnd7/JRA55/JRA.v1.4.runoff.1958_ESMFmesh_cdf5_20201020.nc\"","type":"content","url":"/notebooks/features/add-rof","position":1},{"hierarchy":{"lvl1":"Adding your own runoff product to the CESM (Advanced)"},"type":"lvl1","url":"/notebooks/features/add-runoff-product","position":0},{"hierarchy":{"lvl1":"Adding your own runoff product to the CESM (Advanced)"},"content":"To add your own runoff product to the CESM, you need three things. A Stream Definition File, an ESMF Mesh file, and the dimensions of your raw data.\n\n","type":"content","url":"/notebooks/features/add-runoff-product","position":1},{"hierarchy":{"lvl1":"Adding your own runoff product to the CESM (Advanced)","lvl2":"ESMF Mesh File"},"type":"lvl2","url":"/notebooks/features/add-runoff-product#esmf-mesh-file","position":2},{"hierarchy":{"lvl1":"Adding your own runoff product to the CESM (Advanced)","lvl2":"ESMF Mesh File"},"content":"An ESMF Mesh File can be created like below, using CrocoDash Grids.\n\nfrom CrocoDash.grid import Grid\nfrom CrocoDash.topo import Topo\nimport xarray as xr\n\ngrid = Grid(\n    lenx=360,\n    leny       = 150,         # grid length in y direction\n    cyclic_x=True   ,    \n    ystart     = -60,       # start/end 10 degrees above/below poles to avoid singularity\n    resolution = 0.10,\n    name = \"GLOFAS\",\n)\ntopo = Topo(grid, min_depth = 0)\ntopo.set_flat(10)\ntopo.write_esmf_mesh(\"<path>\")\n\n\n\n","type":"content","url":"/notebooks/features/add-runoff-product#esmf-mesh-file","position":3},{"hierarchy":{"lvl1":"Adding your own runoff product to the CESM (Advanced)","lvl2":"Stream Definition File"},"type":"lvl2","url":"/notebooks/features/add-runoff-product#stream-definition-file","position":4},{"hierarchy":{"lvl1":"Adding your own runoff product to the CESM (Advanced)","lvl2":"Stream Definition File"},"content":"In your case directory, copy the following file and name it drof.streams.xml:<?xml version=\"1.0\"?>\n<file id=\"stream\" version=\"2.0\">\n\n  <stream_info name=\"rof.<PRODUCT_NAME>\">\n    <taxmode>cycle</taxmode>\n    <tintalgo>upper</tintalgo>\n    <readmode>single</readmode>\n    <mapalgo>bilinear</mapalgo>\n    <dtlimit>3.0</dtlimit>\n    <year_first>\"START_YEAR\"</year_first>\n    <year_last>\"END_YEAR\"</year_last>\n    <year_align>\"START_YEAR\"</year_align>\n    <vectors>null</vectors>\n    <meshfile>\"PATH_TO_THE_PREVIOUSLY_GENERATED_ESMF_MESH_FILE\"</meshfile>\n    <lev_dimname>null</lev_dimname>\n    <datafiles>\n      <file>\"PATH_TO_RAW_DATA_FILE_IN_NETCDF3_64BIT_OFFSET\"</file>\n    </datafiles>\n    <datavars>\n      <var>\"NETCDF_VARIABLE_NAME_IN_FILE\" Forr_rofl</var>\n    </datavars>\n    <offset>0</offset>\n  </stream_info>\n\n</file>","type":"content","url":"/notebooks/features/add-runoff-product#stream-definition-file","position":5},{"hierarchy":{"lvl1":"Adding your own runoff product to the CESM (Advanced)","lvl3":"Additional Notes","lvl2":"Stream Definition File"},"type":"lvl3","url":"/notebooks/features/add-runoff-product#additional-notes","position":6},{"hierarchy":{"lvl1":"Adding your own runoff product to the CESM (Advanced)","lvl3":"Additional Notes","lvl2":"Stream Definition File"},"content":"Update the CESM components/cdeps/drof directory.The following entry should already exist in namelist_definition_drof.xml:<value drof_mode=\"<PRODUCT_NAME>\">rof.<PRODUCT_NAME></value>\n\nUpdate config_component with the new <PRODUCT_NAME>:<desc rof=\"DROF[%NULL][%NYF][%<PRODUCT_NAME>]...\">\n\nMake sure to replace placeholders (<PRODUCT_NAME>, START_YEAR, END_YEAR, file paths, and variable names) with your actual values.\n\n","type":"content","url":"/notebooks/features/add-runoff-product#additional-notes","position":7},{"hierarchy":{"lvl1":"Adding your own runoff product to the CESM (Advanced)","lvl2":"Add the dimensions"},"type":"lvl2","url":"/notebooks/features/add-runoff-product#add-the-dimensions","position":8},{"hierarchy":{"lvl1":"Adding your own runoff product to the CESM (Advanced)","lvl2":"Add the dimensions"},"content":"Make the following xml changes in your case directory:./xmlchange ROF_NY=1500\n\n./xmlchange ROF_NX=3600\n\n./xmlchange ROF_DOMAIN_MESH=<MESH PATH>\n\nThere may be a few additional changes in the CESM, but this covers a majority of them.\n\nThen, you can add the mapping files generated through CrocoDash to your case (which requires a passed in ESMF mesh file)","type":"content","url":"/notebooks/features/add-runoff-product#add-the-dimensions","position":9},{"hierarchy":{"lvl1":"Add Boundary Tides to the Run"},"type":"lvl1","url":"/notebooks/features/add-tides","position":0},{"hierarchy":{"lvl1":"Add Boundary Tides to the Run"},"content":"MOM6 can take tides data as a boundary condition in the regional domain. Many tides parameters are impacted by this and can be seen in case.process_forcings.\n\nIn our workflow, we take data from the TPXO tidal model and regrid onto our grid.\n\nTPXO model data can be requested off of the TPXO website or is available on derecho.\n\nThe file paths of the tidal files can be passed into configure forcings as shown below with all wanted tidal constituents, like M2.\nThere are three parameters in configure forcings and many parameters adjusted in MOM6.\n\ncase.configure_forcings(\n    date_range = [\"2020-01-01 00:00:00\", \"2020-01-09 00:00:00\"],\n    tidal_constituents = ['M2'],\n    tpxo_elevation_filepath = \"<TPXO_H>\",\n    tpxo_velocity_filepath = \"<TPXO_U>\"\n)\n\n\n\nNote\n\nIf you‚Äôre working on Derecho or Casper, you can use the following paths:TPXO Elevation: \"/glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/CrocoDash/data/tpxo/h_tpxo9.v1.nc\"\nTPXO Velocity: \"/glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/CrocoDash/data/tpxo/u_tpxo9.v1.nc\"","type":"content","url":"/notebooks/features/add-tides","position":1},{"hierarchy":{"lvl1":"Add Global Subset Grids"},"type":"lvl1","url":"/notebooks/features/subset-global","position":0},{"hierarchy":{"lvl1":"Add Global Subset Grids"},"content":"","type":"content","url":"/notebooks/features/subset-global","position":1},{"hierarchy":{"lvl1":"Add Global Subset Grids","lvl2":"Step 1.1: Horizontal Grid"},"type":"lvl2","url":"/notebooks/features/subset-global#step-1-1-horizontal-grid","position":2},{"hierarchy":{"lvl1":"Add Global Subset Grids","lvl2":"Step 1.1: Horizontal Grid"},"content":"Extract a subgrid from a global grid using the subgrid_from_supergrid method:\n\nfrom CrocoDash.grid import Grid\n\ngrid = Grid.subgrid_from_supergrid(\n    path = \"<HGRID_TRIMMED>\",  # supergrid\n    llc = (16.0, 192.0),  # (l)ower (l)eft (c)orner coords\n    urc = (27.0, 209.0),  # (u)pper (r)ight (c)orner coords\n    name = \"hawaii_2\"\n)\n\n\n\n","type":"content","url":"/notebooks/features/subset-global#step-1-1-horizontal-grid","position":3},{"hierarchy":{"lvl1":"Add Global Subset Grids","lvl2":"Step 1.2: Topography"},"type":"lvl2","url":"/notebooks/features/subset-global#step-1-2-topography","position":4},{"hierarchy":{"lvl1":"Add Global Subset Grids","lvl2":"Step 1.2: Topography"},"content":"\n\nfrom CrocoDash.topo import Topo\n\ntopo = Topo(\n    grid = grid,\n    min_depth = 9.5,\n)\n\n\n\nbathymetry_path='<GEBCO>'\n\ntopo.interpolate_from_file(\n    file_path = bathymetry_path,\n    longitude_coordinate_name=\"lon\",\n    latitude_coordinate_name=\"lat\",\n    vertical_coordinate_name=\"elevation\"\n)\n\n\n\ntopo.depth.plot()\n\n","type":"content","url":"/notebooks/features/subset-global#step-1-2-topography","position":5},{"hierarchy":{"lvl1":"Setup Iterative OBC generation (for Long CESM-MOM6 runs) w/ the  Large Data Workflow (Advanced)"},"type":"lvl1","url":"/notebooks/features/too-much-data","position":0},{"hierarchy":{"lvl1":"Setup Iterative OBC generation (for Long CESM-MOM6 runs) w/ the  Large Data Workflow (Advanced)"},"content":"Often, generating OBC datasets can be done all in one shot, but in longer and larger cases (like running the Northwest Atlantic for a year) we need to start iterating through the generation. Generating open boundary condition (OBC) data is essential for the entire model runtime but can be time-consuming and resource-intensive.\n\nThe Large Data Workflow in CrocoDash helps manage this by breaking data access into smaller, more manageable components.","type":"content","url":"/notebooks/features/too-much-data","position":1},{"hierarchy":{"lvl1":"Setup Iterative OBC generation (for Long CESM-MOM6 runs) w/ the  Large Data Workflow (Advanced)","lvl2":"Large Data Workflow Overview"},"type":"lvl2","url":"/notebooks/features/too-much-data#large-data-workflow-overview","position":2},{"hierarchy":{"lvl1":"Setup Iterative OBC generation (for Long CESM-MOM6 runs) w/ the  Large Data Workflow (Advanced)","lvl2":"Large Data Workflow Overview"},"content":"The workflow is enabled by setting the too_much_data boolean in case.configure_forcings. This triggers the copying of a script folder into the case input directory forcing folder\nand the generation of a configuration file to download the required boundary condition files. An example of this is in CrocoGallery under features/add_data_products.ipynb.\nUsers can trigger the workflow by running driver.py in the forcing folder and adjusting config options in the config file.","type":"content","url":"/notebooks/features/too-much-data#large-data-workflow-overview","position":3},{"hierarchy":{"lvl1":"Setup Iterative OBC generation (for Long CESM-MOM6 runs) w/ the  Large Data Workflow (Advanced)","lvl3":"Folder Structure","lvl2":"Large Data Workflow Overview"},"type":"lvl3","url":"/notebooks/features/too-much-data#folder-structure","position":4},{"hierarchy":{"lvl1":"Setup Iterative OBC generation (for Long CESM-MOM6 runs) w/ the  Large Data Workflow (Advanced)","lvl3":"Folder Structure","lvl2":"Large Data Workflow Overview"},"content":"config.json ‚Äì Defines the region-specific requirements and run parameters.\n\nREADME ‚Äì Explains the workflow.\n\ndriver.py ‚Äì Executes all scripts needed to obtain OBC data.\n\nCode/ ‚Äì Contains all scripts used in the workflow.\n\nraw_data/, regridded_data/ ‚Äì Intermediate storage for workflow steps, preventing the need to rerun all scripts at once.","type":"content","url":"/notebooks/features/too-much-data#folder-structure","position":5},{"hierarchy":{"lvl1":"Setup Iterative OBC generation (for Long CESM-MOM6 runs) w/ the  Large Data Workflow (Advanced)","lvl3":"Scripts","lvl2":"Large Data Workflow Overview"},"type":"lvl3","url":"/notebooks/features/too-much-data#scripts","position":6},{"hierarchy":{"lvl1":"Setup Iterative OBC generation (for Long CESM-MOM6 runs) w/ the  Large Data Workflow (Advanced)","lvl3":"Scripts","lvl2":"Large Data Workflow Overview"},"content":"get_data_piecewise ‚Äì Retrieves raw, unprocessed data in chunks (size defined by config[\"params\"][\"step\"]) and saves it to config[\"raw_data\"].\n\nregrid_data_piecewise ‚Äì Processes raw data and stores it in config[\"regridded_data\"].\n\nmerge_piecewise_dataset ‚Äì Combines regridded data into the final dataset for model input.","type":"content","url":"/notebooks/features/too-much-data#scripts","position":7},{"hierarchy":{"lvl1":"Setup Iterative OBC generation (for Long CESM-MOM6 runs) w/ the  Large Data Workflow (Advanced)","lvl3":"How to Use","lvl2":"Large Data Workflow Overview"},"type":"lvl3","url":"/notebooks/features/too-much-data#how-to-use","position":8},{"hierarchy":{"lvl1":"Setup Iterative OBC generation (for Long CESM-MOM6 runs) w/ the  Large Data Workflow (Advanced)","lvl3":"How to Use","lvl2":"Large Data Workflow Overview"},"content":"Identify and allocate available computing resources.\n\nAdjust the step parameter to match resource constraints (default: 5 days).\n\nRun each step manually or use driver.py as a guide.\n\n","type":"content","url":"/notebooks/features/too-much-data#how-to-use","position":9},{"hierarchy":{"lvl1":"Setup Iterative OBC generation (for Long CESM-MOM6 runs) w/ the  Large Data Workflow (Advanced)","lvl3":"Step 1: Trigger Large Data Workflow","lvl2":"Large Data Workflow Overview"},"type":"lvl3","url":"/notebooks/features/too-much-data#step-1-trigger-large-data-workflow","position":10},{"hierarchy":{"lvl1":"Setup Iterative OBC generation (for Long CESM-MOM6 runs) w/ the  Large Data Workflow (Advanced)","lvl3":"Step 1: Trigger Large Data Workflow","lvl2":"Large Data Workflow Overview"},"content":"This can be done by setting the too_much_data bool to true.\n\ncase.configure_forcings(\n    date_range = [\"2020-01-01 00:00:00\", \"2020-01-09 00:00:00\"],\n    too_much_data = True\n)\n\n\n\n","type":"content","url":"/notebooks/features/too-much-data#step-1-trigger-large-data-workflow","position":11},{"hierarchy":{"lvl1":"Setup Iterative OBC generation (for Long CESM-MOM6 runs) w/ the  Large Data Workflow (Advanced)","lvl3":"Step 2: Run the iterative OBC processor","lvl2":"Large Data Workflow Overview"},"type":"lvl3","url":"/notebooks/features/too-much-data#step-2-run-the-iterative-obc-processor","position":12},{"hierarchy":{"lvl1":"Setup Iterative OBC generation (for Long CESM-MOM6 runs) w/ the  Large Data Workflow (Advanced)","lvl3":"Step 2: Run the iterative OBC processor","lvl2":"Large Data Workflow Overview"},"content":"In a terminal session, locate the large_data_workflow folder, which is put in the case input directory under the forcing folder, default called ‚Äúglorys/large_data_workflow‚Äù. Then, execute the driver.py to generate boundary conditions. It uses the config.json file to generate the OBCs in a piecewise format before merging. Modify the code as you see fit!\n\nEspecially consider adjusting the specific function being used in config.json to download the data. For example, On Derecho? Using the RDA reader. On a local computer? Use the python GLORYS api. You can change the function by changing the respective line in config.json.\n\n","type":"content","url":"/notebooks/features/too-much-data#step-2-run-the-iterative-obc-processor","position":13},{"hierarchy":{"lvl1":"Setup Iterative OBC generation (for Long CESM-MOM6 runs) w/ the  Large Data Workflow (Advanced)","lvl3":"Step 3: Process forcing data","lvl2":"Large Data Workflow Overview"},"type":"lvl3","url":"/notebooks/features/too-much-data#step-3-process-forcing-data","position":14},{"hierarchy":{"lvl1":"Setup Iterative OBC generation (for Long CESM-MOM6 runs) w/ the  Large Data Workflow (Advanced)","lvl3":"Step 3: Process forcing data","lvl2":"Large Data Workflow Overview"},"content":"In this final step, we call the process_forcings method of CrocoDash to interpolate the initial condition as well as all boundaries. CrocoDash also updates MOM6 runtime parameters and CESM xml variables accordingly. It will auto skip the OBCs because of the large data workflow.\n\ncase.process_forcings()\n\n","type":"content","url":"/notebooks/features/too-much-data#step-3-process-forcing-data","position":15},{"hierarchy":{"lvl1":"Calculate EKE for CARIB12"},"type":"lvl1","url":"/notebooks/projects/carib12-eke-ts","position":0},{"hierarchy":{"lvl1":"Calculate EKE for CARIB12"},"content":"import xarray as xr\nimport xgcm as xgcm\nimport numpy as np\nimport gsw\nimport cmocean as cm\nimport matplotlib.pyplot as plt\nimport cartopy\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n","type":"content","url":"/notebooks/projects/carib12-eke-ts","position":1},{"hierarchy":{"lvl1":"Calculate EKE for CARIB12","lvl2":"In this notebook..."},"type":"lvl2","url":"/notebooks/projects/carib12-eke-ts#in-this-notebook","position":2},{"hierarchy":{"lvl1":"Calculate EKE for CARIB12","lvl2":"In this notebook..."},"content":"You will examine output from the CARIB12 simulation or a CARIB12 simulation of your own. The notebook focuses n two simple analyses relevant to model development and validation: 1- Calculating and plotting Eddy Kinetic Energy to examine mesoscale variability of flow. 2- Plotting a Temperature-Salinity diagram to examine water mass properties.\n\nYou will either access the original CARIB12 simulation output or recreate your own CARIB12 simulation using CROCODASH.\n\nThere are several prompts along the notebook to complete the analysis with a comparisson to GLORYS reanlaysis data.\n\nYou can use the NPL 2025b environment for this notebook.\n\nSeijo-Ellis, G., Giglio, D., Marques, G., & Bryan, F. (2024). CARIB12: a regional Community Earth System Model/Modular Ocean Model 6 configuration of the Caribbean Sea. Geoscientific Model Development, 17(24), 8989-9021\n\n","type":"content","url":"/notebooks/projects/carib12-eke-ts#in-this-notebook","position":3},{"hierarchy":{"lvl1":"Calculate EKE for CARIB12","lvl2":"Inputs"},"type":"lvl2","url":"/notebooks/projects/carib12-eke-ts#inputs","position":4},{"hierarchy":{"lvl1":"Calculate EKE for CARIB12","lvl2":"Inputs"},"content":"if using the original CARIB12 output, you do not need to change any paths or file names in this notebook. If you have configured and completed your own CARIB12 simulation you will need to update the paths and filenmae accordingly.\n\n#--- Path to where the model output is located:\nparent_dir = '/glade/derecho/scratch/gseijo/FROM_CHEYENNE/CARIB_012.SMAG.001/run/'\n\n\n\n\n#--- load daily data for 1 month of simulation to calculate EKE:\nds = xr.open_dataset(parent_dir + 'CARIB_12.SMAG.001.mom6.sfc.2017-09.nc',decode_timedelta=True)\n\n\n\nds\n\n\n\n","type":"content","url":"/notebooks/projects/carib12-eke-ts#inputs","position":5},{"hierarchy":{"lvl1":"Calculate EKE for CARIB12","lvl2":"Eddy Kinetic Energy"},"type":"lvl2","url":"/notebooks/projects/carib12-eke-ts#eddy-kinetic-energy","position":6},{"hierarchy":{"lvl1":"Calculate EKE for CARIB12","lvl2":"Eddy Kinetic Energy"},"content":"We will calculate EKE using Reynold‚Äôs decomposition of the velocity field:EKE = (\\frac{1}{2})(u^{\\prime2} + v^{\\prime2}) u = \\overline{u} + u^{\\prime} and v = \\overline{v} + v^{\\prime},where \\overline{u} and \\overline{v} are the time averaged zonal and meridional velocities and, u^{\\prime} and  v^{\\prime} the corresponding anomalies.\n\n","type":"content","url":"/notebooks/projects/carib12-eke-ts#eddy-kinetic-energy","position":7},{"hierarchy":{"lvl1":"Calculate EKE for CARIB12","lvl3":"Remapping velocities","lvl2":"Eddy Kinetic Energy"},"type":"lvl3","url":"/notebooks/projects/carib12-eke-ts#remapping-velocities","position":8},{"hierarchy":{"lvl1":"Calculate EKE for CARIB12","lvl3":"Remapping velocities","lvl2":"Eddy Kinetic Energy"},"content":"On a Arakawa C-grid, the velocities are calculated on cell faces Here, we use xgcm to remap the velocities to the cell centers:\n\n#--- use xGCM to remap u,v to cell center:\ngrid = xgcm.Grid(ds, coords={'X': {'center': 'xh', 'outer': 'xq'},\n                         'Y': {'center': 'yh', 'outer': 'yq'},})\n\nds['u_c'] = grid.interp(ds['SSU'],'X',boundary='fill')\nds['v_c'] = grid.interp(ds['SSV'],'Y',boundary='fill')\n\n\n\nds\n\n\n\n","type":"content","url":"/notebooks/projects/carib12-eke-ts#remapping-velocities","position":9},{"hierarchy":{"lvl1":"Calculate EKE for CARIB12","lvl3":"Calculate EKE","lvl2":"Eddy Kinetic Energy"},"type":"lvl3","url":"/notebooks/projects/carib12-eke-ts#calculate-eke","position":10},{"hierarchy":{"lvl1":"Calculate EKE for CARIB12","lvl3":"Calculate EKE","lvl2":"Eddy Kinetic Energy"},"content":"\n\ndef calculate_eke(ds):\n    ''' this function calculates EKE. It assumes the input dataset has\n    both zonal (u_c) and meridional (v_c) velocities remapped to cell centers in it. It will output a data array with EKE.\n    Inputs: ds     --->  dataset with u_c and v_c variables.\n    Outputs: da_eke ---> EKE in a data array\n    '''\n    #--- get mean velocities:\n    u_ave = ds['u_c'].mean(dim='time')\n    v_ave = ds['v_c'].mean(dim='time')\n    #--- get anomalies:\n    u_anom = ds['u_c'] - u_ave\n    v_anom = ds['v_c'] - v_ave\n    #--- get EKE:\n    da_eke = 0.5 * (u_anom**2 + v_anom**2)\n\n    return da_eke\n    \n    \n\n\n\nEKE = calculate_eke(ds)\nEKE\n\n\n\n","type":"content","url":"/notebooks/projects/carib12-eke-ts#calculate-eke","position":11},{"hierarchy":{"lvl1":"Calculate EKE for CARIB12","lvl3":"Plot time mean EKE","lvl2":"Eddy Kinetic Energy"},"type":"lvl3","url":"/notebooks/projects/carib12-eke-ts#plot-time-mean-eke","position":12},{"hierarchy":{"lvl1":"Calculate EKE for CARIB12","lvl3":"Plot time mean EKE","lvl2":"Eddy Kinetic Energy"},"content":"\n\n#--- some plot params to play with:\nvmini = 0.0\nvmaxi = 0.15\ncolormap = 'turbo'\n\n#--- plot time mean EKE:\nfig, axs = plt.subplots(ncols = 1,figsize=(16, 5),subplot_kw=dict(projection=ccrs.PlateCarree()))\n\nc1 = EKE[:,:,:].mean(dim='time').plot.pcolormesh(vmin=vmini,vmax=vmaxi,cmap=colormap,transform=ccrs.PlateCarree(),add_colorbar = True,extend='max')\n\naxs.coastlines(color='k')\naxs.add_feature(cartopy.feature.LAND,color='gray')\naxs.set_facecolor('gray')\naxs.set_ylabel('')\naxs.set_xlabel('')\naxs.gridlines(color='k',linestyle=':',draw_labels=['left','bottom'], dms=True, x_inline=False, y_inline=False)\n_ = axs.set_title('Time mean EKE [$m^{2}/s^{2}$]',fontweight ='bold')\n\n\n\n\n","type":"content","url":"/notebooks/projects/carib12-eke-ts#plot-time-mean-eke","position":13},{"hierarchy":{"lvl1":"Calculate EKE for CARIB12","lvl3":"Prompt","lvl2":"Eddy Kinetic Energy"},"type":"lvl3","url":"/notebooks/projects/carib12-eke-ts#prompt","position":14},{"hierarchy":{"lvl1":"Calculate EKE for CARIB12","lvl3":"Prompt","lvl2":"Eddy Kinetic Energy"},"content":"Import GLORYS data for the same time period and calculate the mean EKE.The GLORYS data can be found here: /glade/campaign/collections/rda/data/d010049/ Alternatively, you can incorporate other datasets by leveraging other packages you have learned about this week.\n\nUse xESMF to regrid both datasets to a common grid and compare the time mean EKE between CARIB12 and GLORYS.\n\nWhat can you learn from comparing CARIB12 to the GLORYS reanalysis? How would it inform the development f the configuration?\n\n","type":"content","url":"/notebooks/projects/carib12-eke-ts#prompt","position":15},{"hierarchy":{"lvl1":"Calculate EKE for CARIB12","lvl2":"Temperature - Salinity Diagram"},"type":"lvl2","url":"/notebooks/projects/carib12-eke-ts#temperature-salinity-diagram","position":16},{"hierarchy":{"lvl1":"Calculate EKE for CARIB12","lvl2":"Temperature - Salinity Diagram"},"content":"if using the original CARIB12 output, you do not need to change any paths or file names in this notebook. If you have configured and completed your own CARIB12 simulation you will need to update the paths and filenmae accordingly.\n\n#--- load monthly mean 3D data of simulation to plot a T-S diagram:\nds = xr.open_dataset(parent_dir + 'CARIB_12.SMAG.001.mom6.hm.2017-09.nc',decode_timedelta=True)\nds\n\n\n\n","type":"content","url":"/notebooks/projects/carib12-eke-ts#temperature-salinity-diagram","position":17},{"hierarchy":{"lvl1":"Calculate EKE for CARIB12","lvl3":"Potential Density","lvl2":"Temperature - Salinity Diagram"},"type":"lvl3","url":"/notebooks/projects/carib12-eke-ts#potential-density","position":18},{"hierarchy":{"lvl1":"Calculate EKE for CARIB12","lvl3":"Potential Density","lvl2":"Temperature - Salinity Diagram"},"content":"\n\n#--- Get mean temperature and salinity profiles to simplify calculations for the demo:\nptemp = ds['thetao'].mean(dim='time').mean(dim=['xh','yh'])\nsalt  = ds['so'].mean(dim='time').mean(dim=['xh','yh'])\n\n\n\n","type":"content","url":"/notebooks/projects/carib12-eke-ts#potential-density","position":19},{"hierarchy":{"lvl1":"Calculate EKE for CARIB12","lvl3":"Prompt","lvl2":"Temperature - Salinity Diagram"},"type":"lvl3","url":"/notebooks/projects/carib12-eke-ts#prompt-1","position":20},{"hierarchy":{"lvl1":"Calculate EKE for CARIB12","lvl3":"Prompt","lvl2":"Temperature - Salinity Diagram"},"content":"The previous step does an area average without considering variation in the area of each cell. Repeat the previous step by implementing an area-weighted average computation.The following example could be applied to this notebook:weights = np.cos(np.deg2rad(data.lat))\nweights.name = \"weights\"\ndata_weighted = data.weighted(weights)\nweighted_mean = data_weighted.mean((\"lon\", \"lat\"))\n\nwhere ‚Äúdata‚Äù is a data array with dimensions ‚Äúlon‚Äù and ‚Äúlat‚Äù corresponding to longitudes and latitudes, respectively.\n\n#--- Compute sigma-theta (potential density anomaly, referenced to 0 dbar)\n#--- Need Absolute Salinity and Conservative Temperature for GSW\nTgrid = np.linspace(ptemp.min()-0.5, ptemp.max()+0.5, 50)\nSgrid = np.linspace(salt.min()-0.1, salt.max()+0.1, 50)\nSg, Tg = np.meshgrid(Sgrid, Tgrid)\n\nSA = gsw.SA_from_SP(Sg, p=0, lon=0, lat=0)\nCT = gsw.CT_from_pt(SA, Tg)\n\nsigma0 = gsw.sigma0(SA, CT) \n\n\n\n","type":"content","url":"/notebooks/projects/carib12-eke-ts#prompt-1","position":21},{"hierarchy":{"lvl1":"Calculate EKE for CARIB12","lvl3":"Plot TS diagram with sigma contours","lvl2":"Temperature - Salinity Diagram"},"type":"lvl3","url":"/notebooks/projects/carib12-eke-ts#plot-ts-diagram-with-sigma-contours","position":22},{"hierarchy":{"lvl1":"Calculate EKE for CARIB12","lvl3":"Plot TS diagram with sigma contours","lvl2":"Temperature - Salinity Diagram"},"content":"\n\nplt.figure(figsize=(6,6))\n\n# Density contours\ncs = plt.contour(Sg, Tg, sigma0, colors='gray', linewidths=0.7)\nplt.clabel(cs, inline=True, fontsize=8, fmt=\"%.1f\")\n\n# Time- and space-mean profile colored by depth\nsc = plt.scatter(salt, ptemp, c=ptemp['zl'], vmin=0,vmax=2000,cmap='turbo_r')\nplt.colorbar(sc, label='Depth [m]')\n\nplt.xlabel(\"Salinity [PSU]\")\nplt.ylabel(\"Potential Temperature [¬∞C]\")\nplt.title(\"Mean \" + r'$\\theta$' +\"-S Diagram\")\nplt.grid(True)\nplt.show()\n\n\n\n","type":"content","url":"/notebooks/projects/carib12-eke-ts#plot-ts-diagram-with-sigma-contours","position":23},{"hierarchy":{"lvl1":"Calculate EKE for CARIB12","lvl3":"Prompt","lvl2":"Temperature - Salinity Diagram"},"type":"lvl3","url":"/notebooks/projects/carib12-eke-ts#prompt-2","position":24},{"hierarchy":{"lvl1":"Calculate EKE for CARIB12","lvl3":"Prompt","lvl2":"Temperature - Salinity Diagram"},"content":"Repeat for GLORYS data and compare the vertical structure and water mass properties between the two datasets.\n\n","type":"content","url":"/notebooks/projects/carib12-eke-ts#prompt-2","position":25},{"hierarchy":{"lvl1":"Calculate EKE for CARIB12","lvl2":"Final Prompt"},"type":"lvl2","url":"/notebooks/projects/carib12-eke-ts#final-prompt","position":26},{"hierarchy":{"lvl1":"Calculate EKE for CARIB12","lvl2":"Final Prompt"},"content":"Leveraging what you‚Äôve learned throughout the workshop and this notebook, can you incorporate a similar analysis into CUPiD?Good luck!","type":"content","url":"/notebooks/projects/carib12-eke-ts#final-prompt","position":27},{"hierarchy":{"lvl1":"CrocoDash Workshop Project"},"type":"lvl1","url":"/notebooks/projects/crocodash","position":0},{"hierarchy":{"lvl1":"CrocoDash Workshop Project"},"content":"In this notebook, the code has the simplest regional ocean model setup possible through CrocoDash. In the comments and descriptions, we‚Äôll provide some of the ways you can add and build on this demo (recreate a specific domain - like GFDL‚Äôs NWA12, run with CICE - the sea-ice model in CESM, run with MARBL - the ocean BGC model in CESM, and beyond!)\n\nThis notebook is designed for new users of CrocoDash and CESM who want to set up a simple regional ocean model. By following the steps, you will learn how to generate a domain, configure a CESM case, prepare ocean forcing data, and run your simulation.\n\nA typical workflow of utilizing CrocoDash consists of four main steps:\n\nGenerate a regional MOM6 domain.\n\nCreate the CESM case.\n\nPrepare ocean forcing data.\n\nBuild and run the case.\n\nEvery specialization of CrocoDash will use this framework.\n\n","type":"content","url":"/notebooks/projects/crocodash","position":1},{"hierarchy":{"lvl1":"CrocoDash Workshop Project","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl2","url":"/notebooks/projects/crocodash#section-1-generate-a-regional-mom6-domain","position":2},{"hierarchy":{"lvl1":"CrocoDash Workshop Project","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"We begin by defining a regional MOM6 domain using CrocoDash. To do so, we first generate a horizontal grid. We then generate the topography by remapping an existing bathymetric dataset to our horizontal grid. Finally, we define a vertical grid.\n\n","type":"content","url":"/notebooks/projects/crocodash#section-1-generate-a-regional-mom6-domain","position":3},{"hierarchy":{"lvl1":"CrocoDash Workshop Project","lvl3":"Step 1.1: Horizontal Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl3","url":"/notebooks/projects/crocodash#step-1-1-horizontal-grid","position":4},{"hierarchy":{"lvl1":"CrocoDash Workshop Project","lvl3":"Step 1.1: Horizontal Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"\n\nfrom CrocoDash.grid import Grid\n\ngrid = Grid(\n  resolution = 0.05, # in degrees\n  xstart = 278.0, # min longitude in [0, 360]\n  lenx = 3.0, # longitude extent in degrees\n  ystart = 7.0, # min latitude in [-90, 90]\n  leny = 3.0, # latitude extent in degrees\n  name = \"panama1\",\n)\n\n# Try changing the grid resolution or domain boundaries to model a different region!\n\n\n\nThe above cell is a way to generate a rectangular lat/lon grid through CrocoDash\n\nIf you‚Äôre interested in using an already generated regional grid, you can do that by passing in a supergrid, like in \n\nthis demo.\n\nIf you‚Äôre interested in subsetting an already generated global grid, you can do that by passing in lat/lon args, like in \n\nthis demo.\n\n","type":"content","url":"/notebooks/projects/crocodash#step-1-1-horizontal-grid","position":5},{"hierarchy":{"lvl1":"CrocoDash Workshop Project","lvl3":"Step 1.2: Topography","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl3","url":"/notebooks/projects/crocodash#step-1-2-topography","position":6},{"hierarchy":{"lvl1":"CrocoDash Workshop Project","lvl3":"Step 1.2: Topography","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"The below three cells are a way to generate topography with a given grid using the \n\nGEBCO dataset.\n\nIf you‚Äôre interested in using a previously generated topography for your grid, you can do that by passing in a topography file, like in \n\nthis demo.\n\nfrom CrocoDash.topo import Topo\n\ntopo = Topo(\n    grid = grid,\n    min_depth = 9.5, # in meters\n)\n\n\n\nbathymetry_path='<GEBCO>'\n\ntopo.interpolate_from_file(\n    file_path = bathymetry_path,\n    longitude_coordinate_name=\"lon\",\n    latitude_coordinate_name=\"lat\",\n    vertical_coordinate_name=\"elevation\"\n)\n\n\n\n# Validate that the topography looks right!\ntopo.depth.plot()\n\n\n\n","type":"content","url":"/notebooks/projects/crocodash#step-1-2-topography","position":7},{"hierarchy":{"lvl1":"CrocoDash Workshop Project","lvl4":"Editing the topography","lvl3":"Step 1.2: Topography","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl4","url":"/notebooks/projects/crocodash#editing-the-topography","position":8},{"hierarchy":{"lvl1":"CrocoDash Workshop Project","lvl4":"Editing the topography","lvl3":"Step 1.2: Topography","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"Regridding topography from GEBCO may result in features we do not necessarily want. The Topo Editor is a tool to edit the bathymetry as wanted. You can do things like erasing basins, opening or closing bays, or adjusting the minimum depth. On JupyterHub, remember we need the ipympl extension, which can be installed as shown on \n\nthis slide.\n\n%matplotlib ipympl\nfrom CrocoDash.topo_editor import TopoEditor\nTopoEditor(topo)\n\n\n\n","type":"content","url":"/notebooks/projects/crocodash#editing-the-topography","position":9},{"hierarchy":{"lvl1":"CrocoDash Workshop Project","lvl3":"Step 1.3: Vertical Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl3","url":"/notebooks/projects/crocodash#step-1-3-vertical-grid","position":10},{"hierarchy":{"lvl1":"CrocoDash Workshop Project","lvl3":"Step 1.3: Vertical Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"The below cell generates a vertical grid using a hyperbolic function. We can also create a uniform grid instance, or use a previously generated vertical grid, like in \n\nthis demo.\n\nfrom CrocoDash.vgrid import VGrid\n\nvgrid  = VGrid.hyperbolic(\n    nk = 75, # number of vertical levels\n    depth = topo.max_depth,\n    ratio=20.0 # target ratio of top to bottom layer thicknesses\n)\n\n\n\nimport matplotlib.pyplot as plt\nplt.close()\n# Create the plot\nfor depth in vgrid.z:\n    plt.axhline(y=depth, linestyle='-')  # Horizontal lines\n\nplt.ylim(max(vgrid.z) + 10, min(vgrid.z) - 10)  # Invert y-axis so deeper values go down\nplt.ylabel(\"Depth\")\nplt.title(\"Vertical Grid\")\nplt.show()\n\n\n\n","type":"content","url":"/notebooks/projects/crocodash#step-1-3-vertical-grid","position":11},{"hierarchy":{"lvl1":"CrocoDash Workshop Project","lvl2":"SECTION 2: Create the CESM case"},"type":"lvl2","url":"/notebooks/projects/crocodash#section-2-create-the-cesm-case","position":12},{"hierarchy":{"lvl1":"CrocoDash Workshop Project","lvl2":"SECTION 2: Create the CESM case"},"content":"After generating the MOM6 domain, the next step is to create a CESM case using CrocoDash. This process is straightforward and involves instantiating the CrocoDash Case object. The Case object requires the following inputs:\n\nCESM Source Directory: A local path to a compatible CESM source copy.\n\nCase Name: A unique name for the CESM case.\n\nInput Directory: The directory where all necessary input files will be written.\n\nMOM6 Domain Objects: The horizontal grid, topography, and vertical grid created in the previous section.\n\nProject ID: (Optional) A project ID, if required by the machine.\n\nCompset: The set of models to be used in the Case. Standalone Ocean, Ocean-BGC, Ocean-Seaice, Ocean-Runoff, etc...\n\n","type":"content","url":"/notebooks/projects/crocodash#section-2-create-the-cesm-case","position":13},{"hierarchy":{"lvl1":"CrocoDash Workshop Project","lvl3":"Step 2.1: Specify case name and directories:","lvl2":"SECTION 2: Create the CESM case"},"type":"lvl3","url":"/notebooks/projects/crocodash#step-2-1-specify-case-name-and-directories","position":14},{"hierarchy":{"lvl1":"CrocoDash Workshop Project","lvl3":"Step 2.1: Specify case name and directories:","lvl2":"SECTION 2: Create the CESM case"},"content":"Begin by specifying the case name and the necessary directory paths. Ensure the CESM root directory points to your own local copy of CESM. Below is an example setup:\n\nfrom pathlib import Path\n\n\n\n# CESM case (experiment) name\ncasename = \"<casename>\"\n\n# CESM source root (Update this path accordingly!!!)\ncesmroot =\"/glade/work/<YOURUSERNAME>/CROCESM\"\n\n# Place where all your input files go \ninputdir = Path(\"/glade/work/<YOURUSERNAME>/crocodile_2025\") / \"croc_input\" / casename\n    \n# CESM case directory\ncaseroot = Path(\"/glade/work/<YOURUSERNAME>/crocodile_2025\") / \"croc_cases\" / casename\n\n\n\n","type":"content","url":"/notebooks/projects/crocodash#step-2-1-specify-case-name-and-directories","position":15},{"hierarchy":{"lvl1":"CrocoDash Workshop Project","lvl3":"Step 2.2: Create the Case","lvl2":"SECTION 2: Create the CESM case"},"type":"lvl3","url":"/notebooks/projects/crocodash#step-2-2-create-the-case","position":16},{"hierarchy":{"lvl1":"CrocoDash Workshop Project","lvl3":"Step 2.2: Create the Case","lvl2":"SECTION 2: Create the CESM case"},"content":"To create the CESM case, instantiate the Case object as shown below. This will automatically set up the CESM case based on the provided inputs: The cesmroot argument specifies the path to your local CESM source directory.\nThe caseroot argument defines the directory where the case will be created. CrocoDash will handle all necessary namelist modifications and XML changes to align with the MOM6 domain objects generated earlier.\n\nThis is where we can change the model setup to what we want:\n\nInterested in BGC? Check out \n\nthe demo for how to modify this notebook for BGC.\n\nInterested in CICE? Check out \n\nthe demo for how to modify this notebook for CICE.\n\nInterested in Data Runoff from GLOFAS (or JRA)? Check out \n\nthe demo for how to modify this notebook for Data Runoff.\n\nfrom CrocoDash.case import Case\n\ncase = Case(\n    cesmroot = cesmroot,\n    caseroot = caseroot,\n    inputdir = inputdir,\n    ocn_grid = grid,\n    ocn_vgrid = vgrid,\n    ocn_topo = topo,\n    project = 'CESM0030',\n    override = True,\n    machine = \"derecho\",\n    compset = \"CR_JRA\" # Change this as necessary to include whatever models you want. Use the entire compset: BGC? change MOM6 to MOM6%MARBL-BIO, CICE? Change SICE to CICE, GLOFAS Runoff? Change SROF to DROF%GLOFAS\n)\n\n\n\n","type":"content","url":"/notebooks/projects/crocodash#step-2-2-create-the-case","position":17},{"hierarchy":{"lvl1":"CrocoDash Workshop Project","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl2","url":"/notebooks/projects/crocodash#section-3-prepare-ocean-forcing-data","position":18},{"hierarchy":{"lvl1":"CrocoDash Workshop Project","lvl2":"Section 3: Prepare ocean forcing data"},"content":"We need to cut out our ocean forcing. The package expects an initial condition and one time-dependent segment per non-land boundary.\n\nIn this notebook, we are forcing with the Copernicus Marine ‚ÄúGlorys‚Äù reanalysis dataset.","type":"content","url":"/notebooks/projects/crocodash#section-3-prepare-ocean-forcing-data","position":19},{"hierarchy":{"lvl1":"CrocoDash Workshop Project","lvl3":"Step 3.1 Configure Initial Conditions and Forcings","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl3","url":"/notebooks/projects/crocodash#step-3-1-configure-initial-conditions-and-forcings","position":20},{"hierarchy":{"lvl1":"CrocoDash Workshop Project","lvl3":"Step 3.1 Configure Initial Conditions and Forcings","lvl2":"Section 3: Prepare ocean forcing data"},"content":"Interested in adding tides? Check out \n\nthe demo.\n\nInterested in adding chlorophyll? Check out \n\nthe demo.\n\nIs the GLORYS data access function not working? Check out alternative options in \n\nthis demo.\n\nConfigure Forcings MUST be modified if BGC or Data Runoff are part of the compset (and will throw an error). Check out the demos to see the changes required: \n\nBGC Demo, \n\nDROF Demo.\n\ncase.configure_forcings(\n    date_range = [\"2020-01-01 00:00:00\", \"2020-01-06 00:00:00\"],\n    product_name = \"GLORYS\",\n    function_name=\"get_glorys_data_from_rda\"\n)\n\n\n\n","type":"content","url":"/notebooks/projects/crocodash#step-3-1-configure-initial-conditions-and-forcings","position":21},{"hierarchy":{"lvl1":"CrocoDash Workshop Project","lvl3":"Step 3.3: Process forcing data","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl3","url":"/notebooks/projects/crocodash#step-3-3-process-forcing-data","position":22},{"hierarchy":{"lvl1":"CrocoDash Workshop Project","lvl3":"Step 3.3: Process forcing data","lvl2":"Section 3: Prepare ocean forcing data"},"content":"In this final step, we call the process_forcings method of CrocoDash to cut out and interpolate the initial condition as well as all boundaries. CrocoDash also updates MOM6 runtime parameters and CESM xml variables accordingly.\n\ncase.process_forcings() # You can turn off specific processing if it's already been processed previously with process_*name* = False. Ex. case.process_forcings(process_chl = False)\n\n\n\n","type":"content","url":"/notebooks/projects/crocodash#step-3-3-process-forcing-data","position":23},{"hierarchy":{"lvl1":"CrocoDash Workshop Project","lvl2":"Section 4: Build and run the case"},"type":"lvl2","url":"/notebooks/projects/crocodash#section-4-build-and-run-the-case","position":24},{"hierarchy":{"lvl1":"CrocoDash Workshop Project","lvl2":"Section 4: Build and run the case"},"content":"After completing the previous steps, you are ready to build and run your CESM case. Begin by navigating to the case root directory specified during the case creation. Before proceeding, review the user_nl_mom file located in the case directory. This file contains MOM6 parameter settings that were automatically generated by CrocoDash. Carefully examine these parameters and make any necessary adjustments to fine-tune the model for your specific requirements. While CrocoDash aims to provide a solid starting point, further tuning and adjustments are typically necessary to improve the model for your use case.\n\nOnce you have reviewed and modified the parameters as needed, you can build and execute the case using the following commands:qcmd -- ./case.build\n./case.submit\n\n","type":"content","url":"/notebooks/projects/crocodash#section-4-build-and-run-the-case","position":25},{"hierarchy":{"lvl1":"CrocoDash Workshop Project","lvl2":"Where to go from here?"},"type":"lvl2","url":"/notebooks/projects/crocodash#where-to-go-from-here","position":26},{"hierarchy":{"lvl1":"CrocoDash Workshop Project","lvl2":"Where to go from here?"},"content":"This project covered the basics of CrocoDash. Feel free to modify it in any of the ways described above ‚Äî for example, adding biogeochemistry (BGC), river forcing (DROF), or tides ‚Äî to set up your own regional domain of interest.\n\nMore interested in exploring model output? Feel free to check out the CUPiD or CARIB12 projects, which go over basic and advanced diagnostics (respectively) of regional MOM6 simulations. There‚Äôs also a 10-month dataset for a northwest Atlantic domain used by CUPiD that‚Äôs available for analysis.\n\nInterested in moving into advanced applications of regional modeling? We‚Äôd love your help! CrocoDash may not yet have all the features you need - so please work with us! We would love to have YOU as a contributor! Even if you don‚Äôt prefer to code, we would appreciate raising any feedback you have.\n\nOur philosphy is simple - Make the setup of regional domains as easy as possible (no matter the complexity), and take on as much of the software burden as we can. For now, that means:\n\nWe want your feedback! contributions & issues are always welcome\n\nCrocoDash is designed to be an easy framework to dive into the coding aspects (Want to modify boundary conditions? Adjust how the cholorphyll file is generated?) You can easily plug into this framework and modify the tools you need for your regional domain.","type":"content","url":"/notebooks/projects/crocodash#where-to-go-from-here","position":27},{"hierarchy":{"lvl1":"Model-Observations Comparison"},"type":"lvl1","url":"/notebooks/projects/model-obs-comparison-2","position":0},{"hierarchy":{"lvl1":"Model-Observations Comparison"},"content":"These tutorials and projects all use \n\nCrocoCamp, and are expected to run on NCAR‚Äôs Casper machine. If you want to run them outside of Casper, see \n\nbelow.","type":"content","url":"/notebooks/projects/model-obs-comparison-2","position":1},{"hierarchy":{"lvl1":"Model-Observations Comparison","lvl2":"Installing CrocoCamp"},"type":"lvl2","url":"/notebooks/projects/model-obs-comparison-2#installing-crococamp","position":2},{"hierarchy":{"lvl1":"Model-Observations Comparison","lvl2":"Installing CrocoCamp"},"content":"To install CrocoCamp, open a terminal session and rungit clone https://github.com/CROCODILE-CESM/CrocoCamp.git --branch 2025-Crocodile-Workshop --single-branch\ncd CrocoCamp\n./install.sh\nconda activate crococamp-2025\n\nNote that the \n\nCrocoLake‚Äôs project does not need CrocoCamp: CrocoLake is a dataset, not a python package, and installing dask, cartopy, and matplotlib is sufficient to read it and make maps. However, CrocoCamp comes with those packages, so if you have installed it already during the workshop, you can directly use the crococamp-2025 environment to run CrocoLake‚Äôs project too.","type":"content","url":"/notebooks/projects/model-obs-comparison-2#installing-crococamp","position":3},{"hierarchy":{"lvl1":"Model-Observations Comparison","lvl2":"CrocoCamp outside of Casper"},"type":"lvl2","url":"/notebooks/projects/model-obs-comparison-2#crococamp-outside-of-casper","position":4},{"hierarchy":{"lvl1":"Model-Observations Comparison","lvl2":"CrocoCamp outside of Casper"},"content":"CrocoCamp uses DART under the hood, and for this workshop we have set up a DART executable on Casper. If you need to run CrocoCamp outside of Casper, you basically need to compile DART on the machine where you want to run it.","type":"content","url":"/notebooks/projects/model-obs-comparison-2#crococamp-outside-of-casper","position":5},{"hierarchy":{"lvl1":"Model-Observations Comparison","lvl3":"On Derecho","lvl2":"CrocoCamp outside of Casper"},"type":"lvl3","url":"/notebooks/projects/model-obs-comparison-2#on-derecho","position":6},{"hierarchy":{"lvl1":"Model-Observations Comparison","lvl3":"On Derecho","lvl2":"CrocoCamp outside of Casper"},"content":"Make sure the following modules are loaded (use module load to load them):\n\nncarenv\n\ncraype\n\nintel\n\nncarcompilers\n\ncray-mpich\n\nhdf5\n\nnetcdf\n\nBuild DART (I suggest in the work folder):cd $WORK/\ngit clone https://github.com/CROCODILE-CESM/DART.git DART-derecho\ncd DART-derecho\ncd build_templates\ncp mkmf.template.intel.linux mkmf.template\ncd ../models/MOM6/work\n./quickbuild.sh\n\nhttps://‚Äãgithub‚Äã.com‚Äã/CROCODILE‚Äã-CESM‚Äã/DART‚Äã.git contains the latest developments made in the CROCODILE framework. They will be merged in the \n\nofficial DART when relevant.\n\nYou can now set up your config.yaml files to point to the correct path, replacing:perfect_model_obs_dir: /path/to/DART/models/MOM6/work/\n\nwithperfect_model_obs_dir: $WORK/DART-derecho/models/MOM6/work/\n\nor the path you chose.","type":"content","url":"/notebooks/projects/model-obs-comparison-2#on-derecho","position":7},{"hierarchy":{"lvl1":"Model-Observations Comparison","lvl3":"On your machine","lvl2":"CrocoCamp outside of Casper"},"type":"lvl3","url":"/notebooks/projects/model-obs-comparison-2#on-your-machine","position":8},{"hierarchy":{"lvl1":"Model-Observations Comparison","lvl3":"On your machine","lvl2":"CrocoCamp outside of Casper"},"content":"Installing DART differs from machine to machine and we refer you to DART‚Äôs installation guide.\n\nOnce you installed DART succesfully, you can set perfect_model_obs_dir in your config file to point to models/MOM6/work/ from the DART root directory.","type":"content","url":"/notebooks/projects/model-obs-comparison-2#on-your-machine","position":9},{"hierarchy":{"lvl1":"Model-Observations Comparison"},"type":"lvl1","url":"/notebooks/projects/model-obs-comparison-2","position":0},{"hierarchy":{"lvl1":"Model-Observations Comparison"},"content":"These tutorials and projects all use \n\nCrocoCamp, and are expected to run on NCAR‚Äôs Casper machine. If you want to run them outside of Casper, see \n\nbelow.","type":"content","url":"/notebooks/projects/model-obs-comparison-2","position":1},{"hierarchy":{"lvl1":"Model-Observations Comparison","lvl2":"Installing CrocoCamp"},"type":"lvl2","url":"/notebooks/projects/model-obs-comparison-2#installing-crococamp","position":2},{"hierarchy":{"lvl1":"Model-Observations Comparison","lvl2":"Installing CrocoCamp"},"content":"To install CrocoCamp, open a terminal session and rungit clone https://github.com/CROCODILE-CESM/CrocoCamp.git --branch 2025-Crocodile-Workshop --single-branch\ncd CrocoCamp\n./install.sh\nconda activate crococamp-2025\n\nNote that the \n\nCrocoLake‚Äôs project does not need CrocoCamp: CrocoLake is a dataset, not a python package, and installing dask, cartopy, and matplotlib is sufficient to read it and make maps. However, CrocoCamp comes with those packages, so if you have installed it already during the workshop, you can directly use the crococamp-2025 environment to run CrocoLake‚Äôs project too.","type":"content","url":"/notebooks/projects/model-obs-comparison-2#installing-crococamp","position":3},{"hierarchy":{"lvl1":"Model-Observations Comparison","lvl2":"CrocoCamp outside of Casper"},"type":"lvl2","url":"/notebooks/projects/model-obs-comparison-2#crococamp-outside-of-casper","position":4},{"hierarchy":{"lvl1":"Model-Observations Comparison","lvl2":"CrocoCamp outside of Casper"},"content":"CrocoCamp uses DART under the hood, and for this workshop we have set up a DART executable on Casper. If you need to run CrocoCamp outside of Casper, you basically need to compile DART on the machine where you want to run it.","type":"content","url":"/notebooks/projects/model-obs-comparison-2#crococamp-outside-of-casper","position":5},{"hierarchy":{"lvl1":"Model-Observations Comparison","lvl3":"On Derecho","lvl2":"CrocoCamp outside of Casper"},"type":"lvl3","url":"/notebooks/projects/model-obs-comparison-2#on-derecho","position":6},{"hierarchy":{"lvl1":"Model-Observations Comparison","lvl3":"On Derecho","lvl2":"CrocoCamp outside of Casper"},"content":"Make sure the following modules are loaded (use module load to load them):\n\nncarenv\n\ncraype\n\nintel\n\nncarcompilers\n\ncray-mpich\n\nhdf5\n\nnetcdf\n\nBuild DART (I suggest in the work folder):cd $WORK/\ngit clone https://github.com/CROCODILE-CESM/DART.git DART-derecho\ncd DART-derecho\ncd build_templates\ncp mkmf.template.intel.linux mkmf.template\ncd ../models/MOM6/work\n./quickbuild.sh\n\nhttps://‚Äãgithub‚Äã.com‚Äã/CROCODILE‚Äã-CESM‚Äã/DART‚Äã.git contains the latest developments made in the CROCODILE framework. They will be merged in the \n\nofficial DART when relevant.\n\nYou can now set up your config.yaml files to point to the correct path, replacing:perfect_model_obs_dir: /path/to/DART/models/MOM6/work/\n\nwithperfect_model_obs_dir: $WORK/DART-derecho/models/MOM6/work/\n\nor the path you chose.","type":"content","url":"/notebooks/projects/model-obs-comparison-2#on-derecho","position":7},{"hierarchy":{"lvl1":"Model-Observations Comparison","lvl3":"On your machine","lvl2":"CrocoCamp outside of Casper"},"type":"lvl3","url":"/notebooks/projects/model-obs-comparison-2#on-your-machine","position":8},{"hierarchy":{"lvl1":"Model-Observations Comparison","lvl3":"On your machine","lvl2":"CrocoCamp outside of Casper"},"content":"Installing DART differs from machine to machine and we refer you to DART‚Äôs installation guide.\n\nOnce you installed DART succesfully, you can set perfect_model_obs_dir in your config file to point to models/MOM6/work/ from the DART root directory.","type":"content","url":"/notebooks/projects/model-obs-comparison-2#on-your-machine","position":9},{"hierarchy":{"lvl1":"Model-Observations Comparison"},"type":"lvl1","url":"/notebooks/projects/model-obs-comparison-2","position":0},{"hierarchy":{"lvl1":"Model-Observations Comparison"},"content":"These tutorials and projects all use \n\nCrocoCamp, and are expected to run on NCAR‚Äôs Casper machine. If you want to run them outside of Casper, see \n\nbelow.","type":"content","url":"/notebooks/projects/model-obs-comparison-2","position":1},{"hierarchy":{"lvl1":"Model-Observations Comparison","lvl2":"Installing CrocoCamp"},"type":"lvl2","url":"/notebooks/projects/model-obs-comparison-2#installing-crococamp","position":2},{"hierarchy":{"lvl1":"Model-Observations Comparison","lvl2":"Installing CrocoCamp"},"content":"To install CrocoCamp, open a terminal session and rungit clone https://github.com/CROCODILE-CESM/CrocoCamp.git --branch 2025-Crocodile-Workshop --single-branch\ncd CrocoCamp\n./install.sh\nconda activate crococamp-2025\n\nNote that the \n\nCrocoLake‚Äôs project does not need CrocoCamp: CrocoLake is a dataset, not a python package, and installing dask, cartopy, and matplotlib is sufficient to read it and make maps. However, CrocoCamp comes with those packages, so if you have installed it already during the workshop, you can directly use the crococamp-2025 environment to run CrocoLake‚Äôs project too.","type":"content","url":"/notebooks/projects/model-obs-comparison-2#installing-crococamp","position":3},{"hierarchy":{"lvl1":"Model-Observations Comparison","lvl2":"CrocoCamp outside of Casper"},"type":"lvl2","url":"/notebooks/projects/model-obs-comparison-2#crococamp-outside-of-casper","position":4},{"hierarchy":{"lvl1":"Model-Observations Comparison","lvl2":"CrocoCamp outside of Casper"},"content":"CrocoCamp uses DART under the hood, and for this workshop we have set up a DART executable on Casper. If you need to run CrocoCamp outside of Casper, you basically need to compile DART on the machine where you want to run it.","type":"content","url":"/notebooks/projects/model-obs-comparison-2#crococamp-outside-of-casper","position":5},{"hierarchy":{"lvl1":"Model-Observations Comparison","lvl3":"On Derecho","lvl2":"CrocoCamp outside of Casper"},"type":"lvl3","url":"/notebooks/projects/model-obs-comparison-2#on-derecho","position":6},{"hierarchy":{"lvl1":"Model-Observations Comparison","lvl3":"On Derecho","lvl2":"CrocoCamp outside of Casper"},"content":"Make sure the following modules are loaded (use module load to load them):\n\nncarenv\n\ncraype\n\nintel\n\nncarcompilers\n\ncray-mpich\n\nhdf5\n\nnetcdf\n\nBuild DART (I suggest in the work folder):cd $WORK/\ngit clone https://github.com/CROCODILE-CESM/DART.git DART-derecho\ncd DART-derecho\ncd build_templates\ncp mkmf.template.intel.linux mkmf.template\ncd ../models/MOM6/work\n./quickbuild.sh\n\nhttps://‚Äãgithub‚Äã.com‚Äã/CROCODILE‚Äã-CESM‚Äã/DART‚Äã.git contains the latest developments made in the CROCODILE framework. They will be merged in the \n\nofficial DART when relevant.\n\nYou can now set up your config.yaml files to point to the correct path, replacing:perfect_model_obs_dir: /path/to/DART/models/MOM6/work/\n\nwithperfect_model_obs_dir: $WORK/DART-derecho/models/MOM6/work/\n\nor the path you chose.","type":"content","url":"/notebooks/projects/model-obs-comparison-2#on-derecho","position":7},{"hierarchy":{"lvl1":"Model-Observations Comparison","lvl3":"On your machine","lvl2":"CrocoCamp outside of Casper"},"type":"lvl3","url":"/notebooks/projects/model-obs-comparison-2#on-your-machine","position":8},{"hierarchy":{"lvl1":"Model-Observations Comparison","lvl3":"On your machine","lvl2":"CrocoCamp outside of Casper"},"content":"Installing DART differs from machine to machine and we refer you to DART‚Äôs installation guide.\n\nOnce you installed DART succesfully, you can set perfect_model_obs_dir in your config file to point to models/MOM6/work/ from the DART root directory.","type":"content","url":"/notebooks/projects/model-obs-comparison-2#on-your-machine","position":9},{"hierarchy":{"lvl1":"Model-Observation comparison: use your own MOM6 run"},"type":"lvl1","url":"/notebooks/projects/model-obs-your-own-model-1","position":0},{"hierarchy":{"lvl1":"Model-Observation comparison: use your own MOM6 run"},"content":"This notebooks is a stripped down version of CrocoCamp‚Äôs Tutorial 1 notebook, and you can use it as starting point for a model-obs comparison between your own CESM-MOM6 run and CrocoLake or WOD.","type":"content","url":"/notebooks/projects/model-obs-your-own-model-1","position":1},{"hierarchy":{"lvl1":"Model-Observation comparison: use your own MOM6 run","lvl3":"Installing CrocoCamp"},"type":"lvl3","url":"/notebooks/projects/model-obs-your-own-model-1#installing-crococamp","position":2},{"hierarchy":{"lvl1":"Model-Observation comparison: use your own MOM6 run","lvl3":"Installing CrocoCamp"},"content":"If you don‚Äôt have CrocoCamp set up yet, \n\nhere are the instructions to install it on NCAR‚Äôs HPC. If you have any issues or need to install it on a different machine, please contact \n\nenrico‚Äã.milanese@whoi‚Äã.edu.\n\n","type":"content","url":"/notebooks/projects/model-obs-your-own-model-1#installing-crococamp","position":3},{"hierarchy":{"lvl1":"Model-Observation comparison: use your own MOM6 run","lvl2":"CrocoLake observation sequence files"},"type":"lvl2","url":"/notebooks/projects/model-obs-your-own-model-1#crocolake-observation-sequence-files","position":4},{"hierarchy":{"lvl1":"Model-Observation comparison: use your own MOM6 run","lvl2":"CrocoLake observation sequence files"},"content":"If you want to use CrocoLake, generate your own observation files by adapting the code below (NB: some fields are intentionally left blank and the cell returns errors if you do not change it as you need).\n\ncrocolake_path = '/glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/CrocoCamp/CrocoLakePHY'\n\nimport datetime\nimport os\nfrom convert_crocolake_obs import ObsSequence\nbasename = \"myCL_obs_seq_\"\noutdir = \"$WORK/crocodile_2025/CrocoCamp/my_project/in_CL/\"\nbasename = os.path.expandvars(outdir+basename)\noutdir = os.path.expandvars(outdir)\nif not os.path.exists(outdir):\n    os.makedirs(outdir, exist_ok=True)\n\n# define horizontal region\nLAT0 = 5\nLAT1 = 60\nLON0 = -100\nLON1 = -30\n\n# define depth in dbar\nPRES0 = \nPRES1 =\n\n# define variables to import from CrocoLake\nselected_variables = [\n    \"DB_NAME\",  # ARGO, GLODAP, SprayGliders, OleanderXBT, Saildrones\n    \"JULD\", # this contains timestamp\n    \"LATITUDE\",\n    \"LONGITUDE\",\n    \"PRES\",\n    \"TEMP\",\n    \"PRES_QC\",\n    \"TEMP_QC\",\n    \"PRES_ERROR\",\n    \"TEMP_ERROR\",\n    \"PSAL\",\n    \"PSAL_QC\",\n    \"PSAL_ERROR\"\n]\n\n# month and year are constant in out case\nyear0 = \nmonth0 = \nN =\n\n# we loop to generate one file per day\nfor j in range(N):\n\n    # set date range\n    day0 = 1+j\n    day1 = day0+1\n    date0 = datetime.datetime(year0, month0, day0, 0, 0, 0)\n    date1 = datetime.datetime(year0, month0, day1, 0, 0, 0)\n    print(f\"Converting obs between {date0} and {date1}\")\n\n    # this defines AND filters, i.e. we want to load each observation that has latitude within the given range AND longitude within the given range, etc.\n    # to exclude NaNs, impose a range to a variable\n    and_filters = (\n        (\"LATITUDE\",'>',LAT0),  (\"LATITUDE\",'<',LAT1),\n        (\"LONGITUDE\",'>',LON0), (\"LONGITUDE\",'<',LON1),\n        (\"PRES\",'>',PRES0), (\"PRES\",'<',PRES1),\n        (\"JULD\",\">\",date0), (\"JULD\",\"<\",date1)\n    )\n\n    # this adds OR conditions to the and_filters, i.e. we want to load all observations that statisfy the AND conditions above, AND that have finite salinity OR temperature values\n    db_filters = [\n        list(and_filters) + [(\"PSAL\", \">\", -1e30), (\"PSAL\", \"<\", 1e30)],\n        list(and_filters) + [(\"TEMP\", \">\", -1e30), (\"TEMP\", \"<\", 1e30)],\n    ]\n\n    # generate output filename\n    obs_seq_out = basename + f\".{year0}{month0:02d}{day0:02d}.out\"\n\n    # generate obs_seq.in file\n    obsSeq = ObsSequence(\n        crocolake_path,\n        selected_variables,\n        db_filters,\n        obs_seq_out=obs_seq_out,\n        loose=True\n    )\n    obsSeq.write_obs_seq()\n\n\n\n","type":"content","url":"/notebooks/projects/model-obs-your-own-model-1#crocolake-observation-sequence-files","position":5},{"hierarchy":{"lvl1":"Model-Observation comparison: use your own MOM6 run","lvl2":"The configuration file"},"type":"lvl2","url":"/notebooks/projects/model-obs-your-own-model-1#the-configuration-file","position":6},{"hierarchy":{"lvl1":"Model-Observation comparison: use your own MOM6 run","lvl2":"The configuration file"},"content":"Remember to generate a new config file for your workflow:\n\n!cp ../configs/config_template.yaml ../configs/config_my_workflow.yaml\n\n\n\nand to customize the paths as needed.","type":"content","url":"/notebooks/projects/model-obs-your-own-model-1#the-configuration-file","position":7},{"hierarchy":{"lvl1":"Model-Observation comparison: use your own MOM6 run","lvl2":"Running the workflow"},"type":"lvl2","url":"/notebooks/projects/model-obs-your-own-model-1#running-the-workflow","position":8},{"hierarchy":{"lvl1":"Model-Observation comparison: use your own MOM6 run","lvl2":"Running the workflow"},"content":"Generate and run the workflow from your config file. Remember that you need to be running this notebook on Casper, as the configuration file points to DART‚Äôs installation on that machine.\n\nfrom crococamp.workflows import WorkflowModelObs\n\n# Create and run workflow to interpolate MOM6 model onto World Ocean Database obs space\nmy_workflow = WorkflowModelObs.from_config_file('../configs/config_my_workflow.yaml')\nmy_workflow.run() #use flag clear_output=True if you want to re-run it and automatically clean all previous output\n\n\n\n","type":"content","url":"/notebooks/projects/model-obs-your-own-model-1#running-the-workflow","position":9},{"hierarchy":{"lvl1":"Model-Observation comparison: use your own MOM6 run","lvl2":"Displaying the interactive map"},"type":"lvl2","url":"/notebooks/projects/model-obs-your-own-model-1#displaying-the-interactive-map","position":10},{"hierarchy":{"lvl1":"Model-Observation comparison: use your own MOM6 run","lvl2":"Displaying the interactive map"},"content":"Load and explore the data using pandas or dask:\n\ngood_model_obs_df = workflow_float.get_good_model_obs_df(compute=True)  # compute=True triggers the compute of the dask dataframe, returning a pandas dataframe with data loaded in memory\ngood_model_obs_df.head()                                                # displays first 5 rows in the dataframe\n\n\n\nLoad the interactive map:\n\nfrom crococamp.viz import InteractiveWidgetMap\n# Create an interactive map widget to visualize model-observation comparisons\n# The widget provides controls for selecting variables, observation types, and time ranges\nwidget = InteractiveWidgetMap(dgood_model_obs_f)\nwidget.setup()\n\n","type":"content","url":"/notebooks/projects/model-obs-your-own-model-1#displaying-the-interactive-map","position":11},{"hierarchy":{"lvl1":"Model-Observation comparison: use your own MOM6 run"},"type":"lvl1","url":"/notebooks/projects/model-obs-your-own-model-1","position":0},{"hierarchy":{"lvl1":"Model-Observation comparison: use your own MOM6 run"},"content":"This notebooks is a stripped down version of CrocoCamp‚Äôs Tutorial 1 notebook, and you can use it as starting point for a model-obs comparison between your own CESM-MOM6 run and CrocoLake or WOD.","type":"content","url":"/notebooks/projects/model-obs-your-own-model-1","position":1},{"hierarchy":{"lvl1":"Model-Observation comparison: use your own MOM6 run","lvl3":"Installing CrocoCamp"},"type":"lvl3","url":"/notebooks/projects/model-obs-your-own-model-1#installing-crococamp","position":2},{"hierarchy":{"lvl1":"Model-Observation comparison: use your own MOM6 run","lvl3":"Installing CrocoCamp"},"content":"If you don‚Äôt have CrocoCamp set up yet, \n\nhere are the instructions to install it on NCAR‚Äôs HPC. If you have any issues or need to install it on a different machine, please contact \n\nenrico‚Äã.milanese@whoi‚Äã.edu.\n\n","type":"content","url":"/notebooks/projects/model-obs-your-own-model-1#installing-crococamp","position":3},{"hierarchy":{"lvl1":"Model-Observation comparison: use your own MOM6 run","lvl2":"CrocoLake observation sequence files"},"type":"lvl2","url":"/notebooks/projects/model-obs-your-own-model-1#crocolake-observation-sequence-files","position":4},{"hierarchy":{"lvl1":"Model-Observation comparison: use your own MOM6 run","lvl2":"CrocoLake observation sequence files"},"content":"If you want to use CrocoLake, generate your own observation files by adapting the code below (NB: some fields are intentionally left blank and the cell returns errors if you do not change it as you need).\n\ncrocolake_path = '/glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/CrocoCamp/CrocoLakePHY'\n\nimport datetime\nimport os\nfrom convert_crocolake_obs import ObsSequence\nbasename = \"myCL_obs_seq_\"\noutdir = \"$WORK/crocodile_2025/CrocoCamp/my_project/in_CL/\"\nbasename = os.path.expandvars(outdir+basename)\noutdir = os.path.expandvars(outdir)\nif not os.path.exists(outdir):\n    os.makedirs(outdir, exist_ok=True)\n\n# define horizontal region\nLAT0 = 5\nLAT1 = 60\nLON0 = -100\nLON1 = -30\n\n# define depth in dbar\nPRES0 = \nPRES1 =\n\n# define variables to import from CrocoLake\nselected_variables = [\n    \"DB_NAME\",  # ARGO, GLODAP, SprayGliders, OleanderXBT, Saildrones\n    \"JULD\", # this contains timestamp\n    \"LATITUDE\",\n    \"LONGITUDE\",\n    \"PRES\",\n    \"TEMP\",\n    \"PRES_QC\",\n    \"TEMP_QC\",\n    \"PRES_ERROR\",\n    \"TEMP_ERROR\",\n    \"PSAL\",\n    \"PSAL_QC\",\n    \"PSAL_ERROR\"\n]\n\n# month and year are constant in out case\nyear0 = \nmonth0 = \nN =\n\n# we loop to generate one file per day\nfor j in range(N):\n\n    # set date range\n    day0 = 1+j\n    day1 = day0+1\n    date0 = datetime.datetime(year0, month0, day0, 0, 0, 0)\n    date1 = datetime.datetime(year0, month0, day1, 0, 0, 0)\n    print(f\"Converting obs between {date0} and {date1}\")\n\n    # this defines AND filters, i.e. we want to load each observation that has latitude within the given range AND longitude within the given range, etc.\n    # to exclude NaNs, impose a range to a variable\n    and_filters = (\n        (\"LATITUDE\",'>',LAT0),  (\"LATITUDE\",'<',LAT1),\n        (\"LONGITUDE\",'>',LON0), (\"LONGITUDE\",'<',LON1),\n        (\"PRES\",'>',PRES0), (\"PRES\",'<',PRES1),\n        (\"JULD\",\">\",date0), (\"JULD\",\"<\",date1)\n    )\n\n    # this adds OR conditions to the and_filters, i.e. we want to load all observations that statisfy the AND conditions above, AND that have finite salinity OR temperature values\n    db_filters = [\n        list(and_filters) + [(\"PSAL\", \">\", -1e30), (\"PSAL\", \"<\", 1e30)],\n        list(and_filters) + [(\"TEMP\", \">\", -1e30), (\"TEMP\", \"<\", 1e30)],\n    ]\n\n    # generate output filename\n    obs_seq_out = basename + f\".{year0}{month0:02d}{day0:02d}.out\"\n\n    # generate obs_seq.in file\n    obsSeq = ObsSequence(\n        crocolake_path,\n        selected_variables,\n        db_filters,\n        obs_seq_out=obs_seq_out,\n        loose=True\n    )\n    obsSeq.write_obs_seq()\n\n\n\n","type":"content","url":"/notebooks/projects/model-obs-your-own-model-1#crocolake-observation-sequence-files","position":5},{"hierarchy":{"lvl1":"Model-Observation comparison: use your own MOM6 run","lvl2":"The configuration file"},"type":"lvl2","url":"/notebooks/projects/model-obs-your-own-model-1#the-configuration-file","position":6},{"hierarchy":{"lvl1":"Model-Observation comparison: use your own MOM6 run","lvl2":"The configuration file"},"content":"Remember to generate a new config file for your workflow:\n\n!cp ../configs/config_template.yaml ../configs/config_my_workflow.yaml\n\n\n\nand to customize the paths as needed.","type":"content","url":"/notebooks/projects/model-obs-your-own-model-1#the-configuration-file","position":7},{"hierarchy":{"lvl1":"Model-Observation comparison: use your own MOM6 run","lvl2":"Running the workflow"},"type":"lvl2","url":"/notebooks/projects/model-obs-your-own-model-1#running-the-workflow","position":8},{"hierarchy":{"lvl1":"Model-Observation comparison: use your own MOM6 run","lvl2":"Running the workflow"},"content":"Generate and run the workflow from your config file. Remember that you need to be running this notebook on Casper, as the configuration file points to DART‚Äôs installation on that machine.\n\nfrom crococamp.workflows import WorkflowModelObs\n\n# Create and run workflow to interpolate MOM6 model onto World Ocean Database obs space\nmy_workflow = WorkflowModelObs.from_config_file('../configs/config_my_workflow.yaml')\nmy_workflow.run() #use flag clear_output=True if you want to re-run it and automatically clean all previous output\n\n\n\n","type":"content","url":"/notebooks/projects/model-obs-your-own-model-1#running-the-workflow","position":9},{"hierarchy":{"lvl1":"Model-Observation comparison: use your own MOM6 run","lvl2":"Displaying the interactive map"},"type":"lvl2","url":"/notebooks/projects/model-obs-your-own-model-1#displaying-the-interactive-map","position":10},{"hierarchy":{"lvl1":"Model-Observation comparison: use your own MOM6 run","lvl2":"Displaying the interactive map"},"content":"Load and explore the data using pandas or dask:\n\ngood_model_obs_df = workflow_float.get_good_model_obs_df(compute=True)  # compute=True triggers the compute of the dask dataframe, returning a pandas dataframe with data loaded in memory\ngood_model_obs_df.head()                                                # displays first 5 rows in the dataframe\n\n\n\nLoad the interactive map:\n\nfrom crococamp.viz import InteractiveWidgetMap\n# Create an interactive map widget to visualize model-observation comparisons\n# The widget provides controls for selecting variables, observation types, and time ranges\nwidget = InteractiveWidgetMap(dgood_model_obs_f)\nwidget.setup()\n\n","type":"content","url":"/notebooks/projects/model-obs-your-own-model-1#displaying-the-interactive-map","position":11},{"hierarchy":{"lvl1":"BGC Sample Project"},"type":"lvl1","url":"/notebooks/projects/sample-crocodash-projects/bgc","position":0},{"hierarchy":{"lvl1":"BGC Sample Project"},"content":"In this notebook, the code has the simplest regional ocean model setup possible through CrocoDash. In the comments and descriptions, we‚Äôll provide some of the ways you can add and build on this demo (recreate a specific domain - like GFDL‚Äôs NWA12, run with CICE - the sea-ice model in CESM, run with MARBL - the ocean BGC model in CESM, and beyond!)\n\nThis notebook is designed for new users of CrocoDash and CESM who want to set up a simple regional ocean model. By following the steps, you will learn how to generate a domain, configure a CESM case, prepare ocean forcing data, and run your simulation.\n\nA typical workflow of utilizing CrocoDash consists of four main steps:\n\nGenerate a regional MOM6 domain.\n\nCreate the CESM case.\n\nPrepare ocean forcing data.\n\nBuild and run the case.\n\nEvery specialization of CrocoDash will use this framework.\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/bgc","position":1},{"hierarchy":{"lvl1":"BGC Sample Project","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl2","url":"/notebooks/projects/sample-crocodash-projects/bgc#section-1-generate-a-regional-mom6-domain","position":2},{"hierarchy":{"lvl1":"BGC Sample Project","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"We begin by defining a regional MOM6 domain using CrocoDash. To do so, we first generate a horizontal grid. We then generate the topography by remapping an existing bathymetric dataset to our horizontal grid. Finally, we define a vertical grid.\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/bgc#section-1-generate-a-regional-mom6-domain","position":3},{"hierarchy":{"lvl1":"BGC Sample Project","lvl3":"Step 1.1: Horizontal Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/bgc#step-1-1-horizontal-grid","position":4},{"hierarchy":{"lvl1":"BGC Sample Project","lvl3":"Step 1.1: Horizontal Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"\n\nfrom CrocoDash.grid import Grid\n\ngrid = Grid(\n  resolution=0.1, # in degrees\n  xstart=228.0, # min longitude in [0, 360]\n  lenx=16.5, # longitude extent in degrees\n  ystart=30, # min latitude in [-90, 90]\n  leny=20, # latitude extent in degrees\n  name=\"CAcurrent\",\n\n# Try changing the grid resolution or domain boundaries to model a different region!\n\n\n\nThe above cell is a way to generate a rectangular lat/lon grid through CrocoDash\n\nIf you‚Äôre interested in using an already generated regional grid, you can do that by passing in a supergrid, like in \n\nthis demo.\n\nIf you‚Äôre interested in subsetting an already generated global grid, you can do that by passing in lat/lon args, like in \n\nthis demo.\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/bgc#step-1-1-horizontal-grid","position":5},{"hierarchy":{"lvl1":"BGC Sample Project","lvl3":"Step 1.2: Topography","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/bgc#step-1-2-topography","position":6},{"hierarchy":{"lvl1":"BGC Sample Project","lvl3":"Step 1.2: Topography","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"The below three cells are a way to generate topography with a given grid using the \n\nGEBCO dataset.\n\nIf you‚Äôre interested in using a previously generated topography for your grid, you can do that by passing in a topography file, like in \n\nthis demo.\n\nfrom CrocoDash.topo import Topo\n\ntopo = Topo(\n    grid = grid,\n    min_depth = 9.5, # in meters\n)\n\n\n\nbathymetry_path='/glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/CrocoDash/data/gebco/GEBCO_2024.nc'\n\ntopo.interpolate_from_file(\n    file_path = bathymetry_path,\n    longitude_coordinate_name=\"lon\",\n    latitude_coordinate_name=\"lat\",\n    vertical_coordinate_name=\"elevation\"\n)\n\n\n\n# Validate that the topography looks right!\ntopo.depth.plot()\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/bgc#step-1-2-topography","position":7},{"hierarchy":{"lvl1":"BGC Sample Project","lvl4":"Editing the topography","lvl3":"Step 1.2: Topography","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl4","url":"/notebooks/projects/sample-crocodash-projects/bgc#editing-the-topography","position":8},{"hierarchy":{"lvl1":"BGC Sample Project","lvl4":"Editing the topography","lvl3":"Step 1.2: Topography","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"Regridding topography from GEBCO may result in features we do not necessarily want. The Topo Editor is a tool to edit the bathymetry as wanted. You can do things like erasing basins, opening or closing bays, or adjusting the minimum depth. On JupyterHub, remember we need the ipympl extension, which can be installed as shown on \n\nthis slide.\n\nFor this particular case, you will need to mask out two disconnected basins (Gulf of California and part of the Salish Sea that gets landlocked at this resolution).\n\n%matplotlib ipympl\nfrom CrocoDash.topo_editor import TopoEditor\nTopoEditor(topo)\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/bgc#editing-the-topography","position":9},{"hierarchy":{"lvl1":"BGC Sample Project","lvl3":"Step 1.3: Vertical Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/bgc#step-1-3-vertical-grid","position":10},{"hierarchy":{"lvl1":"BGC Sample Project","lvl3":"Step 1.3: Vertical Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"The below cell generates a vertical grid using a hyperbolic function. We can also create a uniform grid instance, or use a previously generated vertical grid, like in \n\nthis demo.\n\nfrom CrocoDash.vgrid import VGrid\n\nvgrid  = VGrid.hyperbolic(\n    nk = 75, # number of vertical levels\n    depth = topo.max_depth,\n    ratio=20.0 # target ratio of top to bottom layer thicknesses\n)\n\n\n\nimport matplotlib.pyplot as plt\nplt.close()\n# Create the plot\nfor depth in vgrid.z:\n    plt.axhline(y=depth, linestyle='-')  # Horizontal lines\n\nplt.ylim(max(vgrid.z) + 10, min(vgrid.z) - 10)  # Invert y-axis so deeper values go down\nplt.ylabel(\"Depth\")\nplt.title(\"Vertical Grid\")\nplt.show()\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/bgc#step-1-3-vertical-grid","position":11},{"hierarchy":{"lvl1":"BGC Sample Project","lvl2":"SECTION 2: Create the CESM case"},"type":"lvl2","url":"/notebooks/projects/sample-crocodash-projects/bgc#section-2-create-the-cesm-case","position":12},{"hierarchy":{"lvl1":"BGC Sample Project","lvl2":"SECTION 2: Create the CESM case"},"content":"After generating the MOM6 domain, the next step is to create a CESM case using CrocoDash. This process is straightforward and involves instantiating the CrocoDash Case object. The Case object requires the following inputs:\n\nCESM Source Directory: A local path to a compatible CESM source copy.\n\nCase Name: A unique name for the CESM case.\n\nInput Directory: The directory where all necessary input files will be written.\n\nMOM6 Domain Objects: The horizontal grid, topography, and vertical grid created in the previous section.\n\nProject ID: (Optional) A project ID, if required by the machine.\n\nCompset: The set of models to be used in the Case. Standalone Ocean, Ocean-BGC, Ocean-Seaice, Ocean-Runoff, etc...\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/bgc#section-2-create-the-cesm-case","position":13},{"hierarchy":{"lvl1":"BGC Sample Project","lvl3":"Step 2.1: Specify case name and directories:","lvl2":"SECTION 2: Create the CESM case"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/bgc#step-2-1-specify-case-name-and-directories","position":14},{"hierarchy":{"lvl1":"BGC Sample Project","lvl3":"Step 2.1: Specify case name and directories:","lvl2":"SECTION 2: Create the CESM case"},"content":"Begin by specifying the case name and the necessary directory paths. Ensure the CESM root directory points to your own local copy of CESM. Below is an example setup:\n\nfrom pathlib import Path\n\n\n\n# CESM case (experiment) name\ncasename = \"CACurrent.001\"\n\n# CESM source root (Update this path accordingly!!!)\ncesmroot =\"/path/to/CROCESM\"\n\n# Place where all your input files go \ninputdir = Path.home()/\"scratch\" / \"croc_input\" / casename\n    \n# CESM case directory\ncaseroot = Path.home() / \"croc_cases\" / casename\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/bgc#step-2-1-specify-case-name-and-directories","position":15},{"hierarchy":{"lvl1":"BGC Sample Project","lvl3":"Step 2.2: Create the Case","lvl2":"SECTION 2: Create the CESM case"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/bgc#step-2-2-create-the-case","position":16},{"hierarchy":{"lvl1":"BGC Sample Project","lvl3":"Step 2.2: Create the Case","lvl2":"SECTION 2: Create the CESM case"},"content":"To create the CESM case, instantiate the Case object as shown below. This will automatically set up the CESM case based on the provided inputs: The cesmroot argument specifies the path to your local CESM source directory.\nThe caseroot argument defines the directory where the case will be created. CrocoDash will handle all necessary namelist modifications and XML changes to align with the MOM6 domain objects generated earlier.\n\nThis is where we can change the model setup to what we want:\n\nInterested in BGC? Check out \n\nthe demo for how to modify this notebook for BGC.\n\nInterested in CICE? Check out \n\nthe demo for how to modify this notebook for CICE.\n\nInterested in Data Runoff from GLOFAS (or JRA)? Check out \n\nthe demo for how to modify this notebook for Data Runoff.\n\nfrom CrocoDash.case import Case\n\ncase = Case(\n    cesmroot = cesmroot,\n    caseroot = caseroot,\n    inputdir = inputdir,\n    ocn_grid = grid,\n    ocn_vgrid = vgrid,\n    ocn_topo = topo,\n    project = 'CESM0030',\n    override = True,\n    machine = \"derecho\",\n    compset = \"CR1850MARBL_JRA\" # Change this as necessary to include whatever models you want. BGC? change MOM6 to MOM6%MARBL-BIO, CICE? Change SICE to CICE, GLOFAS Runoff? Change SROF to DROF%GLOFAS\n)\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/bgc#step-2-2-create-the-case","position":17},{"hierarchy":{"lvl1":"BGC Sample Project","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl2","url":"/notebooks/projects/sample-crocodash-projects/bgc#section-3-prepare-ocean-forcing-data","position":18},{"hierarchy":{"lvl1":"BGC Sample Project","lvl2":"Section 3: Prepare ocean forcing data"},"content":"We need to cut out our ocean forcing. The package expects an initial condition and one time-dependent segment per non-land boundary.\n\nIn this notebook, we are forcing with the Copernicus Marine ‚ÄúGlorys‚Äù reanalysis dataset.","type":"content","url":"/notebooks/projects/sample-crocodash-projects/bgc#section-3-prepare-ocean-forcing-data","position":19},{"hierarchy":{"lvl1":"BGC Sample Project","lvl3":"Step 3.1 Configure Initial Conditions and Forcings","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/bgc#step-3-1-configure-initial-conditions-and-forcings","position":20},{"hierarchy":{"lvl1":"BGC Sample Project","lvl3":"Step 3.1 Configure Initial Conditions and Forcings","lvl2":"Section 3: Prepare ocean forcing data"},"content":"Interested in adding tides? Check out \n\nthe demo.\n\nInterested in adding chlorophyll? Check out \n\nthe demo.\n\nIs the GLORYS data access function not working? Check out alternative options in \n\nthis demo.\n\nConfigure Forcings MUST be modified if BGC or Data Runoff are part of the compset (and will throw an error). Check out the demos to see the changes required: \n\nBGC Demo, \n\nDROF Demo.\n\ncase.configure_forcings(\n    date_range = [\"2000-01-01 00:00:00\", \"2000-02-01 00:00:00\"],\n        data_input_path = \"/glade/campaign/collections/cmip/CMIP6/CESM-HR/FOSI_BGC/HR/g.e22.TL319_t13.G1850ECOIAF_JRA_HR.4p2z.001/ocn/proc/tseries/month_1\",\n        product_name = \"CESM_OUTPUT\",\n        marbl_ic_filepath = \"/glade/campaign/collections/gdex/data/d651077/cesmdata/inputdata/ocn/mom/tx0.66v1/ecosys_jan_IC_omip_latlon_1x1_180W_c231221.nc\",\n        boundaries=['south', 'north', 'west'],  # eastern boundary is land-locked!\n)\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/bgc#step-3-1-configure-initial-conditions-and-forcings","position":21},{"hierarchy":{"lvl1":"BGC Sample Project","lvl3":"Step 3.3: Process forcing data","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/bgc#step-3-3-process-forcing-data","position":22},{"hierarchy":{"lvl1":"BGC Sample Project","lvl3":"Step 3.3: Process forcing data","lvl2":"Section 3: Prepare ocean forcing data"},"content":"In this final step, we call the process_forcings method of CrocoDash to cut out and interpolate the initial condition as well as all boundaries. CrocoDash also updates MOM6 runtime parameters and CESM xml variables accordingly.\n\ncase.process_forcings() # You can turn off specific processing if it's already been processed previously with process_*name* = False. Ex. case.process_forcings(process_chl = False)\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/bgc#step-3-3-process-forcing-data","position":23},{"hierarchy":{"lvl1":"BGC Sample Project","lvl2":"Section 4: Build and run the case"},"type":"lvl2","url":"/notebooks/projects/sample-crocodash-projects/bgc#section-4-build-and-run-the-case","position":24},{"hierarchy":{"lvl1":"BGC Sample Project","lvl2":"Section 4: Build and run the case"},"content":"After completing the previous steps, you are ready to build and run your CESM case. Begin by navigating to the case root directory specified during the case creation. Before proceeding, review the user_nl_mom file located in the case directory. This file contains MOM6 parameter settings that were automatically generated by CrocoDash. Carefully examine these parameters and make any necessary adjustments to fine-tune the model for your specific requirements. While CrocoDash aims to provide a solid starting point, further tuning and adjustments are typically necessary to improve the model for your use case.\n\nOnce you have reviewed and modified the parameters as needed, you can build and execute the case using the following commands:qcmd -- ./case.build\n./case.submit","type":"content","url":"/notebooks/projects/sample-crocodash-projects/bgc#section-4-build-and-run-the-case","position":25},{"hierarchy":{"lvl1":"CICE Antarctic Sample Project"},"type":"lvl1","url":"/notebooks/projects/sample-crocodash-projects/cice-antarctica","position":0},{"hierarchy":{"lvl1":"CICE Antarctic Sample Project"},"content":"In this notebook, the code has the simplest regional ocean model setup possible through CrocoDash. In the comments and descriptions, we‚Äôll provide some of the ways you can add and build on this demo (recreate a specific domain - like GFDL‚Äôs NWA12, run with CICE - the sea-ice model in CESM, run with MARBL - the ocean BGC model in CESM, and beyond!)\n\nThis notebook is designed for new users of CrocoDash and CESM who want to set up a simple regional ocean model. By following the steps, you will learn how to generate a domain, configure a CESM case, prepare ocean forcing data, and run your simulation.\n\nA typical workflow of utilizing CrocoDash consists of four main steps:\n\nGenerate a regional MOM6 domain.\n\nCreate the CESM case.\n\nPrepare ocean forcing data.\n\nBuild and run the case.\n\nEvery specialization of CrocoDash will use this framework.\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/cice-antarctica","position":1},{"hierarchy":{"lvl1":"CICE Antarctic Sample Project","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl2","url":"/notebooks/projects/sample-crocodash-projects/cice-antarctica#section-1-generate-a-regional-mom6-domain","position":2},{"hierarchy":{"lvl1":"CICE Antarctic Sample Project","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"We begin by defining a regional MOM6 domain using CrocoDash. To do so, we first generate a horizontal grid. We then generate the topography by remapping an existing bathymetric dataset to our horizontal grid. Finally, we define a vertical grid.\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/cice-antarctica#section-1-generate-a-regional-mom6-domain","position":3},{"hierarchy":{"lvl1":"CICE Antarctic Sample Project","lvl3":"Step 1.1: Horizontal Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/cice-antarctica#step-1-1-horizontal-grid","position":4},{"hierarchy":{"lvl1":"CICE Antarctic Sample Project","lvl3":"Step 1.1: Horizontal Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"CROCODILE grid generator tools cannot create domains over the poles (but will soon!), instead we pass in previously generated grids over the Artic/Antartic (If you‚Äôre interested in how we generated these, please reach out!).\nAs a quick reminder, sea ice OBCs are not well understood, and it is common practice to have a domain where sea ice does not reach the boundaries.\n\nGrids:\n\nArctic Grid: /glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/CrocoDash/grids/cice_grids/replica_grid_3413.nc\n\nArctic Bathy: /glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/CrocoDash/grids/cice_grids/bathy_3413.nc\n\nAntarctic Grid: /glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/CrocoDash/grids/cice_grids/replica_grid_3031.nc\n\nAntarctic Bathy: /glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/CrocoDash/grids/cice_grids/bathy_3031.nc\n\nfrom CrocoDash.grid import Grid\n\ngrid = Grid.from_supergrid(\"/glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/CrocoDash/grids/cice_grids/replica_grid_3031.nc\")\ngrid.name = \"Antarctic_Grid\"\n\n\n\nThe above cell is a way to generate a rectangular lat/lon grid through CrocoDash\n\nIf you‚Äôre interested in using an already generated regional grid, you can do that by passing in a supergrid, like in \n\nthis demo.\n\nIf you‚Äôre interested in subsetting an already generated global grid, you can do that by passing in lat/lon args, like in \n\nthis demo.\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/cice-antarctica#step-1-1-horizontal-grid","position":5},{"hierarchy":{"lvl1":"CICE Antarctic Sample Project","lvl3":"Step 1.2: Topography","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/cice-antarctica#step-1-2-topography","position":6},{"hierarchy":{"lvl1":"CICE Antarctic Sample Project","lvl3":"Step 1.2: Topography","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"The below three cells are a way to generate topography with a given grid using the \n\nGEBCO dataset.\n\nIf you‚Äôre interested in using a previously generated topography for your grid, you can do that by passing in a topography file, like in \n\nthis demo.\n\nfrom CrocoDash.topo import Topo\n\nbathymetry_path='/glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/CrocoDash/grids/cice_grids/bathy_3031.nc'\n\ntopo = Topo.from_topo_file(\n    grid = grid,\n    topo_file_path=bathymetry_path,\n    min_depth = 9.5,\n)\n\n\n\n# Validate that the topography looks right!\ntopo.depth.plot()\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/cice-antarctica#step-1-2-topography","position":7},{"hierarchy":{"lvl1":"CICE Antarctic Sample Project","lvl3":"Step 1.3: Vertical Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/cice-antarctica#step-1-3-vertical-grid","position":8},{"hierarchy":{"lvl1":"CICE Antarctic Sample Project","lvl3":"Step 1.3: Vertical Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"The below cell generates a vertical grid using a hyperbolic function. We can also create a uniform grid instance, or use a previously generated vertical grid, like in \n\nthis demo.\n\nfrom CrocoDash.vgrid import VGrid\n\nvgrid  = VGrid.hyperbolic(\n    nk = 75, # number of vertical levels\n    depth = topo.max_depth,\n    ratio=20.0 # target ratio of top to bottom layer thicknesses\n)\n\n\n\nimport matplotlib.pyplot as plt\nplt.close()\n# Create the plot\nfor depth in vgrid.z:\n    plt.axhline(y=depth, linestyle='-')  # Horizontal lines\n\nplt.ylim(max(vgrid.z) + 10, min(vgrid.z) - 10)  # Invert y-axis so deeper values go down\nplt.ylabel(\"Depth\")\nplt.title(\"Vertical Grid\")\nplt.show()\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/cice-antarctica#step-1-3-vertical-grid","position":9},{"hierarchy":{"lvl1":"CICE Antarctic Sample Project","lvl2":"SECTION 2: Create the CESM case"},"type":"lvl2","url":"/notebooks/projects/sample-crocodash-projects/cice-antarctica#section-2-create-the-cesm-case","position":10},{"hierarchy":{"lvl1":"CICE Antarctic Sample Project","lvl2":"SECTION 2: Create the CESM case"},"content":"After generating the MOM6 domain, the next step is to create a CESM case using CrocoDash. This process is straightforward and involves instantiating the CrocoDash Case object. The Case object requires the following inputs:\n\nCESM Source Directory: A local path to a compatible CESM source copy.\n\nCase Name: A unique name for the CESM case.\n\nInput Directory: The directory where all necessary input files will be written.\n\nMOM6 Domain Objects: The horizontal grid, topography, and vertical grid created in the previous section.\n\nProject ID: (Optional) A project ID, if required by the machine.\n\nCompset: The set of models to be used in the Case. Standalone Ocean, Ocean-BGC, Ocean-Seaice, Ocean-Runoff, etc...\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/cice-antarctica#section-2-create-the-cesm-case","position":11},{"hierarchy":{"lvl1":"CICE Antarctic Sample Project","lvl3":"Step 2.1: Specify case name and directories:","lvl2":"SECTION 2: Create the CESM case"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/cice-antarctica#step-2-1-specify-case-name-and-directories","position":12},{"hierarchy":{"lvl1":"CICE Antarctic Sample Project","lvl3":"Step 2.1: Specify case name and directories:","lvl2":"SECTION 2: Create the CESM case"},"content":"Begin by specifying the case name and the necessary directory paths. Ensure the CESM root directory points to your own local copy of CESM. Below is an example setup:\n\nfrom pathlib import Path\n\n\n\n# CESM case (experiment) name\ncasename = \"antartic.cice.1\"\n\n# CESM source root (Update this path accordingly!!!)\ncesmroot =\"/glade/work/mlevy/codes/CESM/cesm3_0_alpha07c_CROCO\"\n\n# Place where all your input files go \ninputdir = Path.home()/\"scratch\" / \"croc_input\" / casename\n    \n# CESM case directory\ncaseroot = Path.home() / \"croc_cases\" / casename\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/cice-antarctica#step-2-1-specify-case-name-and-directories","position":13},{"hierarchy":{"lvl1":"CICE Antarctic Sample Project","lvl3":"Step 2.2: Create the Case","lvl2":"SECTION 2: Create the CESM case"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/cice-antarctica#step-2-2-create-the-case","position":14},{"hierarchy":{"lvl1":"CICE Antarctic Sample Project","lvl3":"Step 2.2: Create the Case","lvl2":"SECTION 2: Create the CESM case"},"content":"To create the CESM case, instantiate the Case object as shown below. This will automatically set up the CESM case based on the provided inputs: The cesmroot argument specifies the path to your local CESM source directory.\nThe caseroot argument defines the directory where the case will be created. CrocoDash will handle all necessary namelist modifications and XML changes to align with the MOM6 domain objects generated earlier.\n\nThis is where we can change the model setup to what we want:\n\nInterested in BGC? Check out \n\nthe demo for how to modify this notebook for BGC.\n\nInterested in CICE? Check out \n\nthe demo for how to modify this notebook for CICE.\n\nInterested in Data Runoff from GLOFAS (or JRA)? Check out \n\nthe demo for how to modify this notebook for Data Runoff.\n\nfrom CrocoDash.case import Case\n\ncase = Case(\n    cesmroot = cesmroot,\n    caseroot = caseroot,\n    inputdir = inputdir,\n    ocn_grid = grid,\n    ocn_vgrid = vgrid,\n    ocn_topo = topo,\n    project = 'CESM0030',\n    override = True,\n    machine = \"derecho\",\n    compset = \"GR_JRA\" # Change this as necessary to include whatever models you want. BGC? change MOM6 to MOM6%MARBL-BIO, CICE? Change SICE to CICE, GLOFAS Runoff? Change SROF to DROF%GLOFAS\n)\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/cice-antarctica#step-2-2-create-the-case","position":15},{"hierarchy":{"lvl1":"CICE Antarctic Sample Project","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl2","url":"/notebooks/projects/sample-crocodash-projects/cice-antarctica#section-3-prepare-ocean-forcing-data","position":16},{"hierarchy":{"lvl1":"CICE Antarctic Sample Project","lvl2":"Section 3: Prepare ocean forcing data"},"content":"We need to cut out our ocean forcing. The package expects an initial condition and one time-dependent segment per non-land boundary.\n\nIn this notebook, we are forcing with the Copernicus Marine ‚ÄúGlorys‚Äù reanalysis dataset.","type":"content","url":"/notebooks/projects/sample-crocodash-projects/cice-antarctica#section-3-prepare-ocean-forcing-data","position":17},{"hierarchy":{"lvl1":"CICE Antarctic Sample Project","lvl3":"Step 3.1 Configure Initial Conditions and Forcings","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/cice-antarctica#step-3-1-configure-initial-conditions-and-forcings","position":18},{"hierarchy":{"lvl1":"CICE Antarctic Sample Project","lvl3":"Step 3.1 Configure Initial Conditions and Forcings","lvl2":"Section 3: Prepare ocean forcing data"},"content":"Interested in adding tides? Check out \n\nthe demo.\n\nInterested in adding chlorophyll? Check out \n\nthe demo.\n\nIs the GLORYS data access function not working? Check out alternative options in \n\nthis demo.\n\nConfigure Forcings MUST be modified if BGC or Data Runoff are part of the compset (and will throw an error). Check out the demos to see the changes required: \n\nBGC Demo, \n\nDROF Demo.\n\ncase.configure_forcings(\n    date_range = [\"2020-01-01 00:00:00\", \"2020-01-06 00:00:00\"],\n    product_name = \"GLORYS\",\n    function_name=\"get_glorys_data_from_rda\")\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/cice-antarctica#step-3-1-configure-initial-conditions-and-forcings","position":19},{"hierarchy":{"lvl1":"CICE Antarctic Sample Project","lvl3":"Step 3.3: Process forcing data","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/cice-antarctica#step-3-3-process-forcing-data","position":20},{"hierarchy":{"lvl1":"CICE Antarctic Sample Project","lvl3":"Step 3.3: Process forcing data","lvl2":"Section 3: Prepare ocean forcing data"},"content":"In this final step, we call the process_forcings method of CrocoDash to cut out and interpolate the initial condition as well as all boundaries. CrocoDash also updates MOM6 runtime parameters and CESM xml variables accordingly.\n\ncase.process_forcings() # You can turn off specific processing if it's already been processed previously with process_*name* = False. Ex. case.process_forcings(process_chl = False)\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/cice-antarctica#step-3-3-process-forcing-data","position":21},{"hierarchy":{"lvl1":"CICE Antarctic Sample Project","lvl2":"Section 4: Build and run the case"},"type":"lvl2","url":"/notebooks/projects/sample-crocodash-projects/cice-antarctica#section-4-build-and-run-the-case","position":22},{"hierarchy":{"lvl1":"CICE Antarctic Sample Project","lvl2":"Section 4: Build and run the case"},"content":"After completing the previous steps, you are ready to build and run your CESM case. Begin by navigating to the case root directory specified during the case creation. Before proceeding, review the user_nl_mom file located in the case directory. This file contains MOM6 parameter settings that were automatically generated by CrocoDash. Carefully examine these parameters and make any necessary adjustments to fine-tune the model for your specific requirements. While CrocoDash aims to provide a solid starting point, further tuning and adjustments are typically necessary to improve the model for your use case.\n\nOnce you have reviewed and modified the parameters as needed, you can build and execute the case using the following commands:qcmd -- ./case.build\n./case.submit\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/cice-antarctica#section-4-build-and-run-the-case","position":23},{"hierarchy":{"lvl1":"CICE Antarctic Sample Project","lvl3":"Optional: Use restart files as initial condition for your run (after you run for the first time!)","lvl2":"Section 4: Build and run the case"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/cice-antarctica#optional-use-restart-files-as-initial-condition-for-your-run-after-you-run-for-the-first-time","position":24},{"hierarchy":{"lvl1":"CICE Antarctic Sample Project","lvl3":"Optional: Use restart files as initial condition for your run (after you run for the first time!)","lvl2":"Section 4: Build and run the case"},"content":"In case you want to start the model with a restart file instead of using the generated initial condition, follow the below steps. Note that you have to have finished a run beforehand for the restart files to appear.\n\nLocate your restart file - they are usually in your previous case‚Äôs /archive/rest/<year> folder with a .r infix, e.g., cice.test.cice.r.1994-01-01-00000.nc.\nIf you do not know your /archive folder location, run ./xmlquery DOUT_S_ROOT in your (previous) case folder, which will return a path similar to DOUT_S_ROOT: <PATH TO ARCHIVE>.\n\nUse the cp command to copy the file to your current case /run directory, e.g., cp cice.test.cice.r.1994-01-01-00000.nc <your_run_dir>.\n\nOpen user_nl_cice in your case directory again and change the ice_ic variable from \"UNSET\" to your file name, e.g., ice_ic=cice.test.cice.r.1994-01-01-00000.nc\n\nYour file will now be used automatically as the ice initial condition for your next run. Note that .h and .h1 files (i.e. history files) currently cannot be used as initial conditions for CICE.","type":"content","url":"/notebooks/projects/sample-crocodash-projects/cice-antarctica#optional-use-restart-files-as-initial-condition-for-your-run-after-you-run-for-the-first-time","position":25},{"hierarchy":{"lvl1":"CICE Arctic Sample Project"},"type":"lvl1","url":"/notebooks/projects/sample-crocodash-projects/cice-arctic","position":0},{"hierarchy":{"lvl1":"CICE Arctic Sample Project"},"content":"In this notebook, the code has the simplest regional ocean model setup possible through CrocoDash. In the comments and descriptions, we‚Äôll provide some of the ways you can add and build on this demo (recreate a specific domain - like GFDL‚Äôs NWA12, run with CICE - the sea-ice model in CESM, run with MARBL - the ocean BGC model in CESM, and beyond!)\n\nThis notebook is designed for new users of CrocoDash and CESM who want to set up a simple regional ocean model. By following the steps, you will learn how to generate a domain, configure a CESM case, prepare ocean forcing data, and run your simulation.\n\nA typical workflow of utilizing CrocoDash consists of four main steps:\n\nGenerate a regional MOM6 domain.\n\nCreate the CESM case.\n\nPrepare ocean forcing data.\n\nBuild and run the case.\n\nEvery specialization of CrocoDash will use this framework.\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/cice-arctic","position":1},{"hierarchy":{"lvl1":"CICE Arctic Sample Project","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl2","url":"/notebooks/projects/sample-crocodash-projects/cice-arctic#section-1-generate-a-regional-mom6-domain","position":2},{"hierarchy":{"lvl1":"CICE Arctic Sample Project","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"We begin by defining a regional MOM6 domain using CrocoDash. To do so, we first generate a horizontal grid. We then generate the topography by remapping an existing bathymetric dataset to our horizontal grid. Finally, we define a vertical grid.\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/cice-arctic#section-1-generate-a-regional-mom6-domain","position":3},{"hierarchy":{"lvl1":"CICE Arctic Sample Project","lvl3":"Step 1.1: Horizontal Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/cice-arctic#step-1-1-horizontal-grid","position":4},{"hierarchy":{"lvl1":"CICE Arctic Sample Project","lvl3":"Step 1.1: Horizontal Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"CROCODILE grid generator tools cannot create domains over the poles (but will soon!), instead we pass in previously generated grids over the Artic/Antartic (If you‚Äôre interested in how we generated these, please reach out!).\nAs a quick reminder, sea ice OBCs are not well understood, and it is common practice to have a domain where sea ice does not reach the boundaries.\n\nGrids:\n\nArctic Grid: /glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/CrocoDash/grids/cice_grids/replica_grid_3413.nc\n\nArctic Bathy: /glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/CrocoDash/grids/cice_grids/bathy_3413.nc\n\nAntarctic Grid: /glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/CrocoDash/grids/cice_grids/replica_grid_3031.nc\n\nAntarctic Bathy: /glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/CrocoDash/grids/cice_grids/bathy_3031.nc\n\nfrom CrocoDash.grid import Grid\n\ngrid = Grid.from_supergrid(\"/glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/CrocoDash/grids/cice_grids/replica_grid_3413.nc\")\ngrid.name = \"Arctic_Grid\"\n\n\n\nThe above cell is a way to generate a rectangular lat/lon grid through CrocoDash\n\nIf you‚Äôre interested in using an already generated regional grid, you can do that by passing in a supergrid, like in \n\nthis demo.\n\nIf you‚Äôre interested in subsetting an already generated global grid, you can do that by passing in lat/lon args, like in \n\nthis demo.\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/cice-arctic#step-1-1-horizontal-grid","position":5},{"hierarchy":{"lvl1":"CICE Arctic Sample Project","lvl3":"Step 1.2: Topography","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/cice-arctic#step-1-2-topography","position":6},{"hierarchy":{"lvl1":"CICE Arctic Sample Project","lvl3":"Step 1.2: Topography","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"The below three cells are a way to generate topography with a given grid using the \n\nGEBCO dataset.\n\nIf you‚Äôre interested in using a previously generated topography for your grid, you can do that by passing in a topography file, like in \n\nthis demo.\n\nfrom CrocoDash.topo import Topo\n\nbathymetry_path='/glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/CrocoDash/grids/cice_grids/bathy_3413.nc'\n\ntopo = Topo.from_topo_file(\n    grid = grid,\n    topo_file_path=bathymetry_path,\n    min_depth = 9.5,\n)\n\n\n\n# Validate that the topography looks right!\ntopo.depth.plot()\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/cice-arctic#step-1-2-topography","position":7},{"hierarchy":{"lvl1":"CICE Arctic Sample Project","lvl3":"Step 1.3: Vertical Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/cice-arctic#step-1-3-vertical-grid","position":8},{"hierarchy":{"lvl1":"CICE Arctic Sample Project","lvl3":"Step 1.3: Vertical Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"The below cell generates a vertical grid using a hyperbolic function. We can also create a uniform grid instance, or use a previously generated vertical grid, like in \n\nthis demo.\n\nfrom CrocoDash.vgrid import VGrid\n\nvgrid  = VGrid.hyperbolic(\n    nk = 75, # number of vertical levels\n    depth = topo.max_depth,\n    ratio=20.0 # target ratio of top to bottom layer thicknesses\n)\n\n\n\nimport matplotlib.pyplot as plt\nplt.close()\n# Create the plot\nfor depth in vgrid.z:\n    plt.axhline(y=depth, linestyle='-')  # Horizontal lines\n\nplt.ylim(max(vgrid.z) + 10, min(vgrid.z) - 10)  # Invert y-axis so deeper values go down\nplt.ylabel(\"Depth\")\nplt.title(\"Vertical Grid\")\nplt.show()\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/cice-arctic#step-1-3-vertical-grid","position":9},{"hierarchy":{"lvl1":"CICE Arctic Sample Project","lvl2":"SECTION 2: Create the CESM case"},"type":"lvl2","url":"/notebooks/projects/sample-crocodash-projects/cice-arctic#section-2-create-the-cesm-case","position":10},{"hierarchy":{"lvl1":"CICE Arctic Sample Project","lvl2":"SECTION 2: Create the CESM case"},"content":"After generating the MOM6 domain, the next step is to create a CESM case using CrocoDash. This process is straightforward and involves instantiating the CrocoDash Case object. The Case object requires the following inputs:\n\nCESM Source Directory: A local path to a compatible CESM source copy.\n\nCase Name: A unique name for the CESM case.\n\nInput Directory: The directory where all necessary input files will be written.\n\nMOM6 Domain Objects: The horizontal grid, topography, and vertical grid created in the previous section.\n\nProject ID: (Optional) A project ID, if required by the machine.\n\nCompset: The set of models to be used in the Case. Standalone Ocean, Ocean-BGC, Ocean-Seaice, Ocean-Runoff, etc...\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/cice-arctic#section-2-create-the-cesm-case","position":11},{"hierarchy":{"lvl1":"CICE Arctic Sample Project","lvl3":"Step 2.1: Specify case name and directories:","lvl2":"SECTION 2: Create the CESM case"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/cice-arctic#step-2-1-specify-case-name-and-directories","position":12},{"hierarchy":{"lvl1":"CICE Arctic Sample Project","lvl3":"Step 2.1: Specify case name and directories:","lvl2":"SECTION 2: Create the CESM case"},"content":"Begin by specifying the case name and the necessary directory paths. Ensure the CESM root directory points to your own local copy of CESM. Below is an example setup:\n\nfrom pathlib import Path\n\n\n\n# CESM case (experiment) name\ncasename = \"arctic.cice.1\"\n\n# CESM source root (Update this path accordingly!!!)\ncesmroot =\"/glade/work/mlevy/codes/CESM/cesm3_0_alpha07c_CROCO/\"\n\n# Place where all your input files go \ninputdir = Path.home()/\"scratch\" / \"croc_input\" / casename\n    \n# CESM case directory\ncaseroot = Path.home() / \"croc_cases\" / casename\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/cice-arctic#step-2-1-specify-case-name-and-directories","position":13},{"hierarchy":{"lvl1":"CICE Arctic Sample Project","lvl3":"Step 2.2: Create the Case","lvl2":"SECTION 2: Create the CESM case"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/cice-arctic#step-2-2-create-the-case","position":14},{"hierarchy":{"lvl1":"CICE Arctic Sample Project","lvl3":"Step 2.2: Create the Case","lvl2":"SECTION 2: Create the CESM case"},"content":"To create the CESM case, instantiate the Case object as shown below. This will automatically set up the CESM case based on the provided inputs: The cesmroot argument specifies the path to your local CESM source directory.\nThe caseroot argument defines the directory where the case will be created. CrocoDash will handle all necessary namelist modifications and XML changes to align with the MOM6 domain objects generated earlier.\n\nThis is where we can change the model setup to what we want:\n\nInterested in BGC? Check out \n\nthe demo for how to modify this notebook for BGC.\n\nInterested in CICE? Check out \n\nthe demo for how to modify this notebook for CICE.\n\nInterested in Data Runoff from GLOFAS (or JRA)? Check out \n\nthe demo for how to modify this notebook for Data Runoff.\n\nfrom CrocoDash.case import Case\n\ncase = Case(\n    cesmroot = cesmroot,\n    caseroot = caseroot,\n    inputdir = inputdir,\n    ocn_grid = grid,\n    ocn_vgrid = vgrid,\n    ocn_topo = topo,\n    project = 'CESM0030',\n    override = True,\n    machine = \"derecho\",\n    compset = \"GR_JRA\" # Change this as necessary to include whatever models you want. BGC? change MOM6 to MOM6%MARBL-BIO, CICE? Change SICE to CICE, GLOFAS Runoff? Change SROF to DROF%GLOFAS\n)\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/cice-arctic#step-2-2-create-the-case","position":15},{"hierarchy":{"lvl1":"CICE Arctic Sample Project","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl2","url":"/notebooks/projects/sample-crocodash-projects/cice-arctic#section-3-prepare-ocean-forcing-data","position":16},{"hierarchy":{"lvl1":"CICE Arctic Sample Project","lvl2":"Section 3: Prepare ocean forcing data"},"content":"We need to cut out our ocean forcing. The package expects an initial condition and one time-dependent segment per non-land boundary.\n\nIn this notebook, we are forcing with the Copernicus Marine ‚ÄúGlorys‚Äù reanalysis dataset.","type":"content","url":"/notebooks/projects/sample-crocodash-projects/cice-arctic#section-3-prepare-ocean-forcing-data","position":17},{"hierarchy":{"lvl1":"CICE Arctic Sample Project","lvl3":"Step 3.1 Configure Initial Conditions and Forcings","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/cice-arctic#step-3-1-configure-initial-conditions-and-forcings","position":18},{"hierarchy":{"lvl1":"CICE Arctic Sample Project","lvl3":"Step 3.1 Configure Initial Conditions and Forcings","lvl2":"Section 3: Prepare ocean forcing data"},"content":"Interested in adding tides? Check out \n\nthe demo.\n\nInterested in adding chlorophyll? Check out \n\nthe demo.\n\nIs the GLORYS data access function not working? Check out alternative options in \n\nthis demo.\n\nConfigure Forcings MUST be modified if BGC or Data Runoff are part of the compset (and will throw an error). Check out the demos to see the changes required: \n\nBGC Demo, \n\nDROF Demo.\n\ncase.configure_forcings(\n    date_range = [\"2020-01-01 00:00:00\", \"2020-01-06 00:00:00\"],\n    product_name = \"GLORYS\",\n    function_name=\"get_glorys_data_from_rda\")\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/cice-arctic#step-3-1-configure-initial-conditions-and-forcings","position":19},{"hierarchy":{"lvl1":"CICE Arctic Sample Project","lvl3":"Step 3.3: Process forcing data","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/cice-arctic#step-3-3-process-forcing-data","position":20},{"hierarchy":{"lvl1":"CICE Arctic Sample Project","lvl3":"Step 3.3: Process forcing data","lvl2":"Section 3: Prepare ocean forcing data"},"content":"In this final step, we call the process_forcings method of CrocoDash to cut out and interpolate the initial condition as well as all boundaries. CrocoDash also updates MOM6 runtime parameters and CESM xml variables accordingly.\n\ncase.process_forcings() # You can turn off specific processing if it's already been processed previously with process_*name* = False. Ex. case.process_forcings(process_chl = False)\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/cice-arctic#step-3-3-process-forcing-data","position":21},{"hierarchy":{"lvl1":"CICE Arctic Sample Project","lvl2":"Section 4: Build and run the case"},"type":"lvl2","url":"/notebooks/projects/sample-crocodash-projects/cice-arctic#section-4-build-and-run-the-case","position":22},{"hierarchy":{"lvl1":"CICE Arctic Sample Project","lvl2":"Section 4: Build and run the case"},"content":"After completing the previous steps, you are ready to build and run your CESM case. Begin by navigating to the case root directory specified during the case creation. Before proceeding, review the user_nl_mom file located in the case directory. This file contains MOM6 parameter settings that were automatically generated by CrocoDash. Carefully examine these parameters and make any necessary adjustments to fine-tune the model for your specific requirements. While CrocoDash aims to provide a solid starting point, further tuning and adjustments are typically necessary to improve the model for your use case.\n\nOnce you have reviewed and modified the parameters as needed, you can build and execute the case using the following commands:qcmd -- ./case.build\n./case.submit\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/cice-arctic#section-4-build-and-run-the-case","position":23},{"hierarchy":{"lvl1":"CICE Arctic Sample Project","lvl3":"Optional: Use restart files as initial condition for your run (after you run for the first time!)","lvl2":"Section 4: Build and run the case"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/cice-arctic#optional-use-restart-files-as-initial-condition-for-your-run-after-you-run-for-the-first-time","position":24},{"hierarchy":{"lvl1":"CICE Arctic Sample Project","lvl3":"Optional: Use restart files as initial condition for your run (after you run for the first time!)","lvl2":"Section 4: Build and run the case"},"content":"In case you want to start the model with a restart file instead of using the generated initial condition, follow the below steps. Note that you have to have finished a run beforehand for the restart files to appear.\n\nLocate your restart file - they are usually in your previous case‚Äôs /archive/rest/<year> folder with a .r infix, e.g., cice.test.cice.r.1994-01-01-00000.nc.\nIf you do not know your /archive folder location, run ./xmlquery DOUT_S_ROOT in your (previous) case folder, which will return a path similar to DOUT_S_ROOT: <PATH TO ARCHIVE>.\n\nUse the cp command to copy the file to your current case /run directory, e.g., cp cice.test.cice.r.1994-01-01-00000.nc <your_run_dir>.\n\nOpen user_nl_cice in your case directory again and change the ice_ic variable from \"UNSET\" to your file name, e.g., ice_ic=cice.test.cice.r.1994-01-01-00000.nc\n\nYour file will now be used automatically as the ice initial condition for your next run. Note that .h and .h1 files (i.e. history files) currently cannot be used as initial conditions for CICE.","type":"content","url":"/notebooks/projects/sample-crocodash-projects/cice-arctic#optional-use-restart-files-as-initial-condition-for-your-run-after-you-run-for-the-first-time","position":25},{"hierarchy":{"lvl1":"DROF Sample Project"},"type":"lvl1","url":"/notebooks/projects/sample-crocodash-projects/drof","position":0},{"hierarchy":{"lvl1":"DROF Sample Project"},"content":"In this notebook, the code has the simplest regional ocean model setup possible through CrocoDash. In the comments and descriptions, we‚Äôll provide some of the ways you can add and build on this demo (recreate a specific domain - like GFDL‚Äôs NWA12, run with CICE - the sea-ice model in CESM, run with MARBL - the ocean BGC model in CESM, and beyond!)\n\nThis notebook is designed for new users of CrocoDash and CESM who want to set up a simple regional ocean model. By following the steps, you will learn how to generate a domain, configure a CESM case, prepare ocean forcing data, and run your simulation.\n\nA typical workflow of utilizing CrocoDash consists of four main steps:\n\nGenerate a regional MOM6 domain.\n\nCreate the CESM case.\n\nPrepare ocean forcing data.\n\nBuild and run the case.\n\nEvery specialization of CrocoDash will use this framework.\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/drof","position":1},{"hierarchy":{"lvl1":"DROF Sample Project","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl2","url":"/notebooks/projects/sample-crocodash-projects/drof#section-1-generate-a-regional-mom6-domain","position":2},{"hierarchy":{"lvl1":"DROF Sample Project","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"We begin by defining a regional MOM6 domain using CrocoDash. To do so, we first generate a horizontal grid. We then generate the topography by remapping an existing bathymetric dataset to our horizontal grid. Finally, we define a vertical grid.\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/drof#section-1-generate-a-regional-mom6-domain","position":3},{"hierarchy":{"lvl1":"DROF Sample Project","lvl3":"Step 1.1: Horizontal Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/drof#step-1-1-horizontal-grid","position":4},{"hierarchy":{"lvl1":"DROF Sample Project","lvl3":"Step 1.1: Horizontal Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"\n\nfrom CrocoDash.grid import Grid\n\ngrid = Grid(\n  resolution = 0.05, # in degrees\n  xstart = 278.0, # min longitude in [0, 360]\n  lenx = 3.0, # longitude extent in degrees\n  ystart = 7.0, # min latitude in [-90, 90]\n  leny = 3.0, # latitude extent in degrees\n  name = \"panama1\",\n)\n\n# Try changing the grid resolution or domain boundaries to model a different region!\n\n\n\nThe above cell is a way to generate a rectangular lat/lon grid through CrocoDash\n\nIf you‚Äôre interested in using an already generated regional grid, you can do that by passing in a supergrid, like in \n\nthis demo.\n\nIf you‚Äôre interested in subsetting an already generated global grid, you can do that by passing in lat/lon args, like in \n\nthis demo.\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/drof#step-1-1-horizontal-grid","position":5},{"hierarchy":{"lvl1":"DROF Sample Project","lvl3":"Step 1.2: Topography","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/drof#step-1-2-topography","position":6},{"hierarchy":{"lvl1":"DROF Sample Project","lvl3":"Step 1.2: Topography","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"The below three cells are a way to generate topography with a given grid using the \n\nGEBCO dataset.\n\nIf you‚Äôre interested in using a previously generated topography for your grid, you can do that by passing in a topography file, like in \n\nthis demo.\n\nfrom CrocoDash.topo import Topo\n\ntopo = Topo(\n    grid = grid,\n    min_depth = 9.5, # in meters\n)\n\n\n\nbathymetry_path='/glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/CrocoDash/data/gebco/GEBCO_2024.nc'\n\ntopo.interpolate_from_file(\n    file_path = bathymetry_path,\n    longitude_coordinate_name=\"lon\",\n    latitude_coordinate_name=\"lat\",\n    vertical_coordinate_name=\"elevation\"\n)\n\n\n\n# Validate that the topography looks right!\ntopo.depth.plot()\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/drof#step-1-2-topography","position":7},{"hierarchy":{"lvl1":"DROF Sample Project","lvl4":"Editing the topography","lvl3":"Step 1.2: Topography","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl4","url":"/notebooks/projects/sample-crocodash-projects/drof#editing-the-topography","position":8},{"hierarchy":{"lvl1":"DROF Sample Project","lvl4":"Editing the topography","lvl3":"Step 1.2: Topography","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"Regridding topography from GEBCO may result in features we do not necessarily want. The Topo Editor is a tool to edit the bathymetry as wanted. You can do things like erasing basins, opening or closing bays, or adjusting the minimum depth. On JupyterHub, remember we need the ipympl extension, which can be installed as shown on \n\nthis slide.\n\n%matplotlib ipympl\nfrom CrocoDash.topo_editor import TopoEditor\nTopoEditor(topo)\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/drof#editing-the-topography","position":9},{"hierarchy":{"lvl1":"DROF Sample Project","lvl3":"Step 1.3: Vertical Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/drof#step-1-3-vertical-grid","position":10},{"hierarchy":{"lvl1":"DROF Sample Project","lvl3":"Step 1.3: Vertical Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"The below cell generates a vertical grid using a hyperbolic function. We can also create a uniform grid instance, or use a previously generated vertical grid, like in \n\nthis demo.\n\nfrom CrocoDash.vgrid import VGrid\n\nvgrid  = VGrid.hyperbolic(\n    nk = 75, # number of vertical levels\n    depth = topo.max_depth,\n    ratio=20.0 # target ratio of top to bottom layer thicknesses\n)\n\n\n\nimport matplotlib.pyplot as plt\nplt.close()\n# Create the plot\nfor depth in vgrid.z:\n    plt.axhline(y=depth, linestyle='-')  # Horizontal lines\n\nplt.ylim(max(vgrid.z) + 10, min(vgrid.z) - 10)  # Invert y-axis so deeper values go down\nplt.ylabel(\"Depth\")\nplt.title(\"Vertical Grid\")\nplt.show()\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/drof#step-1-3-vertical-grid","position":11},{"hierarchy":{"lvl1":"DROF Sample Project","lvl2":"SECTION 2: Create the CESM case"},"type":"lvl2","url":"/notebooks/projects/sample-crocodash-projects/drof#section-2-create-the-cesm-case","position":12},{"hierarchy":{"lvl1":"DROF Sample Project","lvl2":"SECTION 2: Create the CESM case"},"content":"After generating the MOM6 domain, the next step is to create a CESM case using CrocoDash. This process is straightforward and involves instantiating the CrocoDash Case object. The Case object requires the following inputs:\n\nCESM Source Directory: A local path to a compatible CESM source copy.\n\nCase Name: A unique name for the CESM case.\n\nInput Directory: The directory where all necessary input files will be written.\n\nMOM6 Domain Objects: The horizontal grid, topography, and vertical grid created in the previous section.\n\nProject ID: (Optional) A project ID, if required by the machine.\n\nCompset: The set of models to be used in the Case. Standalone Ocean, Ocean-BGC, Ocean-Seaice, Ocean-Runoff, etc...\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/drof#section-2-create-the-cesm-case","position":13},{"hierarchy":{"lvl1":"DROF Sample Project","lvl3":"Step 2.1: Specify case name and directories:","lvl2":"SECTION 2: Create the CESM case"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/drof#step-2-1-specify-case-name-and-directories","position":14},{"hierarchy":{"lvl1":"DROF Sample Project","lvl3":"Step 2.1: Specify case name and directories:","lvl2":"SECTION 2: Create the CESM case"},"content":"Begin by specifying the case name and the necessary directory paths. Ensure the CESM root directory points to your own local copy of CESM. Below is an example setup:\n\nfrom pathlib import Path\n\n\n\n# CESM case (experiment) name\ncasename = \"pan.drof.1\"\n\n# CESM source root (Update this path accordingly!!!)\ncesmroot =\"/glade/work/mlevy/codes/CESM/cesm3_0_alpha07c_CROCO\"\n\n# Place where all your input files go \ninputdir = Path.home()/\"scratch\" / \"croc_input\" / casename\n    \n# CESM case directory\ncaseroot = Path.home() / \"croc_cases\" / casename\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/drof#step-2-1-specify-case-name-and-directories","position":15},{"hierarchy":{"lvl1":"DROF Sample Project","lvl3":"Step 2.2: Create the Case","lvl2":"SECTION 2: Create the CESM case"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/drof#step-2-2-create-the-case","position":16},{"hierarchy":{"lvl1":"DROF Sample Project","lvl3":"Step 2.2: Create the Case","lvl2":"SECTION 2: Create the CESM case"},"content":"To create the CESM case, instantiate the Case object as shown below. This will automatically set up the CESM case based on the provided inputs: The cesmroot argument specifies the path to your local CESM source directory.\nThe caseroot argument defines the directory where the case will be created. CrocoDash will handle all necessary namelist modifications and XML changes to align with the MOM6 domain objects generated earlier.\n\nThis is where we can change the model setup to what we want:\n\nInterested in BGC? Check out \n\nthe demo for how to modify this notebook for BGC.\n\nInterested in CICE? Check out \n\nthe demo for how to modify this notebook for CICE.\n\nInterested in Data Runoff from GLOFAS (or JRA)? Check out \n\nthe demo for how to modify this notebook for Data Runoff.\n\nfrom CrocoDash.case import Case\n\ncase = Case(\n    cesmroot = cesmroot,\n    caseroot = caseroot,\n    inputdir = inputdir,\n    ocn_grid = grid,\n    ocn_vgrid = vgrid,\n    ocn_topo = topo,\n    project = 'CESM0030',\n    override = True,\n    machine = \"derecho\",\n    compset = \"CR_JRA_GLOFAS\" # Change this as necessary to include whatever models you want. BGC? change MOM6 to MOM6%MARBL-BIO, CICE? Change SICE to CICE, GLOFAS Runoff? Change SROF to DROF%GLOFAS\n)\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/drof#step-2-2-create-the-case","position":17},{"hierarchy":{"lvl1":"DROF Sample Project","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl2","url":"/notebooks/projects/sample-crocodash-projects/drof#section-3-prepare-ocean-forcing-data","position":18},{"hierarchy":{"lvl1":"DROF Sample Project","lvl2":"Section 3: Prepare ocean forcing data"},"content":"We need to cut out our ocean forcing. The package expects an initial condition and one time-dependent segment per non-land boundary.\n\nIn this notebook, we are forcing with the Copernicus Marine ‚ÄúGlorys‚Äù reanalysis dataset.","type":"content","url":"/notebooks/projects/sample-crocodash-projects/drof#section-3-prepare-ocean-forcing-data","position":19},{"hierarchy":{"lvl1":"DROF Sample Project","lvl3":"Step 3.1 Configure Initial Conditions and Forcings","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/drof#step-3-1-configure-initial-conditions-and-forcings","position":20},{"hierarchy":{"lvl1":"DROF Sample Project","lvl3":"Step 3.1 Configure Initial Conditions and Forcings","lvl2":"Section 3: Prepare ocean forcing data"},"content":"Interested in adding tides? Check out \n\nthe demo.\n\nInterested in adding chlorophyll? Check out \n\nthe demo.\n\nIs the GLORYS data access function not working? Check out alternative options in \n\nthis demo.\n\nConfigure Forcings MUST be modified if BGC or Data Runoff are part of the compset (and will throw an error). Check out the demos to see the changes required: \n\nBGC Demo, \n\nDROF Demo.\n\ncase.configure_forcings(\n    date_range = [\"2000-01-01 00:00:00\", \"2000-02-01 00:00:00\"],\n    product_name = \"GLORYS\",\n    function_name=\"get_glorys_data_from_rda\",\n    runoff_esmf_mesh_filepath=\"/glade/campaign/cesm/cesmdata/cseg/inputdata/ocn/mom/croc/rof/glofas/dis24/GLOFAS_esmf_mesh_v4.nc\",\n)\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/drof#step-3-1-configure-initial-conditions-and-forcings","position":21},{"hierarchy":{"lvl1":"DROF Sample Project","lvl3":"Step 3.3: Process forcing data","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/drof#step-3-3-process-forcing-data","position":22},{"hierarchy":{"lvl1":"DROF Sample Project","lvl3":"Step 3.3: Process forcing data","lvl2":"Section 3: Prepare ocean forcing data"},"content":"In this final step, we call the process_forcings method of CrocoDash to cut out and interpolate the initial condition as well as all boundaries. CrocoDash also updates MOM6 runtime parameters and CESM xml variables accordingly.\n\ncase.process_forcings() # You can turn off specific processing if it's already been processed previously with process_*name* = False. Ex. case.process_forcings(process_chl = False)\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/drof#step-3-3-process-forcing-data","position":23},{"hierarchy":{"lvl1":"DROF Sample Project","lvl2":"Section 4: Build and run the case"},"type":"lvl2","url":"/notebooks/projects/sample-crocodash-projects/drof#section-4-build-and-run-the-case","position":24},{"hierarchy":{"lvl1":"DROF Sample Project","lvl2":"Section 4: Build and run the case"},"content":"After completing the previous steps, you are ready to build and run your CESM case. Begin by navigating to the case root directory specified during the case creation. Before proceeding, review the user_nl_mom file located in the case directory. This file contains MOM6 parameter settings that were automatically generated by CrocoDash. Carefully examine these parameters and make any necessary adjustments to fine-tune the model for your specific requirements. While CrocoDash aims to provide a solid starting point, further tuning and adjustments are typically necessary to improve the model for your use case.\n\nOnce you have reviewed and modified the parameters as needed, you can build and execute the case using the following commands:qcmd -- ./case.build\n./case.submit","type":"content","url":"/notebooks/projects/sample-crocodash-projects/drof#section-4-build-and-run-the-case","position":25},{"hierarchy":{"lvl1":"BGC-DROF-Tides-Chl-CICE Sample Project"},"type":"lvl1","url":"/notebooks/projects/sample-crocodash-projects/everything","position":0},{"hierarchy":{"lvl1":"BGC-DROF-Tides-Chl-CICE Sample Project"},"content":"Caution\n\nThis specific domain and set of options is unstable and fails after a couple timesteps. If you‚Äôre interested, feel free to explore this issue and crack this set of problems!\n\nIn this notebook, the code has the simplest regional ocean model setup possible through CrocoDash. In the comments and descriptions, we‚Äôll provide some of the ways you can add and build on this demo (recreate a specific domain - like GFDL‚Äôs NWA12, run with CICE - the sea-ice model in CESM, run with MARBL - the ocean BGC model in CESM, and beyond!)\n\nThis notebook is designed for new users of CrocoDash and CESM who want to set up a simple regional ocean model. By following the steps, you will learn how to generate a domain, configure a CESM case, prepare ocean forcing data, and run your simulation.\n\nA typical workflow of utilizing CrocoDash consists of four main steps:\n\nGenerate a regional MOM6 domain.\n\nCreate the CESM case.\n\nPrepare ocean forcing data.\n\nBuild and run the case.\n\nEvery specialization of CrocoDash will use this framework.\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/everything","position":1},{"hierarchy":{"lvl1":"BGC-DROF-Tides-Chl-CICE Sample Project","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl2","url":"/notebooks/projects/sample-crocodash-projects/everything#section-1-generate-a-regional-mom6-domain","position":2},{"hierarchy":{"lvl1":"BGC-DROF-Tides-Chl-CICE Sample Project","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"We begin by defining a regional MOM6 domain using CrocoDash. To do so, we first generate a horizontal grid. We then generate the topography by remapping an existing bathymetric dataset to our horizontal grid. Finally, we define a vertical grid.\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/everything#section-1-generate-a-regional-mom6-domain","position":3},{"hierarchy":{"lvl1":"BGC-DROF-Tides-Chl-CICE Sample Project","lvl3":"Step 1.1: Horizontal Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/everything#step-1-1-horizontal-grid","position":4},{"hierarchy":{"lvl1":"BGC-DROF-Tides-Chl-CICE Sample Project","lvl3":"Step 1.1: Horizontal Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"\n\nfrom CrocoDash.grid import Grid\n\ngrid = Grid.from_supergrid(\"/glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/CrocoDash/grids/cice_grids/replica_grid_3413.nc\")\ngrid.name = \"Arctic_Grid\"\n\n\n\nThe above cell is a way to generate a rectangular lat/lon grid through CrocoDash\n\nIf you‚Äôre interested in using an already generated regional grid, you can do that by passing in a supergrid, like in \n\nthis demo.\n\nIf you‚Äôre interested in subsetting an already generated global grid, you can do that by passing in lat/lon args, like in \n\nthis demo.\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/everything#step-1-1-horizontal-grid","position":5},{"hierarchy":{"lvl1":"BGC-DROF-Tides-Chl-CICE Sample Project","lvl3":"Step 1.2: Topography","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/everything#step-1-2-topography","position":6},{"hierarchy":{"lvl1":"BGC-DROF-Tides-Chl-CICE Sample Project","lvl3":"Step 1.2: Topography","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"The below three cells are a way to generate topography with a given grid using the \n\nGEBCO dataset.\n\nIf you‚Äôre interested in using a previously generated topography for your grid, you can do that by passing in a topography file, like in \n\nthis demo.\n\nfrom CrocoDash.topo import Topo\n\nbathymetry_path='/glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/CrocoDash/grids/cice_grids/bathy_3031.nc'\n\ntopo = Topo.from_topo_file(\n    grid = grid,\n    topo_file_path=bathymetry_path,\n    min_depth = 9.5,\n)\n\n\n\n# Validate that the topography looks right!\ntopo.depth.plot()\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/everything#step-1-2-topography","position":7},{"hierarchy":{"lvl1":"BGC-DROF-Tides-Chl-CICE Sample Project","lvl3":"Step 1.3: Vertical Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/everything#step-1-3-vertical-grid","position":8},{"hierarchy":{"lvl1":"BGC-DROF-Tides-Chl-CICE Sample Project","lvl3":"Step 1.3: Vertical Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"The below cell generates a vertical grid using a hyperbolic function. We can also create a uniform grid instance, or use a previously generated vertical grid, like in \n\nthis demo.\n\nfrom CrocoDash.vgrid import VGrid\n\nvgrid  = VGrid.hyperbolic(\n    nk = 75, # number of vertical levels\n    depth = topo.max_depth,\n    ratio=20.0 # target ratio of top to bottom layer thicknesses\n)\n\n\n\nimport matplotlib.pyplot as plt\nplt.close()\n# Create the plot\nfor depth in vgrid.z:\n    plt.axhline(y=depth, linestyle='-')  # Horizontal lines\n\nplt.ylim(max(vgrid.z) + 10, min(vgrid.z) - 10)  # Invert y-axis so deeper values go down\nplt.ylabel(\"Depth\")\nplt.title(\"Vertical Grid\")\nplt.show()\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/everything#step-1-3-vertical-grid","position":9},{"hierarchy":{"lvl1":"BGC-DROF-Tides-Chl-CICE Sample Project","lvl2":"SECTION 2: Create the CESM case"},"type":"lvl2","url":"/notebooks/projects/sample-crocodash-projects/everything#section-2-create-the-cesm-case","position":10},{"hierarchy":{"lvl1":"BGC-DROF-Tides-Chl-CICE Sample Project","lvl2":"SECTION 2: Create the CESM case"},"content":"After generating the MOM6 domain, the next step is to create a CESM case using CrocoDash. This process is straightforward and involves instantiating the CrocoDash Case object. The Case object requires the following inputs:\n\nCESM Source Directory: A local path to a compatible CESM source copy.\n\nCase Name: A unique name for the CESM case.\n\nInput Directory: The directory where all necessary input files will be written.\n\nMOM6 Domain Objects: The horizontal grid, topography, and vertical grid created in the previous section.\n\nProject ID: (Optional) A project ID, if required by the machine.\n\nCompset: The set of models to be used in the Case. Standalone Ocean, Ocean-BGC, Ocean-Seaice, Ocean-Runoff, etc...\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/everything#section-2-create-the-cesm-case","position":11},{"hierarchy":{"lvl1":"BGC-DROF-Tides-Chl-CICE Sample Project","lvl3":"Step 2.1: Specify case name and directories:","lvl2":"SECTION 2: Create the CESM case"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/everything#step-2-1-specify-case-name-and-directories","position":12},{"hierarchy":{"lvl1":"BGC-DROF-Tides-Chl-CICE Sample Project","lvl3":"Step 2.1: Specify case name and directories:","lvl2":"SECTION 2: Create the CESM case"},"content":"Begin by specifying the case name and the necessary directory paths. Ensure the CESM root directory points to your own local copy of CESM. Below is an example setup:\n\nfrom pathlib import Path\n\n\n\n# CESM case (experiment) name\ncasename = \"everything.arctic.1\"\n\n# CESM source root (Update this path accordingly!!!)\ncesmroot =\"/glade/work/mlevy/codes/CESM/cesm3_0_alpha07c_CROCO\"\n\n# Place where all your input files go \ninputdir = Path.home()/\"scratch\" / \"croc_input\" / casename\n    \n# CESM case directory\ncaseroot = Path.home() / \"croc_cases\" / casename\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/everything#step-2-1-specify-case-name-and-directories","position":13},{"hierarchy":{"lvl1":"BGC-DROF-Tides-Chl-CICE Sample Project","lvl3":"Step 2.2: Create the Case","lvl2":"SECTION 2: Create the CESM case"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/everything#step-2-2-create-the-case","position":14},{"hierarchy":{"lvl1":"BGC-DROF-Tides-Chl-CICE Sample Project","lvl3":"Step 2.2: Create the Case","lvl2":"SECTION 2: Create the CESM case"},"content":"To create the CESM case, instantiate the Case object as shown below. This will automatically set up the CESM case based on the provided inputs: The cesmroot argument specifies the path to your local CESM source directory.\nThe caseroot argument defines the directory where the case will be created. CrocoDash will handle all necessary namelist modifications and XML changes to align with the MOM6 domain objects generated earlier.\n\nThis is where we can change the model setup to what we want:\n\nInterested in BGC? Check out \n\nthe demo for how to modify this notebook for BGC.\n\nInterested in CICE? Check out \n\nthe demo for how to modify this notebook for CICE.\n\nInterested in Data Runoff from GLOFAS (or JRA)? Check out \n\nthe demo for how to modify this notebook for Data Runoff.\n\nfrom CrocoDash.case import Case\n\ncase = Case(\n    cesmroot = cesmroot,\n    caseroot = caseroot,\n    inputdir = inputdir,\n    ocn_grid = grid,\n    ocn_vgrid = vgrid,\n    ocn_topo = topo,\n    project = 'CESM0030',\n    override = True,\n    machine = \"derecho\",\n    compset = \"GR1850MARBL_JRA_GLOFAS\" # Change this as necessary to include whatever models you want. BGC? change MOM6 to MOM6%MARBL-BIO, CICE? Change SICE to CICE, GLOFAS Runoff? Change SROF to DROF%GLOFAS\n)\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/everything#step-2-2-create-the-case","position":15},{"hierarchy":{"lvl1":"BGC-DROF-Tides-Chl-CICE Sample Project","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl2","url":"/notebooks/projects/sample-crocodash-projects/everything#section-3-prepare-ocean-forcing-data","position":16},{"hierarchy":{"lvl1":"BGC-DROF-Tides-Chl-CICE Sample Project","lvl2":"Section 3: Prepare ocean forcing data"},"content":"We need to cut out our ocean forcing. The package expects an initial condition and one time-dependent segment per non-land boundary.\n\nIn this notebook, we are forcing with the Copernicus Marine ‚ÄúGlorys‚Äù reanalysis dataset.","type":"content","url":"/notebooks/projects/sample-crocodash-projects/everything#section-3-prepare-ocean-forcing-data","position":17},{"hierarchy":{"lvl1":"BGC-DROF-Tides-Chl-CICE Sample Project","lvl3":"Step 3.1 Configure Initial Conditions and Forcings","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/everything#step-3-1-configure-initial-conditions-and-forcings","position":18},{"hierarchy":{"lvl1":"BGC-DROF-Tides-Chl-CICE Sample Project","lvl3":"Step 3.1 Configure Initial Conditions and Forcings","lvl2":"Section 3: Prepare ocean forcing data"},"content":"Interested in adding tides? Check out \n\nthe demo.\n\nInterested in adding chlorophyll? Check out \n\nthe demo.\n\nIs the GLORYS data access function not working? Check out alternative options in \n\nthis demo.\n\nConfigure Forcings MUST be modified if BGC or Data Runoff are part of the compset (and will throw an error). Check out the demos to see the changes required: \n\nBGC Demo, \n\nDROF Demo.\n\ncase.configure_forcings(\n        date_range = [\"2000-01-01 00:00:00\", \"2000-02-01 00:00:00\"],\n        data_input_path = \"/glade/campaign/collections/cmip/CMIP6/CESM-HR/FOSI_BGC/HR/g.e22.TL319_t13.G1850ECOIAF_JRA_HR.4p2z.001/ocn/proc/tseries/month_1\",\n        product_name = \"CESM_OUTPUT\",\n        marbl_ic_filepath = \"/glade/campaign/collections/gdex/data/d651077/cesmdata/inputdata/ocn/mom/tx0.66v1/ecosys_jan_IC_omip_latlon_1x1_180W_c231221.nc\",\n        runoff_esmf_mesh_filepath = \"/glade/campaign/cesm/cesmdata/cseg/inputdata/ocn/mom/croc/rof/glofas/dis24/GLOFAS_esmf_mesh_v4.nc\",\n        global_river_nutrients_filepath = \"/glade/campaign/cesm/cesmdata/cseg/inputdata/ocn/mom/croc/rof/river_nutrients/river_nutrients.GNEWS_GNM.glofas.20250916.64bit.nc\",\n        tidal_constituents = ['M2','S2','N2','K2','K1','O1','P1','Q1','MM','MF'],\n        tpxo_elevation_filepath = \"/glade/u/home/manishrv/scratch/ice/croc/tpxo/h_tpxo9.v1.nc\",\n        tpxo_velocity_filepath = \"/glade/u/home/manishrv/scratch/ice/croc/tpxo/u_tpxo9.v1.nc\",\n        chl_processed_filepath = \"/glade/campaign/cesm/cesmdata/cseg/inputdata/ocn/mom/croc/chl/data/SeaWIFS.L3m.MC.CHL.chlor_a.0.25deg.nc\",\n)\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/everything#step-3-1-configure-initial-conditions-and-forcings","position":19},{"hierarchy":{"lvl1":"BGC-DROF-Tides-Chl-CICE Sample Project","lvl3":"Step 3.3: Process forcing data","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/everything#step-3-3-process-forcing-data","position":20},{"hierarchy":{"lvl1":"BGC-DROF-Tides-Chl-CICE Sample Project","lvl3":"Step 3.3: Process forcing data","lvl2":"Section 3: Prepare ocean forcing data"},"content":"In this final step, we call the process_forcings method of CrocoDash to cut out and interpolate the initial condition as well as all boundaries. CrocoDash also updates MOM6 runtime parameters and CESM xml variables accordingly.\n\ncase.process_forcings() # You can turn off specific processing if it's already been processed previously with process_*name* = False. Ex. case.process_forcings(process_chl = False)\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/everything#step-3-3-process-forcing-data","position":21},{"hierarchy":{"lvl1":"BGC-DROF-Tides-Chl-CICE Sample Project","lvl2":"Section 4: Build and run the case"},"type":"lvl2","url":"/notebooks/projects/sample-crocodash-projects/everything#section-4-build-and-run-the-case","position":22},{"hierarchy":{"lvl1":"BGC-DROF-Tides-Chl-CICE Sample Project","lvl2":"Section 4: Build and run the case"},"content":"After completing the previous steps, you are ready to build and run your CESM case. Begin by navigating to the case root directory specified during the case creation. Before proceeding, review the user_nl_mom file located in the case directory. This file contains MOM6 parameter settings that were automatically generated by CrocoDash. Carefully examine these parameters and make any necessary adjustments to fine-tune the model for your specific requirements. While CrocoDash aims to provide a solid starting point, further tuning and adjustments are typically necessary to improve the model for your use case.\n\nOnce you have reviewed and modified the parameters as needed, you can build and execute the case using the following commands:qcmd -- ./case.build\n./case.submit","type":"content","url":"/notebooks/projects/sample-crocodash-projects/everything#section-4-build-and-run-the-case","position":23},{"hierarchy":{"lvl1":"NWA12 Sample Project"},"type":"lvl1","url":"/notebooks/projects/sample-crocodash-projects/nwa","position":0},{"hierarchy":{"lvl1":"NWA12 Sample Project"},"content":"In this notebook, the code has the simplest regional ocean model setup possible through CrocoDash. In the comments and descriptions, we‚Äôll provide some of the ways you can add and build on this demo (recreate a specific domain - like GFDL‚Äôs NWA12, run with CICE - the sea-ice model in CESM, run with MARBL - the ocean BGC model in CESM, and beyond!)\n\nThis notebook is designed for new users of CrocoDash and CESM who want to set up a simple regional ocean model. By following the steps, you will learn how to generate a domain, configure a CESM case, prepare ocean forcing data, and run your simulation.\n\nA typical workflow of utilizing CrocoDash consists of four main steps:\n\nGenerate a regional MOM6 domain.\n\nCreate the CESM case.\n\nPrepare ocean forcing data.\n\nBuild and run the case.\n\nEvery specialization of CrocoDash will use this framework.\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/nwa","position":1},{"hierarchy":{"lvl1":"NWA12 Sample Project","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl2","url":"/notebooks/projects/sample-crocodash-projects/nwa#section-1-generate-a-regional-mom6-domain","position":2},{"hierarchy":{"lvl1":"NWA12 Sample Project","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"We begin by defining a regional MOM6 domain using CrocoDash. To do so, we first generate a horizontal grid. We then generate the topography by remapping an existing bathymetric dataset to our horizontal grid. Finally, we define a vertical grid.\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/nwa#section-1-generate-a-regional-mom6-domain","position":3},{"hierarchy":{"lvl1":"NWA12 Sample Project","lvl3":"Step 1.1: Horizontal Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/nwa#step-1-1-horizontal-grid","position":4},{"hierarchy":{"lvl1":"NWA12 Sample Project","lvl3":"Step 1.1: Horizontal Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"\n\nfrom CrocoDash.grid import Grid\n\ngrid = Grid.from_supergrid(\"/glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/CrocoDash/grids/nwa/ocean_hgrid.nc\")\n\ngrid.name = \"NWA\"\n\n\n\nThe above cell is a way to generate a rectangular lat/lon grid through CrocoDash\n\nIf you‚Äôre interested in using an already generated regional grid, you can do that by passing in a supergrid, like in \n\nthis demo.\n\nIf you‚Äôre interested in subsetting an already generated global grid, you can do that by passing in lat/lon args, like in \n\nthis demo.\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/nwa#step-1-1-horizontal-grid","position":5},{"hierarchy":{"lvl1":"NWA12 Sample Project","lvl3":"Step 1.2: Topography","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/nwa#step-1-2-topography","position":6},{"hierarchy":{"lvl1":"NWA12 Sample Project","lvl3":"Step 1.2: Topography","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"The below three cells are a way to generate topography with a given grid using the \n\nGEBCO dataset.\n\nIf you‚Äôre interested in using a previously generated topography for your grid, you can do that by passing in a topography file, like in \n\nthis demo.\n\nfrom CrocoDash.topo import Topo\n\nbathymetry_path='/glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/CrocoDash/grids/nwa/ocean_topo.nc'\n\ntopo = Topo.from_topo_file(\n    grid = grid,\n    topo_file_path=bathymetry_path,\n    min_depth = 9.5,\n)\n\n\n\n# Validate that the topography looks right!\ntopo.depth.plot()\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/nwa#step-1-2-topography","position":7},{"hierarchy":{"lvl1":"NWA12 Sample Project","lvl3":"Step 1.3: Vertical Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/nwa#step-1-3-vertical-grid","position":8},{"hierarchy":{"lvl1":"NWA12 Sample Project","lvl3":"Step 1.3: Vertical Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"The below cell generates a vertical grid using a hyperbolic function. We can also create a uniform grid instance, or use a previously generated vertical grid, like in \n\nthis demo.\n\nfrom CrocoDash.vgrid import VGrid\n\nvgrid_path='/glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/CrocoDash/grids/nwa/vgrid_75_2m.nc'\n\nvgrid  = VGrid.from_file(vgrid_path)\n\n\n\n\nimport matplotlib.pyplot as plt\nplt.close()\n# Create the plot\nfor depth in vgrid.z:\n    plt.axhline(y=depth, linestyle='-')  # Horizontal lines\n\nplt.ylim(max(vgrid.z) + 10, min(vgrid.z) - 10)  # Invert y-axis so deeper values go down\nplt.ylabel(\"Depth\")\nplt.title(\"Vertical Grid\")\nplt.show()\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/nwa#step-1-3-vertical-grid","position":9},{"hierarchy":{"lvl1":"NWA12 Sample Project","lvl2":"SECTION 2: Create the CESM case"},"type":"lvl2","url":"/notebooks/projects/sample-crocodash-projects/nwa#section-2-create-the-cesm-case","position":10},{"hierarchy":{"lvl1":"NWA12 Sample Project","lvl2":"SECTION 2: Create the CESM case"},"content":"After generating the MOM6 domain, the next step is to create a CESM case using CrocoDash. This process is straightforward and involves instantiating the CrocoDash Case object. The Case object requires the following inputs:\n\nCESM Source Directory: A local path to a compatible CESM source copy.\n\nCase Name: A unique name for the CESM case.\n\nInput Directory: The directory where all necessary input files will be written.\n\nMOM6 Domain Objects: The horizontal grid, topography, and vertical grid created in the previous section.\n\nProject ID: (Optional) A project ID, if required by the machine.\n\nCompset: The set of models to be used in the Case. Standalone Ocean, Ocean-BGC, Ocean-Seaice, Ocean-Runoff, etc...\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/nwa#section-2-create-the-cesm-case","position":11},{"hierarchy":{"lvl1":"NWA12 Sample Project","lvl3":"Step 2.1: Specify case name and directories:","lvl2":"SECTION 2: Create the CESM case"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/nwa#step-2-1-specify-case-name-and-directories","position":12},{"hierarchy":{"lvl1":"NWA12 Sample Project","lvl3":"Step 2.1: Specify case name and directories:","lvl2":"SECTION 2: Create the CESM case"},"content":"Begin by specifying the case name and the necessary directory paths. Ensure the CESM root directory points to your own local copy of CESM. Below is an example setup:\n\nfrom pathlib import Path\n\n\n\n# CESM case (experiment) name\ncasename = \"nwa.cg.1\"\n\n# CESM source root (Update this path accordingly!!!)\ncesmroot =\"/glade/work/mlevy/codes/CESM/cesm3_0_alpha07c_CROCO/\"\n\n# Place where all your input files go \ninputdir = Path.home()/\"scratch\" / \"croc_input\" / casename\n    \n# CESM case directory\ncaseroot = Path.home() / \"croc_cases\" / casename\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/nwa#step-2-1-specify-case-name-and-directories","position":13},{"hierarchy":{"lvl1":"NWA12 Sample Project","lvl3":"Step 2.2: Create the Case","lvl2":"SECTION 2: Create the CESM case"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/nwa#step-2-2-create-the-case","position":14},{"hierarchy":{"lvl1":"NWA12 Sample Project","lvl3":"Step 2.2: Create the Case","lvl2":"SECTION 2: Create the CESM case"},"content":"To create the CESM case, instantiate the Case object as shown below. This will automatically set up the CESM case based on the provided inputs: The cesmroot argument specifies the path to your local CESM source directory.\nThe caseroot argument defines the directory where the case will be created. CrocoDash will handle all necessary namelist modifications and XML changes to align with the MOM6 domain objects generated earlier.\n\nThis is where we can change the model setup to what we want:\n\nInterested in BGC? Check out \n\nthe demo for how to modify this notebook for BGC.\n\nInterested in CICE? Check out \n\nthe demo for how to modify this notebook for CICE.\n\nInterested in Data Runoff from GLOFAS (or JRA)? Check out \n\nthe demo for how to modify this notebook for Data Runoff.\n\nfrom CrocoDash.case import Case\n\ncase = Case(\n    cesmroot = cesmroot,\n    caseroot = caseroot,\n    inputdir = inputdir,\n    ocn_grid = grid,\n    ocn_vgrid = vgrid,\n    ocn_topo = topo,\n    project = 'CESM0030',\n    override = True,\n    machine = \"derecho\",\n    compset = \"CR_JRA_GLOFAS\" # Change this as necessary to include whatever models you want. BGC? change MOM6 to MOM6%MARBL-BIO, CICE? Change SICE to CICE, GLOFAS Runoff? Change SROF to DROF%GLOFAS\n)\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/nwa#step-2-2-create-the-case","position":15},{"hierarchy":{"lvl1":"NWA12 Sample Project","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl2","url":"/notebooks/projects/sample-crocodash-projects/nwa#section-3-prepare-ocean-forcing-data","position":16},{"hierarchy":{"lvl1":"NWA12 Sample Project","lvl2":"Section 3: Prepare ocean forcing data"},"content":"We need to cut out our ocean forcing. The package expects an initial condition and one time-dependent segment per non-land boundary.\n\nIn this notebook, we are forcing with the Copernicus Marine ‚ÄúGlorys‚Äù reanalysis dataset.","type":"content","url":"/notebooks/projects/sample-crocodash-projects/nwa#section-3-prepare-ocean-forcing-data","position":17},{"hierarchy":{"lvl1":"NWA12 Sample Project","lvl3":"Step 3.1 Configure Initial Conditions and Forcings","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/nwa#step-3-1-configure-initial-conditions-and-forcings","position":18},{"hierarchy":{"lvl1":"NWA12 Sample Project","lvl3":"Step 3.1 Configure Initial Conditions and Forcings","lvl2":"Section 3: Prepare ocean forcing data"},"content":"Interested in adding tides? Check out \n\nthe demo.\n\nInterested in adding chlorophyll? Check out \n\nthe demo.\n\nIs the GLORYS data access function not working? Check out alternative options in \n\nthis demo.\n\nConfigure Forcings MUST be modified if BGC or Data Runoff are part of the compset (and will throw an error). Check out the demos to see the changes required: \n\nBGC Demo, \n\nDROF Demo.\n\ncase.configure_forcings(\n    date_range = [\"2000-01-01 00:00:00\", \"2000-01-06 00:00:00\"],\n    tidal_constituents = ['M2','S2','N2','K2','K1','O1','P1','Q1','MM','MF'],\n    tpxo_elevation_filepath = \"/glade/u/home/manishrv/scratch/ice/croc/tpxo/h_tpxo9.v1.nc\",\n    tpxo_velocity_filepath = \"/glade/u/home/manishrv/scratch/ice/croc/tpxo/u_tpxo9.v1.nc\",\n    chl_processed_filepath = \"/glade/campaign/cesm/cesmdata/cseg/inputdata/ocn/mom/croc/chl/data/SeaWIFS.L3m.MC.CHL.chlor_a.0.25deg.nc\",\n    runoff_esmf_mesh_filepath = \"/glade/campaign/cesm/cesmdata/cseg/inputdata/ocn/mom/croc/rof/glofas/dis24/GLOFAS_esmf_mesh_v4.nc\",\n    boundaries = [\"south\", \"north\", \"east\"],\n    function_name = \"get_glorys_data_from_rda\",\n)\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/nwa#step-3-1-configure-initial-conditions-and-forcings","position":19},{"hierarchy":{"lvl1":"NWA12 Sample Project","lvl3":"Step 3.3: Process forcing data","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl3","url":"/notebooks/projects/sample-crocodash-projects/nwa#step-3-3-process-forcing-data","position":20},{"hierarchy":{"lvl1":"NWA12 Sample Project","lvl3":"Step 3.3: Process forcing data","lvl2":"Section 3: Prepare ocean forcing data"},"content":"In this final step, we call the process_forcings method of CrocoDash to cut out and interpolate the initial condition as well as all boundaries. CrocoDash also updates MOM6 runtime parameters and CESM xml variables accordingly.\n\ncase.process_forcings() # You can turn off specific processing if it's already been processed previously with process_*name* = False. Ex. case.process_forcings(process_chl = False)\n\n\n\n# If you'd like, align the forcing\n\nimport shutil\nsource_file = \"/glade/u/home/manishrv/croc_cases/gustavo-glorys-2/datm.streams.xml\"\ndestination_file = caseroot/\"datm.streams.xml\"\nshutil.copy(source_file, destination_file)\n\n\n\n","type":"content","url":"/notebooks/projects/sample-crocodash-projects/nwa#step-3-3-process-forcing-data","position":21},{"hierarchy":{"lvl1":"NWA12 Sample Project","lvl2":"Section 4: Build and run the case"},"type":"lvl2","url":"/notebooks/projects/sample-crocodash-projects/nwa#section-4-build-and-run-the-case","position":22},{"hierarchy":{"lvl1":"NWA12 Sample Project","lvl2":"Section 4: Build and run the case"},"content":"After completing the previous steps, you are ready to build and run your CESM case. Begin by navigating to the case root directory specified during the case creation. Before proceeding, review the user_nl_mom file located in the case directory. This file contains MOM6 parameter settings that were automatically generated by CrocoDash. Carefully examine these parameters and make any necessary adjustments to fine-tune the model for your specific requirements. While CrocoDash aims to provide a solid starting point, further tuning and adjustments are typically necessary to improve the model for your use case.\n\nOnce you have reviewed and modified the parameters as needed, you can build and execute the case using the following commands:qcmd -- ./case.build\n./case.submit","type":"content","url":"/notebooks/projects/sample-crocodash-projects/nwa#section-4-build-and-run-the-case","position":23},{"hierarchy":{"lvl1":"Tutorial 1: Model-Observation comparison with MOM6 and CrocoLake"},"type":"lvl1","url":"/notebooks/projects/tutorial1-mom6-cl-comparison-1","position":0},{"hierarchy":{"lvl1":"Tutorial 1: Model-Observation comparison with MOM6 and CrocoLake"},"content":"The goal of this tutorial is to get familiar with the basics of using CrocoCamp to interpolate MOM6 output onto the space of observations stored in CrocoLake. For this tutorial we will use output from a MOM6 run already stored on NCAR‚Äôs HPC system, and CrocoLake observation files already in obs_sequence format and also stored on NCAR‚Äôs HPC system.","type":"content","url":"/notebooks/projects/tutorial1-mom6-cl-comparison-1","position":1},{"hierarchy":{"lvl1":"Tutorial 1: Model-Observation comparison with MOM6 and CrocoLake","lvl3":"Installing CrocoCamp"},"type":"lvl3","url":"/notebooks/projects/tutorial1-mom6-cl-comparison-1#installing-crococamp","position":2},{"hierarchy":{"lvl1":"Tutorial 1: Model-Observation comparison with MOM6 and CrocoLake","lvl3":"Installing CrocoCamp"},"content":"If you don‚Äôt have CrocoCamp set up yet, \n\nhere are the instructions to install it on NCAR‚Äôs HPC. If you have any issues or need to install it on a different machine, please contact \n\nenrico‚Äã.milanese@whoi‚Äã.edu.\n\n","type":"content","url":"/notebooks/projects/tutorial1-mom6-cl-comparison-1#installing-crococamp","position":3},{"hierarchy":{"lvl1":"Tutorial 1: Model-Observation comparison with MOM6 and CrocoLake","lvl2":"Running the workflow"},"type":"lvl2","url":"/notebooks/projects/tutorial1-mom6-cl-comparison-1#running-the-workflow","position":4},{"hierarchy":{"lvl1":"Tutorial 1: Model-Observation comparison with MOM6 and CrocoLake","lvl2":"Running the workflow"},"content":"Running the workflow to interpolate the model is quite simple: create a WorkflowModelObs instance using a configuration file, and then call its run() method. For this tutorial we will use the configuration file provided config_tutorial_1.yaml in the tutorial folder. A template file to use as reference is also provided at ../configs/config_template.yaml.\n\nWhile running, CrocoCamp generates temporary input files that tell DART‚Äôs perfect_model_obs executable where to find MOM6 and CrocoLake data to perform the interpolation. You need to be running this notebook on Casper, as the configuration file points to DART‚Äôs installation on that machine.\n\nfrom crococamp.workflows import WorkflowModelObs\n\n# Create and run workflow to interpolate MOM6 model onto World Ocean Database obs space\nworkflow_crocolake = WorkflowModelObs.from_config_file('config_tutorial_1.yaml')\nworkflow_crocolake.run() #use flag clear_output=True if you want to re-run it and automatically clean all previous output\n\n\n\n","type":"content","url":"/notebooks/projects/tutorial1-mom6-cl-comparison-1#running-the-workflow","position":5},{"hierarchy":{"lvl1":"Tutorial 1: Model-Observation comparison with MOM6 and CrocoLake","lvl2":"Displaying the interactive map"},"type":"lvl2","url":"/notebooks/projects/tutorial1-mom6-cl-comparison-1#displaying-the-interactive-map","position":6},{"hierarchy":{"lvl1":"Tutorial 1: Model-Observation comparison with MOM6 and CrocoLake","lvl2":"Displaying the interactive map"},"content":"CrocoCamp generates a parquet dataset that contains the values of the WOD observations, the MOM6 model data interpolated onto the observations space, and some basic statistics.\n\nThe dataframe by default is stored to model_obs_df (as a dask dataframe), and CrocoCamp offers tools to access all data, only the successful interpolations, or only the failed interpolations. For now, let‚Äôs load only the interpolations that succeeded, which are ~98% of the total (see output message from previous cell):\n\ngood_model_obs_df = workflow_crocolake.get_good_model_obs_df(compute=True) # compute=True triggers the compute of the dask dataframe, returning a pandas dataframe with data loaded in memory\ngood_model_obs_df.head()                                                   # displays first 5 rows in the dataframe\n\n\n\nLoading the interactive map to explore the succesfull interpolations is as simple as importing the widget and passing the dataframe to it:\n\nfrom crococamp.viz import InteractiveWidgetMap\n# Create an interactive map widget to visualize model-observation comparisons\n# The widget provides controls for selecting variables, observation types, and time ranges\nwidget = InteractiveWidgetMap(good_model_obs_df)\nwidget.setup()\n\n\n\n","type":"content","url":"/notebooks/projects/tutorial1-mom6-cl-comparison-1#displaying-the-interactive-map","position":7},{"hierarchy":{"lvl1":"Tutorial 1: Model-Observation comparison with MOM6 and CrocoLake","lvl2":"Interpolation errors"},"type":"lvl2","url":"/notebooks/projects/tutorial1-mom6-cl-comparison-1#interpolation-errors","position":8},{"hierarchy":{"lvl1":"Tutorial 1: Model-Observation comparison with MOM6 and CrocoLake","lvl2":"Interpolation errors"},"content":"The dataframe built by CrocoCamp during WorkflowModelObs.run() contains the column interpolated_model_QC, which stores information about the quality of the interpolation. The value is set by DART‚Äôs perfect_model_obs program, and you can find more information about it \n\nhere and \n\nhere. In general, for this workflow we want QC‚â§2, and indeed the method get_good_model_obs_df() that we used earlier uses this criterion.\n\nLet‚Äôs now have a look at the failed interpolations by loading the data and plotting it:\n\nfailed_model_obs_df = workflow_crocolake.get_failed_model_obs_df(compute=True) # compute=True triggers the compute of the dask dataframe, returning a pandas dataframe with data loaded in memory\nfailed_model_obs_df.head()                                                     # displays first 5 rows in the dataframe\n\n\n\nwidget_failed = InteractiveWidgetMap(good_model_obs_df)\nwidget_failed.setup()\n\n\n\nWe note two things:\n\nwhen an interpolation fails, a value of -888888 is assigned to the interpolated model;\n\nfrom head(), we see values of QC greater than 1000.\n\nthe failed observations are close to the model boundaries (north and east) or to land (west);\n\nThe following command gives us the unique values of model_QC:\n\nfailed_model_obs_df['interpolated_model_QC'].unique()\n\n\n\nWe should have only values equal to 1018. Values greater than 1000 are given as 1000 + failure_code, with failure_code from \n\nhere. In this case, the QC flag 1018 indicates that one or more grid points required for the interpolation are not in the basin, so the interpolation cannot be performed. This is consistent with the visual inspection from the map, where we noticed that failed interpolations are closed to model boundary and to the sea/land border.\n\nFinally, if you want to load all the data at once, you can use the following command:\n\n# Load the parquet dataset generated by the workflow above\nmodel_obs_df = workflow_crocolake.get_all_model_obs_df(compute=True) # compute=True triggers the compute of the dask dataframe, returning a pandas dataframe with data loaded in memory\nmodel_obs_df.head() # displays first 5 rows in the dataframe\n\n","type":"content","url":"/notebooks/projects/tutorial1-mom6-cl-comparison-1#interpolation-errors","position":9},{"hierarchy":{"lvl1":"Tutorial 1: Model-Observation comparison with MOM6 and CrocoLake"},"type":"lvl1","url":"/notebooks/projects/tutorial1-mom6-cl-comparison-1","position":0},{"hierarchy":{"lvl1":"Tutorial 1: Model-Observation comparison with MOM6 and CrocoLake"},"content":"The goal of this tutorial is to get familiar with the basics of using CrocoCamp to interpolate MOM6 output onto the space of observations stored in CrocoLake. For this tutorial we will use output from a MOM6 run already stored on NCAR‚Äôs HPC system, and CrocoLake observation files already in obs_sequence format and also stored on NCAR‚Äôs HPC system.","type":"content","url":"/notebooks/projects/tutorial1-mom6-cl-comparison-1","position":1},{"hierarchy":{"lvl1":"Tutorial 1: Model-Observation comparison with MOM6 and CrocoLake","lvl3":"Installing CrocoCamp"},"type":"lvl3","url":"/notebooks/projects/tutorial1-mom6-cl-comparison-1#installing-crococamp","position":2},{"hierarchy":{"lvl1":"Tutorial 1: Model-Observation comparison with MOM6 and CrocoLake","lvl3":"Installing CrocoCamp"},"content":"If you don‚Äôt have CrocoCamp set up yet, \n\nhere are the instructions to install it on NCAR‚Äôs HPC. If you have any issues or need to install it on a different machine, please contact \n\nenrico‚Äã.milanese@whoi‚Äã.edu.\n\n","type":"content","url":"/notebooks/projects/tutorial1-mom6-cl-comparison-1#installing-crococamp","position":3},{"hierarchy":{"lvl1":"Tutorial 1: Model-Observation comparison with MOM6 and CrocoLake","lvl2":"Running the workflow"},"type":"lvl2","url":"/notebooks/projects/tutorial1-mom6-cl-comparison-1#running-the-workflow","position":4},{"hierarchy":{"lvl1":"Tutorial 1: Model-Observation comparison with MOM6 and CrocoLake","lvl2":"Running the workflow"},"content":"Running the workflow to interpolate the model is quite simple: create a WorkflowModelObs instance using a configuration file, and then call its run() method. For this tutorial we will use the configuration file provided config_tutorial_1.yaml in the tutorial folder. A template file to use as reference is also provided at ../configs/config_template.yaml.\n\nWhile running, CrocoCamp generates temporary input files that tell DART‚Äôs perfect_model_obs executable where to find MOM6 and CrocoLake data to perform the interpolation. You need to be running this notebook on Casper, as the configuration file points to DART‚Äôs installation on that machine.\n\nfrom crococamp.workflows import WorkflowModelObs\n\n# Create and run workflow to interpolate MOM6 model onto World Ocean Database obs space\nworkflow_crocolake = WorkflowModelObs.from_config_file('config_tutorial_1.yaml')\nworkflow_crocolake.run() #use flag clear_output=True if you want to re-run it and automatically clean all previous output\n\n\n\n","type":"content","url":"/notebooks/projects/tutorial1-mom6-cl-comparison-1#running-the-workflow","position":5},{"hierarchy":{"lvl1":"Tutorial 1: Model-Observation comparison with MOM6 and CrocoLake","lvl2":"Displaying the interactive map"},"type":"lvl2","url":"/notebooks/projects/tutorial1-mom6-cl-comparison-1#displaying-the-interactive-map","position":6},{"hierarchy":{"lvl1":"Tutorial 1: Model-Observation comparison with MOM6 and CrocoLake","lvl2":"Displaying the interactive map"},"content":"CrocoCamp generates a parquet dataset that contains the values of the WOD observations, the MOM6 model data interpolated onto the observations space, and some basic statistics.\n\nThe dataframe by default is stored to model_obs_df (as a dask dataframe), and CrocoCamp offers tools to access all data, only the successful interpolations, or only the failed interpolations. For now, let‚Äôs load only the interpolations that succeeded, which are ~98% of the total (see output message from previous cell):\n\ngood_model_obs_df = workflow_crocolake.get_good_model_obs_df(compute=True) # compute=True triggers the compute of the dask dataframe, returning a pandas dataframe with data loaded in memory\ngood_model_obs_df.head()                                                   # displays first 5 rows in the dataframe\n\n\n\nLoading the interactive map to explore the succesfull interpolations is as simple as importing the widget and passing the dataframe to it:\n\nfrom crococamp.viz import InteractiveWidgetMap\n# Create an interactive map widget to visualize model-observation comparisons\n# The widget provides controls for selecting variables, observation types, and time ranges\nwidget = InteractiveWidgetMap(good_model_obs_df)\nwidget.setup()\n\n\n\n","type":"content","url":"/notebooks/projects/tutorial1-mom6-cl-comparison-1#displaying-the-interactive-map","position":7},{"hierarchy":{"lvl1":"Tutorial 1: Model-Observation comparison with MOM6 and CrocoLake","lvl2":"Interpolation errors"},"type":"lvl2","url":"/notebooks/projects/tutorial1-mom6-cl-comparison-1#interpolation-errors","position":8},{"hierarchy":{"lvl1":"Tutorial 1: Model-Observation comparison with MOM6 and CrocoLake","lvl2":"Interpolation errors"},"content":"The dataframe built by CrocoCamp during WorkflowModelObs.run() contains the column interpolated_model_QC, which stores information about the quality of the interpolation. The value is set by DART‚Äôs perfect_model_obs program, and you can find more information about it \n\nhere and \n\nhere. In general, for this workflow we want QC‚â§2, and indeed the method get_good_model_obs_df() that we used earlier uses this criterion.\n\nLet‚Äôs now have a look at the failed interpolations by loading the data and plotting it:\n\nfailed_model_obs_df = workflow_crocolake.get_failed_model_obs_df(compute=True) # compute=True triggers the compute of the dask dataframe, returning a pandas dataframe with data loaded in memory\nfailed_model_obs_df.head()                                                     # displays first 5 rows in the dataframe\n\n\n\nwidget_failed = InteractiveWidgetMap(good_model_obs_df)\nwidget_failed.setup()\n\n\n\nWe note two things:\n\nwhen an interpolation fails, a value of -888888 is assigned to the interpolated model;\n\nfrom head(), we see values of QC greater than 1000.\n\nthe failed observations are close to the model boundaries (north and east) or to land (west);\n\nThe following command gives us the unique values of model_QC:\n\nfailed_model_obs_df['interpolated_model_QC'].unique()\n\n\n\nWe should have only values equal to 1018. Values greater than 1000 are given as 1000 + failure_code, with failure_code from \n\nhere. In this case, the QC flag 1018 indicates that one or more grid points required for the interpolation are not in the basin, so the interpolation cannot be performed. This is consistent with the visual inspection from the map, where we noticed that failed interpolations are closed to model boundary and to the sea/land border.\n\nFinally, if you want to load all the data at once, you can use the following command:\n\n# Load the parquet dataset generated by the workflow above\nmodel_obs_df = workflow_crocolake.get_all_model_obs_df(compute=True) # compute=True triggers the compute of the dask dataframe, returning a pandas dataframe with data loaded in memory\nmodel_obs_df.head() # displays first 5 rows in the dataframe\n\n","type":"content","url":"/notebooks/projects/tutorial1-mom6-cl-comparison-1#interpolation-errors","position":9},{"hierarchy":{"lvl1":"Tutorial 2: Model-Observation comparison with MOM6 and a single Argo float from CrocoLake"},"type":"lvl1","url":"/notebooks/projects/tutorial2-mom6-cl-comparison-float-1","position":0},{"hierarchy":{"lvl1":"Tutorial 2: Model-Observation comparison with MOM6 and a single Argo float from CrocoLake"},"content":"The goal of this tutorial is to learn how to generate your own \n\nobs_seq files from CrocoLake and learn about another interactive widget: the interactive profile. We will select a specific Argo float, interpolate our model output to the temperature and salinity values of that float, and compare the interpolated model and the float‚Äôs profiles.","type":"content","url":"/notebooks/projects/tutorial2-mom6-cl-comparison-float-1","position":1},{"hierarchy":{"lvl1":"Tutorial 2: Model-Observation comparison with MOM6 and a single Argo float from CrocoLake","lvl4":"Installing CrocoCamp"},"type":"lvl4","url":"/notebooks/projects/tutorial2-mom6-cl-comparison-float-1#installing-crococamp","position":2},{"hierarchy":{"lvl1":"Tutorial 2: Model-Observation comparison with MOM6 and a single Argo float from CrocoLake","lvl4":"Installing CrocoCamp"},"content":"If you don‚Äôt have CrocoCamp set up yet, \n\nhere are the instructions to install it on NCAR‚Äôs HPC. If you have any issues or need to install it on a different machine, please contact \n\nenrico‚Äã.milanese@whoi‚Äã.edu.","type":"content","url":"/notebooks/projects/tutorial2-mom6-cl-comparison-float-1#installing-crococamp","position":3},{"hierarchy":{"lvl1":"Tutorial 2: Model-Observation comparison with MOM6 and a single Argo float from CrocoLake","lvl2":"Generating your own obs_seq.in files"},"type":"lvl2","url":"/notebooks/projects/tutorial2-mom6-cl-comparison-float-1#generating-your-own-obs-seq-in-files","position":4},{"hierarchy":{"lvl1":"Tutorial 2: Model-Observation comparison with MOM6 and a single Argo float from CrocoLake","lvl2":"Generating your own obs_seq.in files"},"content":"CrocoLake is a flexible parquet dataset that is fast and easy to access and filter. For this workshop, a copy of the CrocoLake version with physical variables is stored at /glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/CrocoCamp/CrocoLakePHY:\n\ncrocolake_path = '/glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/CrocoCamp/CrocoLakePHY'\n\nimport os\nbasename = \"myCL_obs_seq_\"\noutdir = \"$WORK/crocodile_2025/CrocoCamp/tutorial_2/in_CL/\"\nbasename = os.path.expandvars(outdir+basename)\noutdir = os.path.expandvars(outdir)\n\n\n\nIn the following cell you have the code that I used to generate the observation files that we used in Tutorial 1. Have a look at the code, in particular at how you to select variables and define filters. Once you‚Äôre ready, create a new cell below, copy-paste the code, and add a filter so that you only store the data recorded by the Argo float with identifier 1900256. Also change the file name to add the float number to it. Note: in CrocoLake, the platform identifier is stored as a string in the variable 'PLATFORM_NUMBER'.\n\nimport datetime\nfrom convert_crocolake_obs import ObsSequence\n\n# define horizontal region\nLAT0 = 5\nLAT1 = 60\nLON0 = -100\nLON1 = -30\n\n# define variables to import from CrocoLake\nselected_variables = [\n    \"DB_NAME\",  # ARGO, GLODAP, SprayGliders, OleanderXBT, Saildrones\n    \"JULD\", # this contains timestamp\n    \"LATITUDE\",\n    \"LONGITUDE\",\n    \"PRES\", # This will be automatically converted to depths in meters\n    \"TEMP\",\n    \"PRES_QC\",\n    \"TEMP_QC\",\n    \"PRES_ERROR\",\n    \"TEMP_ERROR\",\n    \"PSAL\",\n    \"PSAL_QC\",\n    \"PSAL_ERROR\"\n]\n\n# month and year are constant in out case\nyear0 = 2010\nmonth0 = 5\n\n# we loop to generate one file per day\nfor j in range(10):\n\n    # set date range\n    day0 = 1+j\n    day1 = day0+1\n    date0 = datetime.datetime(year0, month0, day0, 0, 0, 0)\n    date1 = datetime.datetime(year0, month0, day1, 0, 0, 0)\n    print(f\"Converting obs between {date0} and {date1}\")\n\n    # this defines AND filters, i.e. we want to load each observation that has latitude within the given range AND longitude within the given range, etc.\n    # to exclude NaNs, impose a range to a variable\n    and_filters = (\n        (\"LATITUDE\",'>',LAT0),  (\"LATITUDE\",'<',LAT1),\n        (\"LONGITUDE\",'>',LON0), (\"LONGITUDE\",'<',LON1),\n        (\"PRES\",'>',-1e30), (\"PRES\",'<',1e30),\n        (\"JULD\",\">\",date0), (\"JULD\",\"<\",date1),\n    )\n\n    # this adds OR conditions to the and_filters, i.e. we want to load all observations that statisfy the AND conditions above, AND that have finite salinity OR temperature values\n    db_filters = [\n        list(and_filters) + [(\"PSAL\", \">\", -1e30), (\"PSAL\", \"<\", 1e30)],\n        list(and_filters) + [(\"TEMP\", \">\", -1e30), (\"TEMP\", \"<\", 1e30)],\n    ]\n\n    # generate output filename\n    obs_seq_out = basename + f\".{year0}{month0:02d}{day0:02d}.out\"\n\n    # generate obs_seq.in file\n    obsSeq = ObsSequence(\n        crocolake_path,\n        selected_variables,\n        db_filters,\n        obs_seq_out=obs_seq_out,\n        loose=True\n    )\n    obsSeq.write_obs_seq()\n\nThe following cell contains the solution:\n\nimport datetime\nfrom convert_crocolake_obs import ObsSequence\n\n# define horizontal region\nLAT0 = 5\nLAT1 = 60\nLON0 = -100\nLON1 = -30\n\n# define variables to import from CrocoLake\nselected_variables = [\n    \"DB_NAME\",  # ARGO, GLODAP, SprayGliders, OleanderXBT, Saildrones\n    \"JULD\", # this contains timestamp\n    \"LATITUDE\",\n    \"LONGITUDE\",\n    \"PRES\", # This will be automatically converted to depths in meters\n    \"TEMP\",\n    \"PRES_QC\",\n    \"TEMP_QC\",\n    \"PRES_ERROR\",\n    \"TEMP_ERROR\",\n    \"PSAL\",\n    \"PSAL_QC\",\n    \"PSAL_ERROR\"\n]\n\n# month and year are constant in out case\nyear0 = 2010\nmonth0 = 5\nwmo_id = str(1900256)\nbasename = basename + f\".ARGO_{wmo_id}\"\nif not os.path.exists(outdir):\n    os.makedirs(outdir, exist_ok=True)\n\n# we loop to generate one file per day\nfor j in range(10):\n\n    # set date range\n    day0 = 1+j\n    day1 = day0+1\n    date0 = datetime.datetime(year0, month0, day0, 0, 0, 0)\n    date1 = datetime.datetime(year0, month0, day1, 0, 0, 0)\n    print(f\"Converting obs between {date0} and {date1}\")\n\n    # this defines AND filters, i.e. we want to load each observation that has latitude within the given range AND longitude within the given range, etc.\n    # to exclude NaNs, impose a range to a variable\n    and_filters = (\n        (\"LATITUDE\",'>',LAT0),  (\"LATITUDE\",'<',LAT1),\n        (\"LONGITUDE\",'>',LON0), (\"LONGITUDE\",'<',LON1),\n        (\"PRES\",'>',-1e30), (\"PRES\",'<',1e30),\n        (\"JULD\",\">\",date0), (\"JULD\",\"<\",date1),\n        (\"PLATFORM_NUMBER\",\"==\",wmo_id)\n    )\n\n    # this adds OR conditions to the and_filters, i.e. we want to load all observations that statisfy the AND conditions above, AND that have finite salinity OR temperature values\n    db_filters = [\n        list(and_filters) + [(\"PSAL\", \">\", -1e30), (\"PSAL\", \"<\", 1e30)],\n        list(and_filters) + [(\"TEMP\", \">\", -1e30), (\"TEMP\", \"<\", 1e30)],\n    ]\n\n    # generate output filename\n    obs_seq_out = basename + f\".{year0}{month0:02d}{day0:02d}.out\"\n\n    # generate obs_seq.in file\n    obsSeq = ObsSequence(\n        crocolake_path,\n        selected_variables,\n        db_filters,\n        obs_seq_out=obs_seq_out,\n        loose=True\n    )\n    obsSeq.write_obs_seq()\n\n\n\n\nThe previous cell should have generate one obs_seq file, as the float that we selected recorded data only on one day between 2010-05-01 and 2010-05-10. We will use this generate file in the model-obs comparison in the following.","type":"content","url":"/notebooks/projects/tutorial2-mom6-cl-comparison-float-1#generating-your-own-obs-seq-in-files","position":5},{"hierarchy":{"lvl1":"Tutorial 2: Model-Observation comparison with MOM6 and a single Argo float from CrocoLake","lvl2":"The configuration file"},"type":"lvl2","url":"/notebooks/projects/tutorial2-mom6-cl-comparison-float-1#the-configuration-file","position":6},{"hierarchy":{"lvl1":"Tutorial 2: Model-Observation comparison with MOM6 and a single Argo float from CrocoLake","lvl2":"The configuration file"},"content":"The configuration file config_tutorial_2.yaml has already all the paths set. Make sure that the output is being saved where you wish.","type":"content","url":"/notebooks/projects/tutorial2-mom6-cl-comparison-float-1#the-configuration-file","position":7},{"hierarchy":{"lvl1":"Tutorial 2: Model-Observation comparison with MOM6 and a single Argo float from CrocoLake","lvl2":"Running the workflow"},"type":"lvl2","url":"/notebooks/projects/tutorial2-mom6-cl-comparison-float-1#running-the-workflow","position":8},{"hierarchy":{"lvl1":"Tutorial 2: Model-Observation comparison with MOM6 and a single Argo float from CrocoLake","lvl2":"Running the workflow"},"content":"Running the workflow to interpolate the model is identical as in Tutorial 1 (remember to adapt the config file name in from_config_file()!): the obs_seq files that we generated only contain the float‚Äôs measurements, so only those observtions will be used by DART‚Äôs perfect_model_obs to perform the interpolation.\n\nfrom crococamp.workflows import WorkflowModelObs\n\n# interpolate model onto obs space for the single float\nworkflow_float = WorkflowModelObs.from_config_file('config_tutorial_2.yaml')\nworkflow_float.run() #use flag clear_output=True if you want to re-run it and automatically clean all previous output\n\n\n\n","type":"content","url":"/notebooks/projects/tutorial2-mom6-cl-comparison-float-1#running-the-workflow","position":9},{"hierarchy":{"lvl1":"Tutorial 2: Model-Observation comparison with MOM6 and a single Argo float from CrocoLake","lvl2":"Displaying the interactive map"},"type":"lvl2","url":"/notebooks/projects/tutorial2-mom6-cl-comparison-float-1#displaying-the-interactive-map","position":10},{"hierarchy":{"lvl1":"Tutorial 2: Model-Observation comparison with MOM6 and a single Argo float from CrocoLake","lvl2":"Displaying the interactive map"},"content":"We can load the data with the same tools as in Tutorial 1. Note that in this case all interpolations were succesfull, so get_good_model_obs_df() and get_all_model_obs() will return the same dataframe (get_failed_model_obs() will return an empty dataframe).\n\ngood_model_obs_df = workflow_float.get_good_model_obs_df(compute=True) # compute=True triggers the compute of the dask dataframe, returning a pandas dataframe with data loaded in memory\ngood_model_obs_df.head()                                                   # displays first 5 rows in the dataframe\n\n\n\nNow load the interactive map as in Tutorial 1. What do you see?\n\nfrom crococamp.viz import InteractiveWidgetMap\n\n# Create an interactive map widget to visualize model-observation comparisons\n# The widget provides controls for selecting variables, observation types, and time ranges\nwidget = InteractiveWidgetMap(good_model_obs_df)\nwidget.setup()\n\n\n\nThe previous cell should show a dot in the middle of the ocean, which is not very informative. We can then use MapConfig to pass extra arguments to InteractiveWidgetMap, among which the extent of the map area to plot. Play with the map_extent values in the cell below and re-execute the cell until you‚Äôre happy with your plotted region.\n\nfrom crococamp.viz import MapConfig\n\nmap_config = MapConfig(\n    map_extent=(-90,0,0,90) #(lon_min, lon_max, lat_min, lat_max)\n)\n\n# Create an interactive map widget to visualize model-observation comparisons\n# The widget provides controls for selecting variables, observation types, and time ranges\nwidget = InteractiveWidgetMap(good_model_obs_df, config=map_config)\nwidget.setup()\n\n\n\n","type":"content","url":"/notebooks/projects/tutorial2-mom6-cl-comparison-float-1#displaying-the-interactive-map","position":11},{"hierarchy":{"lvl1":"Tutorial 2: Model-Observation comparison with MOM6 and a single Argo float from CrocoLake","lvl3":"Interactive profile","lvl2":"Displaying the interactive map"},"type":"lvl3","url":"/notebooks/projects/tutorial2-mom6-cl-comparison-float-1#interactive-profile","position":12},{"hierarchy":{"lvl1":"Tutorial 2: Model-Observation comparison with MOM6 and a single Argo float from CrocoLake","lvl3":"Interactive profile","lvl2":"Displaying the interactive map"},"content":"Finally, we can load the interactive profile widget to explore how our model performed compared to the specific float measurements we selected:\n\nfrom crococamp.viz import InteractiveWidgetProfile\n# Create an interactive profile widget to analyze vertical profiles\n# This is ideal for analyzing single float/CTD profiles vs model data\nwidget = InteractiveWidgetProfile(good_model_obs_df)\nwidget.setup()\n\n\n\nYou can also pass some settings through the config argument of the InteractiveWidgetProfile class:\n\nfrom crococamp.viz import ProfileConfig\n\n# Customize the profile widget appearance and behavior\nprofile_config = ProfileConfig(\n    figure_size=(7, 7),\n    marker_size=5,\n    marker_alpha=0.6,\n    invert_yaxis=False,  # Don't invert for this example\n    grid=True\n)\n\n# Create widget with custom configuration\nwidget = InteractiveWidgetProfile(good_model_obs_df, x='obs', y='interpolated_model', config=profile_config)\nwidget.setup()\n\n","type":"content","url":"/notebooks/projects/tutorial2-mom6-cl-comparison-float-1#interactive-profile","position":13},{"hierarchy":{"lvl1":"Tutorial 2: Model-Observation comparison with MOM6 and a single Argo float from CrocoLake"},"type":"lvl1","url":"/notebooks/projects/tutorial2-mom6-cl-comparison-float-1","position":0},{"hierarchy":{"lvl1":"Tutorial 2: Model-Observation comparison with MOM6 and a single Argo float from CrocoLake"},"content":"The goal of this tutorial is to learn how to generate your own \n\nobs_seq files from CrocoLake and learn about another interactive widget: the interactive profile. We will select a specific Argo float, interpolate our model output to the temperature and salinity values of that float, and compare the interpolated model and the float‚Äôs profiles.","type":"content","url":"/notebooks/projects/tutorial2-mom6-cl-comparison-float-1","position":1},{"hierarchy":{"lvl1":"Tutorial 2: Model-Observation comparison with MOM6 and a single Argo float from CrocoLake","lvl4":"Installing CrocoCamp"},"type":"lvl4","url":"/notebooks/projects/tutorial2-mom6-cl-comparison-float-1#installing-crococamp","position":2},{"hierarchy":{"lvl1":"Tutorial 2: Model-Observation comparison with MOM6 and a single Argo float from CrocoLake","lvl4":"Installing CrocoCamp"},"content":"If you don‚Äôt have CrocoCamp set up yet, \n\nhere are the instructions to install it on NCAR‚Äôs HPC. If you have any issues or need to install it on a different machine, please contact \n\nenrico‚Äã.milanese@whoi‚Äã.edu.","type":"content","url":"/notebooks/projects/tutorial2-mom6-cl-comparison-float-1#installing-crococamp","position":3},{"hierarchy":{"lvl1":"Tutorial 2: Model-Observation comparison with MOM6 and a single Argo float from CrocoLake","lvl2":"Generating your own obs_seq.in files"},"type":"lvl2","url":"/notebooks/projects/tutorial2-mom6-cl-comparison-float-1#generating-your-own-obs-seq-in-files","position":4},{"hierarchy":{"lvl1":"Tutorial 2: Model-Observation comparison with MOM6 and a single Argo float from CrocoLake","lvl2":"Generating your own obs_seq.in files"},"content":"CrocoLake is a flexible parquet dataset that is fast and easy to access and filter. For this workshop, a copy of the CrocoLake version with physical variables is stored at /glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/CrocoCamp/CrocoLakePHY:\n\ncrocolake_path = '/glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/CrocoCamp/CrocoLakePHY'\n\nimport os\nbasename = \"myCL_obs_seq_\"\noutdir = \"$WORK/crocodile_2025/CrocoCamp/tutorial_2/in_CL/\"\nbasename = os.path.expandvars(outdir+basename)\noutdir = os.path.expandvars(outdir)\n\n\n\nIn the following cell you have the code that I used to generate the observation files that we used in Tutorial 1. Have a look at the code, in particular at how you to select variables and define filters. Once you‚Äôre ready, create a new cell below, copy-paste the code, and add a filter so that you only store the data recorded by the Argo float with identifier 1900256. Also change the file name to add the float number to it. Note: in CrocoLake, the platform identifier is stored as a string in the variable 'PLATFORM_NUMBER'.\n\nimport datetime\nfrom convert_crocolake_obs import ObsSequence\n\n# define horizontal region\nLAT0 = 5\nLAT1 = 60\nLON0 = -100\nLON1 = -30\n\n# define variables to import from CrocoLake\nselected_variables = [\n    \"DB_NAME\",  # ARGO, GLODAP, SprayGliders, OleanderXBT, Saildrones\n    \"JULD\", # this contains timestamp\n    \"LATITUDE\",\n    \"LONGITUDE\",\n    \"PRES\", # This will be automatically converted to depths in meters\n    \"TEMP\",\n    \"PRES_QC\",\n    \"TEMP_QC\",\n    \"PRES_ERROR\",\n    \"TEMP_ERROR\",\n    \"PSAL\",\n    \"PSAL_QC\",\n    \"PSAL_ERROR\"\n]\n\n# month and year are constant in out case\nyear0 = 2010\nmonth0 = 5\n\n# we loop to generate one file per day\nfor j in range(10):\n\n    # set date range\n    day0 = 1+j\n    day1 = day0+1\n    date0 = datetime.datetime(year0, month0, day0, 0, 0, 0)\n    date1 = datetime.datetime(year0, month0, day1, 0, 0, 0)\n    print(f\"Converting obs between {date0} and {date1}\")\n\n    # this defines AND filters, i.e. we want to load each observation that has latitude within the given range AND longitude within the given range, etc.\n    # to exclude NaNs, impose a range to a variable\n    and_filters = (\n        (\"LATITUDE\",'>',LAT0),  (\"LATITUDE\",'<',LAT1),\n        (\"LONGITUDE\",'>',LON0), (\"LONGITUDE\",'<',LON1),\n        (\"PRES\",'>',-1e30), (\"PRES\",'<',1e30),\n        (\"JULD\",\">\",date0), (\"JULD\",\"<\",date1),\n    )\n\n    # this adds OR conditions to the and_filters, i.e. we want to load all observations that statisfy the AND conditions above, AND that have finite salinity OR temperature values\n    db_filters = [\n        list(and_filters) + [(\"PSAL\", \">\", -1e30), (\"PSAL\", \"<\", 1e30)],\n        list(and_filters) + [(\"TEMP\", \">\", -1e30), (\"TEMP\", \"<\", 1e30)],\n    ]\n\n    # generate output filename\n    obs_seq_out = basename + f\".{year0}{month0:02d}{day0:02d}.out\"\n\n    # generate obs_seq.in file\n    obsSeq = ObsSequence(\n        crocolake_path,\n        selected_variables,\n        db_filters,\n        obs_seq_out=obs_seq_out,\n        loose=True\n    )\n    obsSeq.write_obs_seq()\n\nThe following cell contains the solution:\n\nimport datetime\nfrom convert_crocolake_obs import ObsSequence\n\n# define horizontal region\nLAT0 = 5\nLAT1 = 60\nLON0 = -100\nLON1 = -30\n\n# define variables to import from CrocoLake\nselected_variables = [\n    \"DB_NAME\",  # ARGO, GLODAP, SprayGliders, OleanderXBT, Saildrones\n    \"JULD\", # this contains timestamp\n    \"LATITUDE\",\n    \"LONGITUDE\",\n    \"PRES\", # This will be automatically converted to depths in meters\n    \"TEMP\",\n    \"PRES_QC\",\n    \"TEMP_QC\",\n    \"PRES_ERROR\",\n    \"TEMP_ERROR\",\n    \"PSAL\",\n    \"PSAL_QC\",\n    \"PSAL_ERROR\"\n]\n\n# month and year are constant in out case\nyear0 = 2010\nmonth0 = 5\nwmo_id = str(1900256)\nbasename = basename + f\".ARGO_{wmo_id}\"\nif not os.path.exists(outdir):\n    os.makedirs(outdir, exist_ok=True)\n\n# we loop to generate one file per day\nfor j in range(10):\n\n    # set date range\n    day0 = 1+j\n    day1 = day0+1\n    date0 = datetime.datetime(year0, month0, day0, 0, 0, 0)\n    date1 = datetime.datetime(year0, month0, day1, 0, 0, 0)\n    print(f\"Converting obs between {date0} and {date1}\")\n\n    # this defines AND filters, i.e. we want to load each observation that has latitude within the given range AND longitude within the given range, etc.\n    # to exclude NaNs, impose a range to a variable\n    and_filters = (\n        (\"LATITUDE\",'>',LAT0),  (\"LATITUDE\",'<',LAT1),\n        (\"LONGITUDE\",'>',LON0), (\"LONGITUDE\",'<',LON1),\n        (\"PRES\",'>',-1e30), (\"PRES\",'<',1e30),\n        (\"JULD\",\">\",date0), (\"JULD\",\"<\",date1),\n        (\"PLATFORM_NUMBER\",\"==\",wmo_id)\n    )\n\n    # this adds OR conditions to the and_filters, i.e. we want to load all observations that statisfy the AND conditions above, AND that have finite salinity OR temperature values\n    db_filters = [\n        list(and_filters) + [(\"PSAL\", \">\", -1e30), (\"PSAL\", \"<\", 1e30)],\n        list(and_filters) + [(\"TEMP\", \">\", -1e30), (\"TEMP\", \"<\", 1e30)],\n    ]\n\n    # generate output filename\n    obs_seq_out = basename + f\".{year0}{month0:02d}{day0:02d}.out\"\n\n    # generate obs_seq.in file\n    obsSeq = ObsSequence(\n        crocolake_path,\n        selected_variables,\n        db_filters,\n        obs_seq_out=obs_seq_out,\n        loose=True\n    )\n    obsSeq.write_obs_seq()\n\n\n\n\nThe previous cell should have generate one obs_seq file, as the float that we selected recorded data only on one day between 2010-05-01 and 2010-05-10. We will use this generate file in the model-obs comparison in the following.","type":"content","url":"/notebooks/projects/tutorial2-mom6-cl-comparison-float-1#generating-your-own-obs-seq-in-files","position":5},{"hierarchy":{"lvl1":"Tutorial 2: Model-Observation comparison with MOM6 and a single Argo float from CrocoLake","lvl2":"The configuration file"},"type":"lvl2","url":"/notebooks/projects/tutorial2-mom6-cl-comparison-float-1#the-configuration-file","position":6},{"hierarchy":{"lvl1":"Tutorial 2: Model-Observation comparison with MOM6 and a single Argo float from CrocoLake","lvl2":"The configuration file"},"content":"The configuration file config_tutorial_2.yaml has already all the paths set. Make sure that the output is being saved where you wish.","type":"content","url":"/notebooks/projects/tutorial2-mom6-cl-comparison-float-1#the-configuration-file","position":7},{"hierarchy":{"lvl1":"Tutorial 2: Model-Observation comparison with MOM6 and a single Argo float from CrocoLake","lvl2":"Running the workflow"},"type":"lvl2","url":"/notebooks/projects/tutorial2-mom6-cl-comparison-float-1#running-the-workflow","position":8},{"hierarchy":{"lvl1":"Tutorial 2: Model-Observation comparison with MOM6 and a single Argo float from CrocoLake","lvl2":"Running the workflow"},"content":"Running the workflow to interpolate the model is identical as in Tutorial 1 (remember to adapt the config file name in from_config_file()!): the obs_seq files that we generated only contain the float‚Äôs measurements, so only those observtions will be used by DART‚Äôs perfect_model_obs to perform the interpolation.\n\nfrom crococamp.workflows import WorkflowModelObs\n\n# interpolate model onto obs space for the single float\nworkflow_float = WorkflowModelObs.from_config_file('config_tutorial_2.yaml')\nworkflow_float.run() #use flag clear_output=True if you want to re-run it and automatically clean all previous output\n\n\n\n","type":"content","url":"/notebooks/projects/tutorial2-mom6-cl-comparison-float-1#running-the-workflow","position":9},{"hierarchy":{"lvl1":"Tutorial 2: Model-Observation comparison with MOM6 and a single Argo float from CrocoLake","lvl2":"Displaying the interactive map"},"type":"lvl2","url":"/notebooks/projects/tutorial2-mom6-cl-comparison-float-1#displaying-the-interactive-map","position":10},{"hierarchy":{"lvl1":"Tutorial 2: Model-Observation comparison with MOM6 and a single Argo float from CrocoLake","lvl2":"Displaying the interactive map"},"content":"We can load the data with the same tools as in Tutorial 1. Note that in this case all interpolations were succesfull, so get_good_model_obs_df() and get_all_model_obs() will return the same dataframe (get_failed_model_obs() will return an empty dataframe).\n\ngood_model_obs_df = workflow_float.get_good_model_obs_df(compute=True) # compute=True triggers the compute of the dask dataframe, returning a pandas dataframe with data loaded in memory\ngood_model_obs_df.head()                                                   # displays first 5 rows in the dataframe\n\n\n\nNow load the interactive map as in Tutorial 1. What do you see?\n\nfrom crococamp.viz import InteractiveWidgetMap\n\n# Create an interactive map widget to visualize model-observation comparisons\n# The widget provides controls for selecting variables, observation types, and time ranges\nwidget = InteractiveWidgetMap(good_model_obs_df)\nwidget.setup()\n\n\n\nThe previous cell should show a dot in the middle of the ocean, which is not very informative. We can then use MapConfig to pass extra arguments to InteractiveWidgetMap, among which the extent of the map area to plot. Play with the map_extent values in the cell below and re-execute the cell until you‚Äôre happy with your plotted region.\n\nfrom crococamp.viz import MapConfig\n\nmap_config = MapConfig(\n    map_extent=(-90,0,0,90) #(lon_min, lon_max, lat_min, lat_max)\n)\n\n# Create an interactive map widget to visualize model-observation comparisons\n# The widget provides controls for selecting variables, observation types, and time ranges\nwidget = InteractiveWidgetMap(good_model_obs_df, config=map_config)\nwidget.setup()\n\n\n\n","type":"content","url":"/notebooks/projects/tutorial2-mom6-cl-comparison-float-1#displaying-the-interactive-map","position":11},{"hierarchy":{"lvl1":"Tutorial 2: Model-Observation comparison with MOM6 and a single Argo float from CrocoLake","lvl3":"Interactive profile","lvl2":"Displaying the interactive map"},"type":"lvl3","url":"/notebooks/projects/tutorial2-mom6-cl-comparison-float-1#interactive-profile","position":12},{"hierarchy":{"lvl1":"Tutorial 2: Model-Observation comparison with MOM6 and a single Argo float from CrocoLake","lvl3":"Interactive profile","lvl2":"Displaying the interactive map"},"content":"Finally, we can load the interactive profile widget to explore how our model performed compared to the specific float measurements we selected:\n\nfrom crococamp.viz import InteractiveWidgetProfile\n# Create an interactive profile widget to analyze vertical profiles\n# This is ideal for analyzing single float/CTD profiles vs model data\nwidget = InteractiveWidgetProfile(good_model_obs_df)\nwidget.setup()\n\n\n\nYou can also pass some settings through the config argument of the InteractiveWidgetProfile class:\n\nfrom crococamp.viz import ProfileConfig\n\n# Customize the profile widget appearance and behavior\nprofile_config = ProfileConfig(\n    figure_size=(7, 7),\n    marker_size=5,\n    marker_alpha=0.6,\n    invert_yaxis=False,  # Don't invert for this example\n    grid=True\n)\n\n# Create widget with custom configuration\nwidget = InteractiveWidgetProfile(good_model_obs_df, x='obs', y='interpolated_model', config=profile_config)\nwidget.setup()\n\n","type":"content","url":"/notebooks/projects/tutorial2-mom6-cl-comparison-float-1#interactive-profile","position":13},{"hierarchy":{"lvl1":"CrocoLake - Temperature map in the North West Atlantic"},"type":"lvl1","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1","position":0},{"hierarchy":{"lvl1":"CrocoLake - Temperature map in the North West Atlantic"},"content":"","type":"content","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1","position":1},{"hierarchy":{"lvl1":"CrocoLake - Temperature map in the North West Atlantic"},"type":"lvl1","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#crocolake-temperature-map-in-the-north-west-atlantic","position":2},{"hierarchy":{"lvl1":"CrocoLake - Temperature map in the North West Atlantic"},"content":"This tutorial shows how to read and manipulate CrocoLake data. CrocoLake is stored in parquet format across multiple files for faster reading operations: we will load into memory only what we need by applying some filters, and we will create a map showing the temperature measurements in the North West Atlantic.\n\nCrocoLake contains QCed data from different datasets (Argo, GLODAP, Spray Gliders as of today).\n\nYou can find more examples on the \n\ncrocolake-python repository.","type":"content","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#crocolake-temperature-map-in-the-north-west-atlantic","position":3},{"hierarchy":{"lvl1":"CrocoLake - Temperature map in the North West Atlantic","lvl2":"‚ÄúInstalling‚Äù CrocoLake"},"type":"lvl2","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#installing-crocolake","position":4},{"hierarchy":{"lvl1":"CrocoLake - Temperature map in the North West Atlantic","lvl2":"‚ÄúInstalling‚Äù CrocoLake"},"content":"CrocoLake is not a package but a dataset, thus it does not require any installation per se. To access the data with Python you‚Äôll need the \n\npandas and \n\ndask packages though. If you have installed CrocoCamp in a previous tutorial, you‚Äôre already set and you can use the same conda environment you have used before (instructions were provided also \n\nhere). If you still have issues, look also at the \n\ncrocolake-python repository, or contact \n\nenrico‚Äã.milanese@whoi‚Äã.edu.","type":"content","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#installing-crocolake","position":5},{"hierarchy":{"lvl1":"CrocoLake - Temperature map in the North West Atlantic","lvl3":"Note on parquet files","lvl2":"‚ÄúInstalling‚Äù CrocoLake"},"type":"lvl3","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#note-on-parquet-files","position":6},{"hierarchy":{"lvl1":"CrocoLake - Temperature map in the North West Atlantic","lvl3":"Note on parquet files","lvl2":"‚ÄúInstalling‚Äù CrocoLake"},"content":"There are several ways to load parquet files in a dataframe in Python, and this notebook shows two of them:\n\npandas + pyarrow: this approach uses the pyarrow package to load the data into a pandas dataframe;\n\ndask (+ pandas + pyarrow): this approach uses the dask package to load the data into a dask dataframe; it uses pandas and pyarrow under the hood, and a dask dataframe is (almost) identical to a pandas dataframe.\n\nWe will first use pyarrow to load the dataset, so that we can provide a target schema (containing the Argo variable names) and load the data consistently across floats, independently of what variables each float actually has. We will finally convert the dataset to a pandas dataframe for manipulation and plots.\n\nOther ways to read parquet files are by using pandas (make sure you have pyarrow, fastparquet or some other suitable engine installed), and Dask. Generally speaking, you‚Äôll want to use Dask only if you need a large amount of data at the same time so that you can benefit from its parallelization. You should avoid Dask whenever the data fits in your RAM.\n\nWhen reading parquet files with pyarrow (or pandas), you can either specificy the file name(s), or the directory containing all the parquet files. In latter case if you apply any filter, pandas and pyarrow will sort through all the files in the folder, reading into memory only the subsets that satisfy your filter.\n\n","type":"content","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#note-on-parquet-files","position":7},{"hierarchy":{"lvl1":"CrocoLake - Temperature map in the North West Atlantic","lvl2":"Getting started"},"type":"lvl2","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#getting-started","position":8},{"hierarchy":{"lvl1":"CrocoLake - Temperature map in the North West Atlantic","lvl2":"Getting started"},"content":"We then import the necessary modules and set up the path to the dataset (update the crocolake_path variable below if you stored CrocoLake at a different location).\n\nimport datetime\nimport glob\nfrom pprint import pprint\n\nimport pandas as pd\nimport pyarrow.parquet as pq\n\n# Path to CrocoLake PHY\ncrocolake_path = '/glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/CrocoCamp/CrocoLakePHY/'\n\n# Setting up parquet schema\ncrocolake_schema = pq.read_schema(crocolake_path+\"_common_metadata\")\n\n# Geographical boundaries\nlat0 = 30\nlat1 = 45\nlon0 = -85\nlon1 = -60\n\n\n\n","type":"content","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#getting-started","position":9},{"hierarchy":{"lvl1":"pyarrow + pandas approach"},"type":"lvl1","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#pyarrow-pandas-approach","position":10},{"hierarchy":{"lvl1":"pyarrow + pandas approach"},"content":"We now create a ParquetDataset object that will allow us to read the data. Specifying a schema (that we read into crocolake_schema in the previous cell) will make later operations faster.\n\nWe can get a peak at what variables are available in the dataset looking at its schema attribute. Note that we have not load any data into memory yet (except for the schema).\n\ndataset = pq.ParquetDataset(\n    crocolake_path, \n    schema=crocolake_schema\n)\nschema = dataset.schema\npprint(sorted(schema.names))\n\n\n\nWe now want to filter the dataset to load in memory only the data from the NWA (we set the values for lat0,lat1,lon0,lon1 in the first cell) and recorded in the last 6 months.\n\nThe geographical coordinates are stored in the variables 'LATITUDE‚Äôand ‚ÄòLONGITUDE‚Äô. We then generate the filter, with its syntax being: [[(column, op, val), ‚Ä¶],‚Ä¶] where column is the variable name, and val is the value to for the operator op, which accepts [==, =, >, >=, <, <=, !=, in, not in],\n\ndate0 = datetime.datetime(2010, 1, 1, 0, 0, 0)\ndate1 = datetime.datetime(2020, 1, 1, 0, 0, 0)\n\nfilter_coords_time = [\n    (\"LATITUDE\",\">\",lat0), (\"LATITUDE\",\"<\",lat1),\n    (\"LONGITUDE\",\">\",lon0), (\"LONGITUDE\",\"<\",lon1),\n    (\"JULD\",\">\",date0), (\"JULD\",\"<\",date1)\n]\n\n\n\nTo get a pandas dataframe, we re-generate a ParquetDataset object adding the filters to it, and then we read it into a pandas dataframe with the read().to_pandas() methods of the dataset.\n\nNB: The following operation fetches a large amount of data (~1.6 GB), so you can reduce the number of days in the filter above if you cannot use this much memory.\n\n%%time\nds = pq.ParquetDataset(crocolake_path, schema=crocolake_schema, filters=filter_coords_time)\ndf = ds.read().to_pandas()\ndf\n\n\n\nYou can explore the dataframe just by calling it (df) as we did above. If you want a list of the variables that are stored, you can use sorted(df.columns.to_list()). The following cell shows that we loaded roughly 2 GB of data.\n\nmemory_usage = df.memory_usage(deep=True).sum()/(1024**2)\nprint(f\"DataFrame size: {memory_usage:.2f} MB\")\n\n\n\nIf we now that we only need a subset of variables, we can specify them in the dataset‚Äôs read() method.\n\nFor example, let‚Äôs say that we are interested in the adjusted measurements of the dissolved oxygen recorded in the past 6 months in the NWA. To exclude non valid and missing observations, we filter the temperature (TEMP) to be within a (large) range of valid values. Besides the oxygen, we also want to keep the spatial and time coordinates.\n\n%%time\ncols = [\"DB_NAME\",\"LATITUDE\",\"LONGITUDE\",\"PRES\",\"JULD\",\"TEMP\"]\nfilter_coords_time_temp = [\n    (\"LATITUDE\",\">\",lat0), (\"LATITUDE\",\"<\",lat1),\n    (\"LONGITUDE\",\">\",lon0), (\"LONGITUDE\",\"<\",lon1),\n    (\"JULD\",\">\",date0), (\"JULD\",\"<\",date1),\n    (\"TEMP\",\">=\",-1e30),(\"TEMP\",\"<=\",+1e30)\n]\nds = pq.ParquetDataset(crocolake_path, schema=crocolake_schema, filters=filter_coords_time_temp)\ndf = ds.read(columns=cols).to_pandas()\ndf\n\n\n\nThis operation was much faster (3-4x faster on my machine) and loaded a smaller dataframe (~800 MB):\n\nmemory_usage = df.memory_usage(deep=True).sum()/(1024**2)\nprint(f\"DataFrame size: {memory_usage:.2f} MB\")\n\n\n\n","type":"content","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#pyarrow-pandas-approach","position":11},{"hierarchy":{"lvl1":"pyarrow + pandas approach","lvl2":"Map"},"type":"lvl2","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#map","position":12},{"hierarchy":{"lvl1":"pyarrow + pandas approach","lvl2":"Map"},"content":"We can now produce a scatter plot as we‚Äôd normally do with pandas. For example, here is the average dissolved oxygen at each location in the dataframe that we loaded:\n\nimport cartopy.crs as ccrs\nimport matplotlib.pyplot as plt\nfrom matplotlib import colormaps\n\nref_var = \"TEMP\"\n# Group by 'LATITUDE' and 'LONGITUDE' and database\ndf_for_map = df.groupby([\"LATITUDE\", \"LONGITUDE\", \"DB_NAME\"]).agg({\n    ref_var: 'mean'  # Take the mean intensity at that coordinate\n}).reset_index()\n\n# Plotting using Cartopy\nplt.figure(figsize=(16, 10))\nax = plt.axes(projection=ccrs.PlateCarree())\nax.coastlines()\ncbar_min = df_for_map[ref_var].quantile(q=0.05)\ncbar_max = df_for_map[ref_var].quantile(q=0.95)\n\n# Group by DB_NAME and plot each group\ndf_for_map[\"DB_NAME\"] = pd.Categorical(df_for_map[\"DB_NAME\"], categories=[\"ARGO\",\"GLODAP\",\"SprayGliders\"], ordered=True)\n\ncmaps = iter( [\"Greens\",\"Oranges\",\"Blues\"] )\nfor db_name, group in df_for_map.groupby(\"DB_NAME\",observed=False):\n    plt.scatter(\n        group['LONGITUDE'],\n        group['LATITUDE'],\n        s=10,\n        c=group[ref_var],\n        vmin=cbar_min,\n        vmax=cbar_max,\n        cmap=next(cmaps),\n        transform=ccrs.PlateCarree()\n    )\n    plt.colorbar(shrink=0.5, label='Average ' + ref_var + \" (\" + db_name + \")\", pad=0.02)\n\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.title('North-West Atlantic average temperature 2010-2019 (Celsius)')\nplt.grid(True)\nplt.xlim([lon0, lon1])\nplt.ylim([lat0, lat1])\nplt.show()\n\n\n\n","type":"content","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#map","position":13},{"hierarchy":{"lvl1":"dask approach"},"type":"lvl1","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#dask-approach","position":14},{"hierarchy":{"lvl1":"dask approach"},"content":"dask can be faster at reading parquet databases thanks to its parallel and lazy evaluation of operations.\n\nWhen using dask, very few things change compared to using pandas and pyarrow: for example, dask dataframes are almost identical to pandas dataframes and indeed for our needs we will use the same syntax! The only difference is that dask does not evaluate the instructions immediately: it creates so-called delayed objects, through which it builds internally a graph of instructions optimized for the sequence of operations called. To trigger the actual computation we then need to call the compute() method of the dask dataframe. This allows dask to handle larger-than-memory datasets, by reading into memory only the portions needed to perform the optimized set of instructions.\n\nWe start by importing a few extra modules.\n\nimport dask\nimport dask.dataframe as dd\n\n\n\nWe can use the same filters defined earlier, and we use read_parquet() to filter the database and prescribe what we will need.\nNote that:\n\nunlike the pyarrow-pandas approach, here we provide both filters and columns in the same method;\n\nwe provide the schema crocolake_schema that we read during the set up of the excercise\n\ndask uses pyarrow under the hood (engine variable, other options are available)\n\n%%time\ndate0 = datetime.datetime(2010, 1, 1, 0, 0, 0)\ndate1 = datetime.datetime(2020, 1, 1, 0, 0, 0)\n\ncols = [\"DB_NAME\",\"LATITUDE\",\"LONGITUDE\",\"PRES\",\"JULD\",\"TEMP\"]\nfilter_coords_time_temp = [\n    (\"LATITUDE\",\">\",lat0), (\"LATITUDE\",\"<\",lat1),\n    (\"LONGITUDE\",\">\",lon0), (\"LONGITUDE\",\"<\",lon1),\n    (\"JULD\",\">\",date0), (\"JULD\",\"<\",date1),\n    (\"TEMP\",\">=\",-1e30),(\"TEMP\",\"<=\",+1e30)\n]\n\nddf = dd.read_parquet(\n    crocolake_path,\n    engine=\"pyarrow\",\n    schema=crocolake_schema,\n    columns= cols,\n    filters=filter_coords_time_temp\n)\n\n\n\nIt appears dask is much faster! Yet, it actually creates a delayed object, i.e. ddf contains the instructions that dask will later use to load the dataframe into memory. This allows us to use ddf to schedule all the operations that we‚Äôd normally perform on a dataframe. Eventually we will call ddf.compute() to actually evaluate the instructions.\n\nIf we look at ddf, the dataframe will in fact appear empty:\n\nddf\n\n\n\nWe can explore its first entries with head(), which loads into memory only the first 5 entries:\n\n(Note: head() can return an empty dataframe even when ddf is not empty. This happens because head() looks for the first 5 rows of the first partition, which sometimes happens to be empty. Repartitioning the dask dataframe will get rid of empty partitions and solve this.)\n\n#ddf=ddf.repartition(partition_size=\"300MB\")\nddf.head()\n\n\n\nTo load the whole dataframe, we call compute() (this returns a pandas dataframe).\n\n%%time\nddf_loaded = ddf.compute()\nddf_loaded\n\n\n\nIt took just a few hundreds milliseconds! So yes, dask can be much faster than just using pyarrow and pandas.\n\nIf we look into ddf_loaded now, it will show the populated pandas dataframe.\n\n(Note: if you compare ddf_loaded with df loaded with pyarrow, you‚Äôll see that the rows are not in the same order, yet under the dataframe you can see that the total number of rows is the same. Also the index seems smaller, but dask holds the dataframe in multiple partitions, and the index is reset at every partition.)\n\ndask‚Äôs main advantage is not just in loading the data faster, but in performing operations on larger-than-memory data. For example, it can compute the mean value of TEMPERATURE in the whole Argo PHY dataset without loading the whole data in memory (and pretty quickly, too):\n\n# note that we are not passing any filter conditions or selection of columns to load\nddf_all = dd.read_parquet(\n    crocolake_path,\n    engine=\"pyarrow\",\n    schema=crocolake_schema,\n)\ntemp_mean = ddf_all['TEMP'].mean() # temp_mean is a delayed object: it knows what operations to perform to compute the mean, \n                                   # but doesn't perform them until we call compute() in the next cell\n\n\n\n%%time\ntemp_mean.compute()\n\n\n\n","type":"content","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#dask-approach","position":15},{"hierarchy":{"lvl1":"dask approach","lvl2":"Map"},"type":"lvl2","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#map-1","position":16},{"hierarchy":{"lvl1":"dask approach","lvl2":"Map"},"content":"When producing the map with our dask approach, we would create the grouped dataframe from the delayed dataframe ddf, and compute() it as late as possible.\n\nIn this example, the compute that we are triggering includes four operations:\n\nreading the filtered dataset;\n\ngrouping the dataframe by geographical coordinates;\n\naveraging by pressure and time;\n\nresetting the index;\n\nDask internally builds a graph of all of these operations, so that it knows what subset of data is needed and it optimizes the number and sequence of instructions before executing them.\n\nimport cartopy.crs as ccrs\nimport matplotlib.pyplot as plt\nfrom matplotlib import colormaps\n\nref_var = \"TEMP\"\n# Group by 'LATITUDE' and 'LONGITUDE' and database\nddf_for_map = ddf.groupby([\"LATITUDE\", \"LONGITUDE\", \"DB_NAME\"]).agg({\n    ref_var: 'mean'  # Take the mean intensity at that coordinate\n}).reset_index()\n\n# Plotting using Cartopy\nplt.figure(figsize=(16, 10))\nax = plt.axes(projection=ccrs.PlateCarree())\nax.coastlines()\ncbar_min = ddf_for_map[ref_var].quantile(q=0.05).compute()\ncbar_max = ddf_for_map[ref_var].quantile(q=0.95).compute()\n\n# Group by DB_NAME and plot each group\nddf['DB_NAME'] = ddf.DB_NAME.astype('category')\n\ncmaps = iter( [\"Greens\",\"Oranges\",\"Blues\"] )\nfor db_name, group in ddf_for_map.compute().groupby(\"DB_NAME\"):\n    plt.scatter(\n        group['LONGITUDE'],\n        group['LATITUDE'],\n        s=10,\n        c=group[ref_var],\n        vmin=cbar_min,\n        vmax=cbar_max,\n        cmap=next(cmaps),\n        transform=ccrs.PlateCarree()\n    )\n    plt.colorbar(shrink=0.5, label='Average ' + ref_var + \" (\" + db_name + \")\", pad=0.02)\n\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.title('North-West Atlantic average temperature 2010-2019 (Celsius)')\nplt.grid(True)\nplt.xlim([lon0, lon1])\nplt.ylim([lat0, lat1])\nplt.show()\n\n\n\n","type":"content","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#map-1","position":17},{"hierarchy":{"lvl1":"Suggested exercises"},"type":"lvl1","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#suggested-exercises","position":18},{"hierarchy":{"lvl1":"Suggested exercises"},"content":"Try and access some other data, for example:\n\nfiltering by different time periods;\n\nmapping a different parameter;\n\nrestraining the data of interest further by imposing the pressure PRES to be below 100db;\n\nperforming reads/manipulations that you would need to perform your tasks.\n\nIf you encounter any issues, please \n\nreach out!","type":"content","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#suggested-exercises","position":19},{"hierarchy":{"lvl1":"CrocoLake - Temperature map in the North West Atlantic"},"type":"lvl1","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1","position":0},{"hierarchy":{"lvl1":"CrocoLake - Temperature map in the North West Atlantic"},"content":"","type":"content","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1","position":1},{"hierarchy":{"lvl1":"CrocoLake - Temperature map in the North West Atlantic"},"type":"lvl1","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#crocolake-temperature-map-in-the-north-west-atlantic","position":2},{"hierarchy":{"lvl1":"CrocoLake - Temperature map in the North West Atlantic"},"content":"This tutorial shows how to read and manipulate CrocoLake data. CrocoLake is stored in parquet format across multiple files for faster reading operations: we will load into memory only what we need by applying some filters, and we will create a map showing the temperature measurements in the North West Atlantic.\n\nCrocoLake contains QCed data from different datasets (Argo, GLODAP, Spray Gliders as of today).\n\nYou can find more examples on the \n\ncrocolake-python repository.","type":"content","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#crocolake-temperature-map-in-the-north-west-atlantic","position":3},{"hierarchy":{"lvl1":"CrocoLake - Temperature map in the North West Atlantic","lvl2":"‚ÄúInstalling‚Äù CrocoLake"},"type":"lvl2","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#installing-crocolake","position":4},{"hierarchy":{"lvl1":"CrocoLake - Temperature map in the North West Atlantic","lvl2":"‚ÄúInstalling‚Äù CrocoLake"},"content":"CrocoLake is not a package but a dataset, thus it does not require any installation per se. To access the data with Python you‚Äôll need the \n\npandas and \n\ndask packages though. If you have installed CrocoCamp in a previous tutorial, you‚Äôre already set and you can use the same conda environment you have used before (instructions were provided also \n\nhere). If you still have issues, look also at the \n\ncrocolake-python repository, or contact \n\nenrico‚Äã.milanese@whoi‚Äã.edu.","type":"content","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#installing-crocolake","position":5},{"hierarchy":{"lvl1":"CrocoLake - Temperature map in the North West Atlantic","lvl3":"Note on parquet files","lvl2":"‚ÄúInstalling‚Äù CrocoLake"},"type":"lvl3","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#note-on-parquet-files","position":6},{"hierarchy":{"lvl1":"CrocoLake - Temperature map in the North West Atlantic","lvl3":"Note on parquet files","lvl2":"‚ÄúInstalling‚Äù CrocoLake"},"content":"There are several ways to load parquet files in a dataframe in Python, and this notebook shows two of them:\n\npandas + pyarrow: this approach uses the pyarrow package to load the data into a pandas dataframe;\n\ndask (+ pandas + pyarrow): this approach uses the dask package to load the data into a dask dataframe; it uses pandas and pyarrow under the hood, and a dask dataframe is (almost) identical to a pandas dataframe.\n\nWe will first use pyarrow to load the dataset, so that we can provide a target schema (containing the Argo variable names) and load the data consistently across floats, independently of what variables each float actually has. We will finally convert the dataset to a pandas dataframe for manipulation and plots.\n\nOther ways to read parquet files are by using pandas (make sure you have pyarrow, fastparquet or some other suitable engine installed), and Dask. Generally speaking, you‚Äôll want to use Dask only if you need a large amount of data at the same time so that you can benefit from its parallelization. You should avoid Dask whenever the data fits in your RAM.\n\nWhen reading parquet files with pyarrow (or pandas), you can either specificy the file name(s), or the directory containing all the parquet files. In latter case if you apply any filter, pandas and pyarrow will sort through all the files in the folder, reading into memory only the subsets that satisfy your filter.\n\n","type":"content","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#note-on-parquet-files","position":7},{"hierarchy":{"lvl1":"CrocoLake - Temperature map in the North West Atlantic","lvl2":"Getting started"},"type":"lvl2","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#getting-started","position":8},{"hierarchy":{"lvl1":"CrocoLake - Temperature map in the North West Atlantic","lvl2":"Getting started"},"content":"We then import the necessary modules and set up the path to the dataset (update the crocolake_path variable below if you stored CrocoLake at a different location).\n\nimport datetime\nimport glob\nfrom pprint import pprint\n\nimport pandas as pd\nimport pyarrow.parquet as pq\n\n# Path to CrocoLake PHY\ncrocolake_path = '/glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/CrocoCamp/CrocoLakePHY/'\n\n# Setting up parquet schema\ncrocolake_schema = pq.read_schema(crocolake_path+\"_common_metadata\")\n\n# Geographical boundaries\nlat0 = 30\nlat1 = 45\nlon0 = -85\nlon1 = -60\n\n\n\n","type":"content","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#getting-started","position":9},{"hierarchy":{"lvl1":"pyarrow + pandas approach"},"type":"lvl1","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#pyarrow-pandas-approach","position":10},{"hierarchy":{"lvl1":"pyarrow + pandas approach"},"content":"We now create a ParquetDataset object that will allow us to read the data. Specifying a schema (that we read into crocolake_schema in the previous cell) will make later operations faster.\n\nWe can get a peak at what variables are available in the dataset looking at its schema attribute. Note that we have not load any data into memory yet (except for the schema).\n\ndataset = pq.ParquetDataset(\n    crocolake_path, \n    schema=crocolake_schema\n)\nschema = dataset.schema\npprint(sorted(schema.names))\n\n\n\nWe now want to filter the dataset to load in memory only the data from the NWA (we set the values for lat0,lat1,lon0,lon1 in the first cell) and recorded in the last 6 months.\n\nThe geographical coordinates are stored in the variables 'LATITUDE‚Äôand ‚ÄòLONGITUDE‚Äô. We then generate the filter, with its syntax being: [[(column, op, val), ‚Ä¶],‚Ä¶] where column is the variable name, and val is the value to for the operator op, which accepts [==, =, >, >=, <, <=, !=, in, not in],\n\ndate0 = datetime.datetime(2010, 1, 1, 0, 0, 0)\ndate1 = datetime.datetime(2020, 1, 1, 0, 0, 0)\n\nfilter_coords_time = [\n    (\"LATITUDE\",\">\",lat0), (\"LATITUDE\",\"<\",lat1),\n    (\"LONGITUDE\",\">\",lon0), (\"LONGITUDE\",\"<\",lon1),\n    (\"JULD\",\">\",date0), (\"JULD\",\"<\",date1)\n]\n\n\n\nTo get a pandas dataframe, we re-generate a ParquetDataset object adding the filters to it, and then we read it into a pandas dataframe with the read().to_pandas() methods of the dataset.\n\nNB: The following operation fetches a large amount of data (~1.6 GB), so you can reduce the number of days in the filter above if you cannot use this much memory.\n\n%%time\nds = pq.ParquetDataset(crocolake_path, schema=crocolake_schema, filters=filter_coords_time)\ndf = ds.read().to_pandas()\ndf\n\n\n\nYou can explore the dataframe just by calling it (df) as we did above. If you want a list of the variables that are stored, you can use sorted(df.columns.to_list()). The following cell shows that we loaded roughly 2 GB of data.\n\nmemory_usage = df.memory_usage(deep=True).sum()/(1024**2)\nprint(f\"DataFrame size: {memory_usage:.2f} MB\")\n\n\n\nIf we now that we only need a subset of variables, we can specify them in the dataset‚Äôs read() method.\n\nFor example, let‚Äôs say that we are interested in the adjusted measurements of the dissolved oxygen recorded in the past 6 months in the NWA. To exclude non valid and missing observations, we filter the temperature (TEMP) to be within a (large) range of valid values. Besides the oxygen, we also want to keep the spatial and time coordinates.\n\n%%time\ncols = [\"DB_NAME\",\"LATITUDE\",\"LONGITUDE\",\"PRES\",\"JULD\",\"TEMP\"]\nfilter_coords_time_temp = [\n    (\"LATITUDE\",\">\",lat0), (\"LATITUDE\",\"<\",lat1),\n    (\"LONGITUDE\",\">\",lon0), (\"LONGITUDE\",\"<\",lon1),\n    (\"JULD\",\">\",date0), (\"JULD\",\"<\",date1),\n    (\"TEMP\",\">=\",-1e30),(\"TEMP\",\"<=\",+1e30)\n]\nds = pq.ParquetDataset(crocolake_path, schema=crocolake_schema, filters=filter_coords_time_temp)\ndf = ds.read(columns=cols).to_pandas()\ndf\n\n\n\nThis operation was much faster (3-4x faster on my machine) and loaded a smaller dataframe (~800 MB):\n\nmemory_usage = df.memory_usage(deep=True).sum()/(1024**2)\nprint(f\"DataFrame size: {memory_usage:.2f} MB\")\n\n\n\n","type":"content","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#pyarrow-pandas-approach","position":11},{"hierarchy":{"lvl1":"pyarrow + pandas approach","lvl2":"Map"},"type":"lvl2","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#map","position":12},{"hierarchy":{"lvl1":"pyarrow + pandas approach","lvl2":"Map"},"content":"We can now produce a scatter plot as we‚Äôd normally do with pandas. For example, here is the average dissolved oxygen at each location in the dataframe that we loaded:\n\nimport cartopy.crs as ccrs\nimport matplotlib.pyplot as plt\nfrom matplotlib import colormaps\n\nref_var = \"TEMP\"\n# Group by 'LATITUDE' and 'LONGITUDE' and database\ndf_for_map = df.groupby([\"LATITUDE\", \"LONGITUDE\", \"DB_NAME\"]).agg({\n    ref_var: 'mean'  # Take the mean intensity at that coordinate\n}).reset_index()\n\n# Plotting using Cartopy\nplt.figure(figsize=(16, 10))\nax = plt.axes(projection=ccrs.PlateCarree())\nax.coastlines()\ncbar_min = df_for_map[ref_var].quantile(q=0.05)\ncbar_max = df_for_map[ref_var].quantile(q=0.95)\n\n# Group by DB_NAME and plot each group\ndf_for_map[\"DB_NAME\"] = pd.Categorical(df_for_map[\"DB_NAME\"], categories=[\"ARGO\",\"GLODAP\",\"SprayGliders\"], ordered=True)\n\ncmaps = iter( [\"Greens\",\"Oranges\",\"Blues\"] )\nfor db_name, group in df_for_map.groupby(\"DB_NAME\",observed=False):\n    plt.scatter(\n        group['LONGITUDE'],\n        group['LATITUDE'],\n        s=10,\n        c=group[ref_var],\n        vmin=cbar_min,\n        vmax=cbar_max,\n        cmap=next(cmaps),\n        transform=ccrs.PlateCarree()\n    )\n    plt.colorbar(shrink=0.5, label='Average ' + ref_var + \" (\" + db_name + \")\", pad=0.02)\n\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.title('North-West Atlantic average temperature 2010-2019 (Celsius)')\nplt.grid(True)\nplt.xlim([lon0, lon1])\nplt.ylim([lat0, lat1])\nplt.show()\n\n\n\n","type":"content","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#map","position":13},{"hierarchy":{"lvl1":"dask approach"},"type":"lvl1","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#dask-approach","position":14},{"hierarchy":{"lvl1":"dask approach"},"content":"dask can be faster at reading parquet databases thanks to its parallel and lazy evaluation of operations.\n\nWhen using dask, very few things change compared to using pandas and pyarrow: for example, dask dataframes are almost identical to pandas dataframes and indeed for our needs we will use the same syntax! The only difference is that dask does not evaluate the instructions immediately: it creates so-called delayed objects, through which it builds internally a graph of instructions optimized for the sequence of operations called. To trigger the actual computation we then need to call the compute() method of the dask dataframe. This allows dask to handle larger-than-memory datasets, by reading into memory only the portions needed to perform the optimized set of instructions.\n\nWe start by importing a few extra modules.\n\nimport dask\nimport dask.dataframe as dd\n\n\n\nWe can use the same filters defined earlier, and we use read_parquet() to filter the database and prescribe what we will need.\nNote that:\n\nunlike the pyarrow-pandas approach, here we provide both filters and columns in the same method;\n\nwe provide the schema crocolake_schema that we read during the set up of the excercise\n\ndask uses pyarrow under the hood (engine variable, other options are available)\n\n%%time\ndate0 = datetime.datetime(2010, 1, 1, 0, 0, 0)\ndate1 = datetime.datetime(2020, 1, 1, 0, 0, 0)\n\ncols = [\"DB_NAME\",\"LATITUDE\",\"LONGITUDE\",\"PRES\",\"JULD\",\"TEMP\"]\nfilter_coords_time_temp = [\n    (\"LATITUDE\",\">\",lat0), (\"LATITUDE\",\"<\",lat1),\n    (\"LONGITUDE\",\">\",lon0), (\"LONGITUDE\",\"<\",lon1),\n    (\"JULD\",\">\",date0), (\"JULD\",\"<\",date1),\n    (\"TEMP\",\">=\",-1e30),(\"TEMP\",\"<=\",+1e30)\n]\n\nddf = dd.read_parquet(\n    crocolake_path,\n    engine=\"pyarrow\",\n    schema=crocolake_schema,\n    columns= cols,\n    filters=filter_coords_time_temp\n)\n\n\n\nIt appears dask is much faster! Yet, it actually creates a delayed object, i.e. ddf contains the instructions that dask will later use to load the dataframe into memory. This allows us to use ddf to schedule all the operations that we‚Äôd normally perform on a dataframe. Eventually we will call ddf.compute() to actually evaluate the instructions.\n\nIf we look at ddf, the dataframe will in fact appear empty:\n\nddf\n\n\n\nWe can explore its first entries with head(), which loads into memory only the first 5 entries:\n\n(Note: head() can return an empty dataframe even when ddf is not empty. This happens because head() looks for the first 5 rows of the first partition, which sometimes happens to be empty. Repartitioning the dask dataframe will get rid of empty partitions and solve this.)\n\n#ddf=ddf.repartition(partition_size=\"300MB\")\nddf.head()\n\n\n\nTo load the whole dataframe, we call compute() (this returns a pandas dataframe).\n\n%%time\nddf_loaded = ddf.compute()\nddf_loaded\n\n\n\nIt took just a few hundreds milliseconds! So yes, dask can be much faster than just using pyarrow and pandas.\n\nIf we look into ddf_loaded now, it will show the populated pandas dataframe.\n\n(Note: if you compare ddf_loaded with df loaded with pyarrow, you‚Äôll see that the rows are not in the same order, yet under the dataframe you can see that the total number of rows is the same. Also the index seems smaller, but dask holds the dataframe in multiple partitions, and the index is reset at every partition.)\n\ndask‚Äôs main advantage is not just in loading the data faster, but in performing operations on larger-than-memory data. For example, it can compute the mean value of TEMPERATURE in the whole Argo PHY dataset without loading the whole data in memory (and pretty quickly, too):\n\n# note that we are not passing any filter conditions or selection of columns to load\nddf_all = dd.read_parquet(\n    crocolake_path,\n    engine=\"pyarrow\",\n    schema=crocolake_schema,\n)\ntemp_mean = ddf_all['TEMP'].mean() # temp_mean is a delayed object: it knows what operations to perform to compute the mean, \n                                   # but doesn't perform them until we call compute() in the next cell\n\n\n\n%%time\ntemp_mean.compute()\n\n\n\n","type":"content","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#dask-approach","position":15},{"hierarchy":{"lvl1":"dask approach","lvl2":"Map"},"type":"lvl2","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#map-1","position":16},{"hierarchy":{"lvl1":"dask approach","lvl2":"Map"},"content":"When producing the map with our dask approach, we would create the grouped dataframe from the delayed dataframe ddf, and compute() it as late as possible.\n\nIn this example, the compute that we are triggering includes four operations:\n\nreading the filtered dataset;\n\ngrouping the dataframe by geographical coordinates;\n\naveraging by pressure and time;\n\nresetting the index;\n\nDask internally builds a graph of all of these operations, so that it knows what subset of data is needed and it optimizes the number and sequence of instructions before executing them.\n\nimport cartopy.crs as ccrs\nimport matplotlib.pyplot as plt\nfrom matplotlib import colormaps\n\nref_var = \"TEMP\"\n# Group by 'LATITUDE' and 'LONGITUDE' and database\nddf_for_map = ddf.groupby([\"LATITUDE\", \"LONGITUDE\", \"DB_NAME\"]).agg({\n    ref_var: 'mean'  # Take the mean intensity at that coordinate\n}).reset_index()\n\n# Plotting using Cartopy\nplt.figure(figsize=(16, 10))\nax = plt.axes(projection=ccrs.PlateCarree())\nax.coastlines()\ncbar_min = ddf_for_map[ref_var].quantile(q=0.05).compute()\ncbar_max = ddf_for_map[ref_var].quantile(q=0.95).compute()\n\n# Group by DB_NAME and plot each group\nddf['DB_NAME'] = ddf.DB_NAME.astype('category')\n\ncmaps = iter( [\"Greens\",\"Oranges\",\"Blues\"] )\nfor db_name, group in ddf_for_map.compute().groupby(\"DB_NAME\"):\n    plt.scatter(\n        group['LONGITUDE'],\n        group['LATITUDE'],\n        s=10,\n        c=group[ref_var],\n        vmin=cbar_min,\n        vmax=cbar_max,\n        cmap=next(cmaps),\n        transform=ccrs.PlateCarree()\n    )\n    plt.colorbar(shrink=0.5, label='Average ' + ref_var + \" (\" + db_name + \")\", pad=0.02)\n\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.title('North-West Atlantic average temperature 2010-2019 (Celsius)')\nplt.grid(True)\nplt.xlim([lon0, lon1])\nplt.ylim([lat0, lat1])\nplt.show()\n\n\n\n","type":"content","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#map-1","position":17},{"hierarchy":{"lvl1":"Suggested exercises"},"type":"lvl1","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#suggested-exercises","position":18},{"hierarchy":{"lvl1":"Suggested exercises"},"content":"Try and access some other data, for example:\n\nfiltering by different time periods;\n\nmapping a different parameter;\n\nrestraining the data of interest further by imposing the pressure PRES to be below 100db;\n\nperforming reads/manipulations that you would need to perform your tasks.\n\nIf you encounter any issues, please \n\nreach out!","type":"content","url":"/notebooks/projects/tutorial3-crocolake-map-temperature-1#suggested-exercises","position":19},{"hierarchy":{"lvl1":"Model-Observation comparison with MOM6 and World Ocean Database"},"type":"lvl1","url":"/notebooks/projects/tutorial4-mom6-wod13-comparison-1","position":0},{"hierarchy":{"lvl1":"Model-Observation comparison with MOM6 and World Ocean Database"},"content":"The goal of this notebook is to get familiar with the basics of using CrocoCamp to interpolate MOM6 output onto the space of observations stored in the World Ocean Database (WOD). We will use output from a MOM6 run already stored on NCAR‚Äôs HPC system, and WOD13 observation files already in obs_seq format and also stored on NCAR‚Äôs HPC system.","type":"content","url":"/notebooks/projects/tutorial4-mom6-wod13-comparison-1","position":1},{"hierarchy":{"lvl1":"Model-Observation comparison with MOM6 and World Ocean Database","lvl3":"Installing CrocoCamp"},"type":"lvl3","url":"/notebooks/projects/tutorial4-mom6-wod13-comparison-1#installing-crococamp","position":2},{"hierarchy":{"lvl1":"Model-Observation comparison with MOM6 and World Ocean Database","lvl3":"Installing CrocoCamp"},"content":"If you don‚Äôt have CrocoCamp set up yet, \n\nhere are the instructions to install it on NCAR‚Äôs HPC. If you have any issues or need to install it on a different machine, please contact \n\nenrico‚Äã.milanese@whoi‚Äã.edu.","type":"content","url":"/notebooks/projects/tutorial4-mom6-wod13-comparison-1#installing-crococamp","position":3},{"hierarchy":{"lvl1":"Model-Observation comparison with MOM6 and World Ocean Database","lvl2":"Running the workflow with custom config options"},"type":"lvl2","url":"/notebooks/projects/tutorial4-mom6-wod13-comparison-1#running-the-workflow-with-custom-config-options","position":4},{"hierarchy":{"lvl1":"Model-Observation comparison with MOM6 and World Ocean Database","lvl2":"Running the workflow with custom config options"},"content":"This notebook uses the same MOM6 files of tutorials 1 and 2, but different observations and output paths. We can either generate a new config file using as reference of from those tutorials (e.g. see config_tutorial_3.yaml), or we can generate a workflow with one of the old templates, and override the paths that are different. While using a config file is the recommended way as it helps keep better track of our workflow, overriding parameters can come handy during exploratory phases. I‚Äôll do the latter to demonstrate this option in CrocoCamp.\n\nRemember that you need to be running the workflow on Casper, as the configuration settings point to DART‚Äôs installation on that machine.\n\nfrom crococamp.workflows import WorkflowModelObs\n\n# Create and run workflow to interpolate MOM6 model onto World Ocean Database obs space\nworkflow_WOD13 = WorkflowModelObs.from_config_file(\n    'config_tutorial_1.yaml', \n    obs_seq_in_folder = '/glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/CrocoCamp/tutorial_3/in_WOD13/',\n    output_folder='$WORK/crocodile_2025/CrocoCamp/tutorial_3/out_obs_seq_in/',\n    input_nml_bck='$WORK/crocodile_2025/CrocoCamp/tutorial_3/input_bckp/',\n    trimmed_obs_folder='$WORK/crocodile_2025/CrocoCamp/tutorial_3/out_trimmed_obs_seq_in/',\n    parquet_folder='$WORK/crocodile_2025/CrocoCamp/tutorial_3/out_parquet/',\n    tmp_folder='$WORK/crocodile_2025/CrocoCamp/tutorial_3/tmp/'\n)\nworkflow_WOD13.run(clear_output=True) #use flag clear_output=True if you want to re-run it and automatically clean all previous output\n\n\n\n","type":"content","url":"/notebooks/projects/tutorial4-mom6-wod13-comparison-1#running-the-workflow-with-custom-config-options","position":5},{"hierarchy":{"lvl1":"Model-Observation comparison with MOM6 and World Ocean Database","lvl2":"Displaying the interactive map"},"type":"lvl2","url":"/notebooks/projects/tutorial4-mom6-wod13-comparison-1#displaying-the-interactive-map","position":6},{"hierarchy":{"lvl1":"Model-Observation comparison with MOM6 and World Ocean Database","lvl2":"Displaying the interactive map"},"content":"CrocoCamp generates a parquet dataset that contains the values of the WOD observations, the MOM6 model data interpolated onto the observations space, and some basic statistics.\n\nYou can load and explore the data using pandas (CrocoCamp supports also dask for large datasets).\n\ngood_model_obs_df = workflow_WOD13.get_good_model_obs_df(compute=True) # compute=True triggers the compute of the dask dataframe, returning a pandas dataframe with data loaded in memory\ngood_model_obs_df.head()                                                   # displays first 5 rows in the dataframe\n\n\n\nLoading the interactive map is as simple as importing the widget and passing the dataframe to it:\n\nfrom crococamp.viz import InteractiveWidgetMap\n# Create an interactive map widget to visualize model-observation comparisons\n# The widget provides controls for selecting variables, observation types, and time ranges\nwidget = InteractiveWidgetMap(good_model_obs_df)\nwidget.setup()\n\n","type":"content","url":"/notebooks/projects/tutorial4-mom6-wod13-comparison-1#displaying-the-interactive-map","position":7},{"hierarchy":{"lvl1":"Model-Observation comparison with MOM6 and World Ocean Database"},"type":"lvl1","url":"/notebooks/projects/tutorial4-mom6-wod13-comparison-1","position":0},{"hierarchy":{"lvl1":"Model-Observation comparison with MOM6 and World Ocean Database"},"content":"The goal of this notebook is to get familiar with the basics of using CrocoCamp to interpolate MOM6 output onto the space of observations stored in the World Ocean Database (WOD). We will use output from a MOM6 run already stored on NCAR‚Äôs HPC system, and WOD13 observation files already in obs_seq format and also stored on NCAR‚Äôs HPC system.","type":"content","url":"/notebooks/projects/tutorial4-mom6-wod13-comparison-1","position":1},{"hierarchy":{"lvl1":"Model-Observation comparison with MOM6 and World Ocean Database","lvl3":"Installing CrocoCamp"},"type":"lvl3","url":"/notebooks/projects/tutorial4-mom6-wod13-comparison-1#installing-crococamp","position":2},{"hierarchy":{"lvl1":"Model-Observation comparison with MOM6 and World Ocean Database","lvl3":"Installing CrocoCamp"},"content":"If you don‚Äôt have CrocoCamp set up yet, \n\nhere are the instructions to install it on NCAR‚Äôs HPC. If you have any issues or need to install it on a different machine, please contact \n\nenrico‚Äã.milanese@whoi‚Äã.edu.","type":"content","url":"/notebooks/projects/tutorial4-mom6-wod13-comparison-1#installing-crococamp","position":3},{"hierarchy":{"lvl1":"Model-Observation comparison with MOM6 and World Ocean Database","lvl2":"Running the workflow with custom config options"},"type":"lvl2","url":"/notebooks/projects/tutorial4-mom6-wod13-comparison-1#running-the-workflow-with-custom-config-options","position":4},{"hierarchy":{"lvl1":"Model-Observation comparison with MOM6 and World Ocean Database","lvl2":"Running the workflow with custom config options"},"content":"This notebook uses the same MOM6 files of tutorials 1 and 2, but different observations and output paths. We can either generate a new config file using as reference of from those tutorials (e.g. see config_tutorial_3.yaml), or we can generate a workflow with one of the old templates, and override the paths that are different. While using a config file is the recommended way as it helps keep better track of our workflow, overriding parameters can come handy during exploratory phases. I‚Äôll do the latter to demonstrate this option in CrocoCamp.\n\nRemember that you need to be running the workflow on Casper, as the configuration settings point to DART‚Äôs installation on that machine.\n\nfrom crococamp.workflows import WorkflowModelObs\n\n# Create and run workflow to interpolate MOM6 model onto World Ocean Database obs space\nworkflow_WOD13 = WorkflowModelObs.from_config_file(\n    'config_tutorial_1.yaml', \n    obs_seq_in_folder = '/glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/CrocoCamp/tutorial_3/in_WOD13/',\n    output_folder='$WORK/crocodile_2025/CrocoCamp/tutorial_3/out_obs_seq_in/',\n    input_nml_bck='$WORK/crocodile_2025/CrocoCamp/tutorial_3/input_bckp/',\n    trimmed_obs_folder='$WORK/crocodile_2025/CrocoCamp/tutorial_3/out_trimmed_obs_seq_in/',\n    parquet_folder='$WORK/crocodile_2025/CrocoCamp/tutorial_3/out_parquet/',\n    tmp_folder='$WORK/crocodile_2025/CrocoCamp/tutorial_3/tmp/'\n)\nworkflow_WOD13.run(clear_output=True) #use flag clear_output=True if you want to re-run it and automatically clean all previous output\n\n\n\n","type":"content","url":"/notebooks/projects/tutorial4-mom6-wod13-comparison-1#running-the-workflow-with-custom-config-options","position":5},{"hierarchy":{"lvl1":"Model-Observation comparison with MOM6 and World Ocean Database","lvl2":"Displaying the interactive map"},"type":"lvl2","url":"/notebooks/projects/tutorial4-mom6-wod13-comparison-1#displaying-the-interactive-map","position":6},{"hierarchy":{"lvl1":"Model-Observation comparison with MOM6 and World Ocean Database","lvl2":"Displaying the interactive map"},"content":"CrocoCamp generates a parquet dataset that contains the values of the WOD observations, the MOM6 model data interpolated onto the observations space, and some basic statistics.\n\nYou can load and explore the data using pandas (CrocoCamp supports also dask for large datasets).\n\ngood_model_obs_df = workflow_WOD13.get_good_model_obs_df(compute=True) # compute=True triggers the compute of the dask dataframe, returning a pandas dataframe with data loaded in memory\ngood_model_obs_df.head()                                                   # displays first 5 rows in the dataframe\n\n\n\nLoading the interactive map is as simple as importing the widget and passing the dataframe to it:\n\nfrom crococamp.viz import InteractiveWidgetMap\n# Create an interactive map widget to visualize model-observation comparisons\n# The widget provides controls for selecting variables, observation types, and time ranges\nwidget = InteractiveWidgetMap(good_model_obs_df)\nwidget.setup()\n\n","type":"content","url":"/notebooks/projects/tutorial4-mom6-wod13-comparison-1#displaying-the-interactive-map","position":7},{"hierarchy":{"lvl1":"Basic CrocoDash Tutorial"},"type":"lvl1","url":"/notebooks/tutorials/crocodash-tutorial","position":0},{"hierarchy":{"lvl1":"Basic CrocoDash Tutorial"},"content":"Set up a basic regional ocean run!\n\nA typical workflow of utilizing CrocoDash consists of four main steps:\n\nGenerate a regional MOM6 domain.\n\nCreate the CESM case.\n\nPrepare ocean forcing data.\n\nBuild and run the case.\n\nThe first step in running CrocoDash is installing it! Please go to \n\nhttps://‚Äãcrocodile‚Äã-cesm‚Äã.github‚Äã.io‚Äã/CrocoDash‚Äã/installation‚Äã.html to install CrocoDash!\n\n","type":"content","url":"/notebooks/tutorials/crocodash-tutorial","position":1},{"hierarchy":{"lvl1":"Basic CrocoDash Tutorial","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl2","url":"/notebooks/tutorials/crocodash-tutorial#section-1-generate-a-regional-mom6-domain","position":2},{"hierarchy":{"lvl1":"Basic CrocoDash Tutorial","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"We begin by defining a regional MOM6 domain using CrocoDash. To do so, we first generate a horizontal grid. We then generate the topography by remapping an existing bathymetric dataset to our horizontal grid. Finally, we define a vertical grid.\n\n","type":"content","url":"/notebooks/tutorials/crocodash-tutorial#section-1-generate-a-regional-mom6-domain","position":3},{"hierarchy":{"lvl1":"Basic CrocoDash Tutorial","lvl3":"Step 1.1: Horizontal Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl3","url":"/notebooks/tutorials/crocodash-tutorial#step-1-1-horizontal-grid","position":4},{"hierarchy":{"lvl1":"Basic CrocoDash Tutorial","lvl3":"Step 1.1: Horizontal Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"\n\nfrom CrocoDash.grid import Grid\n\ngrid = Grid(\n  resolution = 0.05, # in degrees\n  xstart = 278.0, # min longitude in [0, 360]\n  lenx = 3.0, # longitude extent in degrees\n  ystart = 7.0, # min latitude in [-90, 90]\n  leny = 3.0, # latitude extent in degrees\n  name = \"panama1\",\n)\n\n\n\n","type":"content","url":"/notebooks/tutorials/crocodash-tutorial#step-1-1-horizontal-grid","position":5},{"hierarchy":{"lvl1":"Basic CrocoDash Tutorial","lvl3":"Step 1.2: Topography","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl3","url":"/notebooks/tutorials/crocodash-tutorial#step-1-2-topography","position":6},{"hierarchy":{"lvl1":"Basic CrocoDash Tutorial","lvl3":"Step 1.2: Topography","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"\n\nfrom CrocoDash.topo import Topo\n\ntopo = Topo(\n    grid = grid,\n    min_depth = 9.5, # in meters\n)\n\n\n\nfrom pathlib import Path\n\nbathymetry_path= Path(\"/glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/CrocoDash/data/gebco/GEBCO_2024.nc\")\n# The GEBCO bathymetry path on Derecho is: /glade/campaign/cgd/oce/projects/CROCODILE/workshops/2025/CrocoDash/data/gebco/GEBCO_2024.nc\n\nif not bathymetry_path.exists():\n    raise FileNotFoundError(\"Bathymetry file not found, please replace with path to bathymetry file\")\n\ntopo.interpolate_from_file(\n    file_path = bathymetry_path,\n    longitude_coordinate_name=\"lon\",\n    latitude_coordinate_name=\"lat\",\n    vertical_coordinate_name=\"elevation\"\n)\n\n\n\ntopo.depth.plot()\n\n\n\n%matplotlib ipympl\nfrom CrocoDash.topo_editor import TopoEditor\nTopoEditor(topo)\n\n\n\n","type":"content","url":"/notebooks/tutorials/crocodash-tutorial#step-1-2-topography","position":7},{"hierarchy":{"lvl1":"Basic CrocoDash Tutorial","lvl3":"Step 1.3: Vertical Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl3","url":"/notebooks/tutorials/crocodash-tutorial#step-1-3-vertical-grid","position":8},{"hierarchy":{"lvl1":"Basic CrocoDash Tutorial","lvl3":"Step 1.3: Vertical Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"\n\nfrom CrocoDash.vgrid import VGrid\n\nvgrid  = VGrid.hyperbolic(\n    nk = 75, # number of vertical levels\n    depth = topo.max_depth,\n    ratio=20.0 # target ratio of top to bottom layer thicknesses\n)\n\n\n\nimport matplotlib.pyplot as plt\nplt.close()\n# Create the plot\nfor depth in vgrid.z:\n    plt.axhline(y=depth, linestyle='-')  # Horizontal lines\n\nplt.ylim(max(vgrid.z) + 10, min(vgrid.z) - 10)  # Invert y-axis so deeper values go down\nplt.ylabel(\"Depth\")\nplt.title(\"Vertical Grid\")\nplt.show()\n\n\n\n","type":"content","url":"/notebooks/tutorials/crocodash-tutorial#step-1-3-vertical-grid","position":9},{"hierarchy":{"lvl1":"Basic CrocoDash Tutorial","lvl2":"SECTION 2: Create the CESM case"},"type":"lvl2","url":"/notebooks/tutorials/crocodash-tutorial#section-2-create-the-cesm-case","position":10},{"hierarchy":{"lvl1":"Basic CrocoDash Tutorial","lvl2":"SECTION 2: Create the CESM case"},"content":"After generating the MOM6 domain, the next step is to create a CESM case using CrocoDash. This process is straightforward and involves instantiating the CrocoDash Case object. The Case object requires the following inputs:\n\nCESM Source Directory: A local path to a compatible CESM source copy.\n\nCase Name: A unique name for the CESM case.\n\nInput Directory: The directory where all necessary input files will be written.\n\nMOM6 Domain Objects: The horizontal grid, topography, and vertical grid created in the previous section.\n\nProject ID: (Optional) A project ID, if required by the machine.\n\nCompset: The set of models to be used in the Case. Standalone Ocean, Ocean-BGC, Ocean-Seaice, Ocean-Runoff\n\n","type":"content","url":"/notebooks/tutorials/crocodash-tutorial#section-2-create-the-cesm-case","position":11},{"hierarchy":{"lvl1":"Basic CrocoDash Tutorial","lvl3":"Step 2.1: Specify case name and directories:","lvl2":"SECTION 2: Create the CESM case"},"type":"lvl3","url":"/notebooks/tutorials/crocodash-tutorial#step-2-1-specify-case-name-and-directories","position":12},{"hierarchy":{"lvl1":"Basic CrocoDash Tutorial","lvl3":"Step 2.1: Specify case name and directories:","lvl2":"SECTION 2: Create the CESM case"},"content":"Begin by specifying the case name and the necessary directory paths. Ensure the CESM root directory points to your own local copy of CESM. Below is an example setup:\n\nfrom pathlib import Path\n\n\n\n# CESM case (experiment) name\ncasename = \"panama-not\"\n\n# CESM source root (Update this path accordingly!!!)\ncesmroot =\"/glade/work/<YOURUSERNAME>/CROCESM\"\n\n# Place where all your input files go \ninputdir = Path(\"/glade/work/<YOURUSERNAME>/crocodile_2025\") / \"croc_input\" / casename\n    \n# CESM case directory\ncaseroot = Path(\"/glade/work/<YOURUSERNAME>/crocodile_2025\") / \"croc_cases\" / casename\n\n\n\n","type":"content","url":"/notebooks/tutorials/crocodash-tutorial#step-2-1-specify-case-name-and-directories","position":13},{"hierarchy":{"lvl1":"Basic CrocoDash Tutorial","lvl3":"Step 2.2: Create the Case","lvl2":"SECTION 2: Create the CESM case"},"type":"lvl3","url":"/notebooks/tutorials/crocodash-tutorial#step-2-2-create-the-case","position":14},{"hierarchy":{"lvl1":"Basic CrocoDash Tutorial","lvl3":"Step 2.2: Create the Case","lvl2":"SECTION 2: Create the CESM case"},"content":"To create the CESM case, instantiate the Case object as shown below. This will automatically set up the CESM case based on the provided inputs: The cesmroot argument specifies the path to your local CESM source directory.\nThe caseroot argument defines the directory where the case will be created. CrocoDash will handle all necessary namelist modifications and XML changes to align with the MOM6 domain objects generated earlier.\n\nfrom CrocoDash.case import Case\n\ncase = Case(\n    cesmroot = cesmroot,\n    caseroot = caseroot,\n    inputdir = inputdir,\n    ocn_grid = grid,\n    ocn_vgrid = vgrid,\n    ocn_topo = topo,\n    project = 'CESM0030',\n    override = True,\n    machine = \"derecho\",\n    compset = \"GR_JRA\" # This is the alias of the compset, the longname (which is printed when you run this command) is 1850_DATM%JRA_SLND_SICE_MOM6%REGIONAL_SROF_SGLC_SWAV. Feel free to use either way!\n)\n\n\n\n","type":"content","url":"/notebooks/tutorials/crocodash-tutorial#step-2-2-create-the-case","position":15},{"hierarchy":{"lvl1":"Basic CrocoDash Tutorial","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl2","url":"/notebooks/tutorials/crocodash-tutorial#section-3-prepare-ocean-forcing-data","position":16},{"hierarchy":{"lvl1":"Basic CrocoDash Tutorial","lvl2":"Section 3: Prepare ocean forcing data"},"content":"We need to cut out our ocean forcing. The package expects an initial condition and one time-dependent segment per non-land boundary. Naming convention is \"east_unprocessed\" for segments and \"ic_unprocessed\" for the initial condition.\n\nIn this notebook, we are forcing with the Copernicus Marine ‚ÄúGlorys‚Äù reanalysis dataset. There‚Äôs a function in the CrocoDash package, called configure_forcings, that generates a bash script to download the correct boundary forcing files for your experiment. First, you will need to create an account with Copernicus, and then call copernicusmarine login to set up your login details on your machine. Then you can run the get_glorys_data.sh bash script.","type":"content","url":"/notebooks/tutorials/crocodash-tutorial#section-3-prepare-ocean-forcing-data","position":17},{"hierarchy":{"lvl1":"Basic CrocoDash Tutorial","lvl3":"Step 3.1 Configure Initial Conditions and Forcings","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl3","url":"/notebooks/tutorials/crocodash-tutorial#step-3-1-configure-initial-conditions-and-forcings","position":18},{"hierarchy":{"lvl1":"Basic CrocoDash Tutorial","lvl3":"Step 3.1 Configure Initial Conditions and Forcings","lvl2":"Section 3: Prepare ocean forcing data"},"content":"\n\ncase.configure_forcings(\n    date_range = [\"2020-01-01 00:00:00\", \"2020-01-09 00:00:00\"],\n    boundaries=[\"south\",\"east\",\"west\"],\n    function_name=\"get_glorys_data_from_rda\"\n)\n\n\n\n","type":"content","url":"/notebooks/tutorials/crocodash-tutorial#step-3-1-configure-initial-conditions-and-forcings","position":19},{"hierarchy":{"lvl1":"Basic CrocoDash Tutorial","lvl3":"Step 3.3: Process forcing data","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl3","url":"/notebooks/tutorials/crocodash-tutorial#step-3-3-process-forcing-data","position":20},{"hierarchy":{"lvl1":"Basic CrocoDash Tutorial","lvl3":"Step 3.3: Process forcing data","lvl2":"Section 3: Prepare ocean forcing data"},"content":"In this final step, we call the process_forcings method of CrocoDash to cut out and interpolate the initial condition as well as all boundaries. CrocoDash also updates MOM6 runtime parameters and CESM xml variables accordingly.\n\ncase.process_forcings()\n\n\n\nprint(\"You can now build and run your case at\",caseroot)\n\n\n\n","type":"content","url":"/notebooks/tutorials/crocodash-tutorial#step-3-3-process-forcing-data","position":21},{"hierarchy":{"lvl1":"Basic CrocoDash Tutorial","lvl2":"Section 4: Build and run the case"},"type":"lvl2","url":"/notebooks/tutorials/crocodash-tutorial#section-4-build-and-run-the-case","position":22},{"hierarchy":{"lvl1":"Basic CrocoDash Tutorial","lvl2":"Section 4: Build and run the case"},"content":"After completing the previous steps, you are ready to build and run your CESM case. Begin by navigating to the case root directory specified during the case creation. Before proceeding, review the user_nl_mom file located in the case directory. This file contains MOM6 parameter settings that were automatically generated by CrocoDash. Carefully examine these parameters and make any necessary adjustments to fine-tune the model for your specific requirements. While CrocoDash aims to provide a solid starting point, further tuning and adjustments are typically necessary to improve the model for your use case.\n\nOnce you have reviewed and modified the parameters as needed, you can build and execute the case using the following commands:qcmd -- ./case.build\n./case.submit","type":"content","url":"/notebooks/tutorials/crocodash-tutorial#section-4-build-and-run-the-case","position":23},{"hierarchy":{"lvl1":"Set up a rectangular regional CESM-MOM6-CICE coupled run"},"type":"lvl1","url":"/notebooks/use-cases/mom6-cice-antarctica","position":0},{"hierarchy":{"lvl1":"Set up a rectangular regional CESM-MOM6-CICE coupled run"},"content":"We‚Äôll go through these steps:\n\nGenerate a regional MOM6 domain.\n\nCreate the CESM case.\n\nPrepare ocean forcing data.\n\nGenerate the regional CICE grid.\n\nBuild and run the case.\n\nRemember to swap your environment to CrocoDash first!\n\n","type":"content","url":"/notebooks/use-cases/mom6-cice-antarctica","position":1},{"hierarchy":{"lvl1":"Set up a rectangular regional CESM-MOM6-CICE coupled run","lvl2":"Section 1: Generate a regional MOM6 domain"},"type":"lvl2","url":"/notebooks/use-cases/mom6-cice-antarctica#section-1-generate-a-regional-mom6-domain","position":2},{"hierarchy":{"lvl1":"Set up a rectangular regional CESM-MOM6-CICE coupled run","lvl2":"Section 1: Generate a regional MOM6 domain"},"content":"We begin by defining a regional MOM6 domain using CrocoDash. To do so, we first generate a horizontal grid. We then generate the topography by remapping an existing bathymetric dataset to our horizontal grid. Finally, we define a vertical grid.\n\n","type":"content","url":"/notebooks/use-cases/mom6-cice-antarctica#section-1-generate-a-regional-mom6-domain","position":3},{"hierarchy":{"lvl1":"Set up a rectangular regional CESM-MOM6-CICE coupled run","lvl3":"Step 1.1: Horizontal Grid","lvl2":"Section 1: Generate a regional MOM6 domain"},"type":"lvl3","url":"/notebooks/use-cases/mom6-cice-antarctica#step-1-1-horizontal-grid","position":4},{"hierarchy":{"lvl1":"Set up a rectangular regional CESM-MOM6-CICE coupled run","lvl3":"Step 1.1: Horizontal Grid","lvl2":"Section 1: Generate a regional MOM6 domain"},"content":"The horizontal grid described below is from the Antarctic domain (\n\nhttps://‚Äãmaps‚Äã.app‚Äã.goo‚Äã.gl‚Äã/WLLzioPgahQpLMSK8 - near the Mendel station) at 60¬∞W, 64¬∞S.\n\nfrom CrocoDash.grid import Grid\n\ngrid = Grid(\n  resolution = 0.01,\n  xstart = 300.0,\n  lenx = 4.0,\n  ystart = -64.0,\n  leny = 4.0,\n  name = \"antarctica2\",\n)\n\n\n\n","type":"content","url":"/notebooks/use-cases/mom6-cice-antarctica#step-1-1-horizontal-grid","position":5},{"hierarchy":{"lvl1":"Set up a rectangular regional CESM-MOM6-CICE coupled run","lvl3":"Step 1.2: Topography","lvl2":"Section 1: Generate a regional MOM6 domain"},"type":"lvl3","url":"/notebooks/use-cases/mom6-cice-antarctica#step-1-2-topography","position":6},{"hierarchy":{"lvl1":"Set up a rectangular regional CESM-MOM6-CICE coupled run","lvl3":"Step 1.2: Topography","lvl2":"Section 1: Generate a regional MOM6 domain"},"content":"\n\nfrom CrocoDash.topo import Topo\n\ntopo = Topo(\n    grid = grid,\n    min_depth = 9.5,\n)\n\n\n\nSwap your bathymetry_path accordingly below. Grab your data from: \n\nhttps://‚Äãwww‚Äã.gebco‚Äã.net‚Äã/data‚Äã-products‚Äã/gridded‚Äã-bathymetry‚Äã-data\n\nbathymetry_path='<GEBCO>' # Swap this!\n\ntopo.interpolate_from_file(\n    file_path = bathymetry_path,\n    longitude_coordinate_name=\"lon\",\n    latitude_coordinate_name=\"lat\",\n    vertical_coordinate_name=\"elevation\",\n    write_to_file = True\n)\n\n\n\nTODO: when the above topo.interpolate_from_file() method is called with the original GEBCO dataset, the interpolation fails do to large computational demand when run on login nodes. Check if there is a failsafe way, or at least, a means to precaution the user.\n\ntopo.depth.plot()\n\n\n\nRun the topo-editor below to configure your bathymetry!\n\n%matplotlib ipympl\nfrom CrocoDash.topo_editor import TopoEditor\ntopo.depth[\"units\"] = \"m\"\nTopoEditor(topo)\n\n\n\n","type":"content","url":"/notebooks/use-cases/mom6-cice-antarctica#step-1-2-topography","position":7},{"hierarchy":{"lvl1":"Set up a rectangular regional CESM-MOM6-CICE coupled run","lvl3":"Step 1.3: Vertical Grid","lvl2":"Section 1: Generate a regional MOM6 domain"},"type":"lvl3","url":"/notebooks/use-cases/mom6-cice-antarctica#step-1-3-vertical-grid","position":8},{"hierarchy":{"lvl1":"Set up a rectangular regional CESM-MOM6-CICE coupled run","lvl3":"Step 1.3: Vertical Grid","lvl2":"Section 1: Generate a regional MOM6 domain"},"content":"\n\nfrom CrocoDash.vgrid import VGrid\n\nvgrid  = VGrid.hyperbolic(\n    nk = 75,\n    depth = topo.max_depth,\n    ratio=20.0\n)\n\n\n\nprint(vgrid.dz)\n\n\n\nimport matplotlib.pyplot as plt\nplt.close()\n# Create the plot\nfor depth in vgrid.z:\n    plt.axhline(y=depth, linestyle='-')  # Horizontal lines\n\nplt.ylim(max(vgrid.z) + 10, min(vgrid.z) - 10)  # Invert y-axis so deeper values go down\nplt.ylabel(\"Depth\")\nplt.title(\"Vertical Grid\")\nplt.show()\n\n\n\n","type":"content","url":"/notebooks/use-cases/mom6-cice-antarctica#step-1-3-vertical-grid","position":9},{"hierarchy":{"lvl1":"Set up a rectangular regional CESM-MOM6-CICE coupled run","lvl2":"SECTION 3: Create the CESM case"},"type":"lvl2","url":"/notebooks/use-cases/mom6-cice-antarctica#section-3-create-the-cesm-case","position":10},{"hierarchy":{"lvl1":"Set up a rectangular regional CESM-MOM6-CICE coupled run","lvl2":"SECTION 3: Create the CESM case"},"content":"After generating the MOM6 domain, the next step is to create a CESM case using CrocoDash. This process is straightforward and involves instantiating the CrocoDash Case object. The Case object requires the following inputs:\n\nCESM Source Directory: A local path to a compatible CESM source copy.\n\nCase Name: A unique name for the CESM case.\n\nInput Directory: The directory where all necessary input files will be written.\n\nMOM6 Domain Objects: The horizontal grid, topography, and vertical grid created in the previous section.\n\nProject ID: (Optional) A project ID, if required by the machine.\n\n","type":"content","url":"/notebooks/use-cases/mom6-cice-antarctica#section-3-create-the-cesm-case","position":11},{"hierarchy":{"lvl1":"Set up a rectangular regional CESM-MOM6-CICE coupled run","lvl3":"Step 2.1: Specify case name and directories:","lvl2":"SECTION 3: Create the CESM case"},"type":"lvl3","url":"/notebooks/use-cases/mom6-cice-antarctica#step-2-1-specify-case-name-and-directories","position":12},{"hierarchy":{"lvl1":"Set up a rectangular regional CESM-MOM6-CICE coupled run","lvl3":"Step 2.1: Specify case name and directories:","lvl2":"SECTION 3: Create the CESM case"},"content":"Begin by specifying the case name and the necessary directory paths. Ensure the CESM root directory points to your own local copy of CESM.\nNote that if the case is already created - in case you want to rerun this notebook - the /run directory must be removed for the case to be created anew.\nBelow is an example setup:\n\nfrom pathlib import Path\n\n\n\n# CESM case (experiment) name\ncasename = \"antarctica-tutorial\"\n\n# CESM source root (Update this path accordingly!!!)\ncesmroot = \"<CESM>\"\n\n# Place where all your input files go\ninputdir = Path.home() / \"input\" / casename\n\n# CESM case directory\ncaseroot = Path.home() / \"cases\" / casename\nprint(cesmroot, inputdir, caseroot) # View your directory and change as you need!\n\n\n\n","type":"content","url":"/notebooks/use-cases/mom6-cice-antarctica#step-2-1-specify-case-name-and-directories","position":13},{"hierarchy":{"lvl1":"Set up a rectangular regional CESM-MOM6-CICE coupled run","lvl3":"Step 2.2: Create the Case","lvl2":"SECTION 3: Create the CESM case"},"type":"lvl3","url":"/notebooks/use-cases/mom6-cice-antarctica#step-2-2-create-the-case","position":14},{"hierarchy":{"lvl1":"Set up a rectangular regional CESM-MOM6-CICE coupled run","lvl3":"Step 2.2: Create the Case","lvl2":"SECTION 3: Create the CESM case"},"content":"To create the CESM case, instantiate the Case object as shown below. This will automatically set up the CESM case based on the provided inputs: The cesmroot argument specifies the path to your local CESM source directory.\nThe caseroot argument defines the directory where the case will be created. CrocoDash will handle all necessary namelist modifications and XML changes to align with the MOM6 domain objects generated earlier.\n\nRemember to change/remove your project ID (with parameter project) and machine (with parameter machine, likely derecho) as needed.\n\nfrom CrocoDash.case import Case\n\ncase = Case(\n    cesmroot = cesmroot,\n    caseroot = caseroot,\n    inputdir = inputdir,\n    ocn_grid = grid,\n    ocn_vgrid = vgrid,\n    ocn_topo = topo,\n    project = 'P93300012', # Switch this\n    override = True,\n    machine = \"derecho\", # And this\n    compset = \"1850_DATM%JRA_SLND_CICE_MOM6_SROF_SGLC_SWAV\", \n)\n\n\n\n","type":"content","url":"/notebooks/use-cases/mom6-cice-antarctica#step-2-2-create-the-case","position":15},{"hierarchy":{"lvl1":"Set up a rectangular regional CESM-MOM6-CICE coupled run","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl2","url":"/notebooks/use-cases/mom6-cice-antarctica#section-3-prepare-ocean-forcing-data","position":16},{"hierarchy":{"lvl1":"Set up a rectangular regional CESM-MOM6-CICE coupled run","lvl2":"Section 3: Prepare ocean forcing data"},"content":"We need to cut out our ocean forcing. The package expects an initial condition and one time-dependent segment per non-land boundary. Naming convention is \"east_unprocessed\" for segments and \"ic_unprocessed\" for the initial condition.\n\nIn this notebook, we are forcing with the Copernicus Marine ‚ÄúGlorys‚Äù reanalysis dataset. There‚Äôs a function in the CrocoDash package, called configure_forcings, that generates a bash script to download the correct boundary forcing files for your experiment. First, you will need to create an account with Copernicus, and then call copernicusmarine login to set up your login details on your machine. Then you can run the get_glorys_data.sh bash script.","type":"content","url":"/notebooks/use-cases/mom6-cice-antarctica#section-3-prepare-ocean-forcing-data","position":17},{"hierarchy":{"lvl1":"Set up a rectangular regional CESM-MOM6-CICE coupled run","lvl3":"Step 3.1 Configure Initial Conditions and Forcings","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl3","url":"/notebooks/use-cases/mom6-cice-antarctica#step-3-1-configure-initial-conditions-and-forcings","position":18},{"hierarchy":{"lvl1":"Set up a rectangular regional CESM-MOM6-CICE coupled run","lvl3":"Step 3.1 Configure Initial Conditions and Forcings","lvl2":"Section 3: Prepare ocean forcing data"},"content":"\n\nfrom CrocoDash.case import Case\ncase.configure_forcings(\n    date_range = [\"2020-01-01 00:00:00\", \"2020-01-09 00:00:00\"],\n)\n\n\n\n","type":"content","url":"/notebooks/use-cases/mom6-cice-antarctica#step-3-1-configure-initial-conditions-and-forcings","position":19},{"hierarchy":{"lvl1":"Set up a rectangular regional CESM-MOM6-CICE coupled run","lvl3":"Step 3.2 Run get_glorys_data.sh","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl3","url":"/notebooks/use-cases/mom6-cice-antarctica#step-3-2-run-get-glorys-data-sh","position":20},{"hierarchy":{"lvl1":"Set up a rectangular regional CESM-MOM6-CICE coupled run","lvl3":"Step 3.2 Run get_glorys_data.sh","lvl2":"Section 3: Prepare ocean forcing data"},"content":"Follow the instructions printed by the configure_forcings method above to navigate to your glorys folder.\nOnce you‚Äôre in your /glorys folder, run chmod +x get_glorys_data.sh to enable execution of the bash script. Also activate your conda environment with:module load conda && conda activate CrocoDash ! or the name of the virtual environment you have installed with CrocoDash enabled\n\nif you haven‚Äôt done so. Finally, run ./get_glorys_data.sh.\n\nNote: You‚Äôll have to enter your login multiple times for copernicus marine. Do not escape the script until finished.\n\nTODO: user copernicusmarine python API within CrocoDash, instead of directing users to run it via CLI. Also, on a derecho login node, both CLI and API fails to run due to the computational demand. We also need to address that.\n\n","type":"content","url":"/notebooks/use-cases/mom6-cice-antarctica#step-3-2-run-get-glorys-data-sh","position":21},{"hierarchy":{"lvl1":"Set up a rectangular regional CESM-MOM6-CICE coupled run","lvl3":"Step 3.3: Process forcing data","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl3","url":"/notebooks/use-cases/mom6-cice-antarctica#step-3-3-process-forcing-data","position":22},{"hierarchy":{"lvl1":"Set up a rectangular regional CESM-MOM6-CICE coupled run","lvl3":"Step 3.3: Process forcing data","lvl2":"Section 3: Prepare ocean forcing data"},"content":"In this final step, we call the process_forcings method of CrocoDash to cut out and interpolate the initial condition as well as all boundaries. CrocoDash also updates MOM6 runtime parameters and CESM xml variables accordingly.\n\ncase.process_forcings()\n\n\n\n","type":"content","url":"/notebooks/use-cases/mom6-cice-antarctica#step-3-3-process-forcing-data","position":23},{"hierarchy":{"lvl1":"Set up a rectangular regional CESM-MOM6-CICE coupled run","lvl3":"Optional: Use restart files as initial condition for your run","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl3","url":"/notebooks/use-cases/mom6-cice-antarctica#optional-use-restart-files-as-initial-condition-for-your-run","position":24},{"hierarchy":{"lvl1":"Set up a rectangular regional CESM-MOM6-CICE coupled run","lvl3":"Optional: Use restart files as initial condition for your run","lvl2":"Section 3: Prepare ocean forcing data"},"content":"\n\nIn case you want to start the model with a restart file instead of using the generated initial condition, follow the below steps. Note that you have to have finished a run beforehand for the restart files to appear.\n\nLocate your restart file - they are usually in your previous case‚Äôs /archive/rest/<year> folder with a .r infix, e.g., cice.test.cice.r.1994-01-01-00000.nc.\nIf you do not know your /archive folder location, run ./xmlquery DOUT_S_ROOT in your (previous) case folder, which will return a path similar to DOUT_S_ROOT: <PATH TO ARCHIVE>.\n\nUse the cp command to copy the file to your current case /run directory, e.g., cp cice.test.cice.r.1994-01-01-00000.nc <your_run_dir>.\n\nOpen user_nl_cice in your case directory again and change the ice_ic variable from \"UNSET\" to your file name, e.g., ice_ic=cice.test.cice.r.1994-01-01-00000.nc\n\nYour file will now be used automatically as the ice initial condition for your next run. Note that .h and .h1 files (i.e. history files) currently cannot be used as initial conditions for CICE.\n\n","type":"content","url":"/notebooks/use-cases/mom6-cice-antarctica#optional-use-restart-files-as-initial-condition-for-your-run","position":25},{"hierarchy":{"lvl1":"Set up a rectangular regional CESM-MOM6-CICE coupled run","lvl2":"Section 5: Build and run the case"},"type":"lvl2","url":"/notebooks/use-cases/mom6-cice-antarctica#section-5-build-and-run-the-case","position":26},{"hierarchy":{"lvl1":"Set up a rectangular regional CESM-MOM6-CICE coupled run","lvl2":"Section 5: Build and run the case"},"content":"After completing the previous steps, you are ready to build and run your CESM case. Begin by navigating to the case root directory specified during the case creation. Before proceeding, review the user_nl_mom file located in the case directory. This file contains MOM6 parameter settings that were automatically generated by CrocoDash. Carefully examine these parameters and make any necessary adjustments to fine-tune the model for your specific requirements. While CrocoDash aims to provide a solid starting point, further tuning and adjustments are typically necessary to improve the model for your use case.\n\nOnce you have reviewed and modified the parameters as needed, you can build and execute the case using the following commands:./case.build\n./case.submit\n\n","type":"content","url":"/notebooks/use-cases/mom6-cice-antarctica#section-5-build-and-run-the-case","position":27},{"hierarchy":{"lvl1":"Set up a rectangular regional CESM-MOM6-CICE coupled run","lvl4":"Optional: Write full grid","lvl2":"Section 5: Build and run the case"},"type":"lvl4","url":"/notebooks/use-cases/mom6-cice-antarctica#optional-write-full-grid","position":28},{"hierarchy":{"lvl1":"Set up a rectangular regional CESM-MOM6-CICE coupled run","lvl4":"Optional: Write full grid","lvl2":"Section 5: Build and run the case"},"content":"\n\nThe function below will output the full MOM6 grid in case you need it!\n\nimport xarray as xr\nfrom CrocoDash.grid import Grid\n\n# List of explicitly defined grid metric attributes\nvarnames = [\n    \"tlon\", \"tlat\", \"ulon\", \"ulat\", \"vlon\", \"vlat\", \"qlon\", \"qlat\",\n    \"dxt\", \"dyt\", \"dxCv\", \"dyCu\", \"dxCu\", \"dyCv\", \"angle\", \"tarea\"\n]\n\ndata_vars = {name: getattr(grid, name) for name in varnames}\n\n# Create Dataset\nds = xr.Dataset(data_vars)\nds.attrs[\"name\"] = getattr(grid, \"_name\", \"unnamed_grid\")\n\n# Write to NetCDF\nds.to_netcdf(\"antarctica_fullvar_grid.nc\") # Change as needed!\n\n\n\n# Check your variables!\nfor name in varnames:\n    var = getattr(grid, name)\n    print(f\"{name}: shape={var.shape}, dims={getattr(var, 'dims', 'unknown')}\")\n\n","type":"content","url":"/notebooks/use-cases/mom6-cice-antarctica#optional-write-full-grid","position":29},{"hierarchy":{"lvl1":"Setup the NWA12 domain through CrocoDash"},"type":"lvl1","url":"/notebooks/use-cases/three-boundary","position":0},{"hierarchy":{"lvl1":"Setup the NWA12 domain through CrocoDash"},"content":"","type":"content","url":"/notebooks/use-cases/three-boundary","position":1},{"hierarchy":{"lvl1":"Setup the NWA12 domain through CrocoDash","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl2","url":"/notebooks/use-cases/three-boundary#section-1-generate-a-regional-mom6-domain","position":2},{"hierarchy":{"lvl1":"Setup the NWA12 domain through CrocoDash","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"We begin by defining a regional MOM6 domain using CrocoDash. To do so, we first carve out a subdomain from a 1/12 degree global grid. We then generate the topography by remapping an existing bathymetric dataset to our horizontal grid. Finally, we define a vertical grid.\n\n","type":"content","url":"/notebooks/use-cases/three-boundary#section-1-generate-a-regional-mom6-domain","position":3},{"hierarchy":{"lvl1":"Setup the NWA12 domain through CrocoDash","lvl3":"Step 1.1: Horizontal Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl3","url":"/notebooks/use-cases/three-boundary#step-1-1-horizontal-grid","position":4},{"hierarchy":{"lvl1":"Setup the NWA12 domain through CrocoDash","lvl3":"Step 1.1: Horizontal Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"Extract a subgrid from a global grid using the subgrid_from_supergrid method:\n\nfrom CrocoDash.grid import Grid\n\ngrid = Grid.subgrid_from_supergrid(\n    path = \"<HGRID_TRIMMED>\",  # supergrid\n    llc = (5, -99),  # (l)ower (l)eft (c)orner coords\n    urc = (59, -36.),  # (u)pper (r)ight (c)orner coords\n    name = \"nwa12\"\n)\n\n\n\n","type":"content","url":"/notebooks/use-cases/three-boundary#step-1-1-horizontal-grid","position":5},{"hierarchy":{"lvl1":"Setup the NWA12 domain through CrocoDash","lvl3":"Step 1.2: Topography","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl3","url":"/notebooks/use-cases/three-boundary#step-1-2-topography","position":6},{"hierarchy":{"lvl1":"Setup the NWA12 domain through CrocoDash","lvl3":"Step 1.2: Topography","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"\n\nfrom CrocoDash.topo import Topo\n\ntopo = Topo(\n    grid = grid,\n    min_depth = 9.5,\n)\n\n\n\nbathymetry_path='<GEBCO>'\n\ntopo.interpolate_from_file(\n    file_path = bathymetry_path,\n    longitude_coordinate_name=\"lon\",\n    latitude_coordinate_name=\"lat\",\n    vertical_coordinate_name=\"elevation\"\n)\n\n\n\ntopo.depth.plot()\n\n\n\nTODO: Implement a subset_global_topo-like method in mom6 and use it for this example instead of interpolation.\n\n","type":"content","url":"/notebooks/use-cases/three-boundary#step-1-2-topography","position":7},{"hierarchy":{"lvl1":"Setup the NWA12 domain through CrocoDash","lvl3":"Step 1.3: Vertical Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl3","url":"/notebooks/use-cases/three-boundary#step-1-3-vertical-grid","position":8},{"hierarchy":{"lvl1":"Setup the NWA12 domain through CrocoDash","lvl3":"Step 1.3: Vertical Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"\n\nfrom CrocoDash.vgrid import VGrid\n\nvgrid  = VGrid.hyperbolic(\n    nk = 75,\n    depth = topo.max_depth,\n    ratio=20.0\n)\n\n\n\n","type":"content","url":"/notebooks/use-cases/three-boundary#step-1-3-vertical-grid","position":9},{"hierarchy":{"lvl1":"Setup the NWA12 domain through CrocoDash","lvl2":"SECTION 2: Create the CESM case"},"type":"lvl2","url":"/notebooks/use-cases/three-boundary#section-2-create-the-cesm-case","position":10},{"hierarchy":{"lvl1":"Setup the NWA12 domain through CrocoDash","lvl2":"SECTION 2: Create the CESM case"},"content":"After generating the MOM6 domain, the next step is to create a CESM case using CrocoDash. This process is straightforward and involves instantiating the CrocoDash Case object. The Case object requires the following inputs:\n\nCESM Source Directory: A local path to a compatible CESM source copy.\n\nCase Name: A unique name for the CESM case.\n\nInput Directory: The directory where all necessary input files will be written.\n\nMOM6 Domain Objects: The horizontal grid, topography, and vertical grid created in the previous section.\n\nProject ID: (Optional) A project ID, if required by the machine.\n\nfrom pathlib import Path\n\n\n\n\n# CESM case (experiment) name\ncasename = \"three-boundary-nwa\"\n\n# CESM source root (Update this path accordingly!!!)\ncesmroot = \"<CESM>\"\n\n# Place where all your input files go \ninputdir = Path.home() / \"croc_input\" / casename\n    \n# CESM case directory\ncaseroot = Path.home() / \"croc_cases\" / casename\n\n\n\n","type":"content","url":"/notebooks/use-cases/three-boundary#section-2-create-the-cesm-case","position":11},{"hierarchy":{"lvl1":"Setup the NWA12 domain through CrocoDash","lvl3":"Step 2.2: Create the Case","lvl2":"SECTION 2: Create the CESM case"},"type":"lvl3","url":"/notebooks/use-cases/three-boundary#step-2-2-create-the-case","position":12},{"hierarchy":{"lvl1":"Setup the NWA12 domain through CrocoDash","lvl3":"Step 2.2: Create the Case","lvl2":"SECTION 2: Create the CESM case"},"content":"To create the CESM case, instantiate the Case object as shown below. This will automatically set up the CESM case based on the provided inputs: The cesmroot argument specifies the path to your local CESM source directory.\nThe caseroot argument defines the directory where the case will be created. CrocoDash will handle all necessary namelist modifications and XML changes to align with the MOM6 domain objects generated earlier.\n\nfrom CrocoDash.case import Case\n\ncase = Case(\n    cesmroot = cesmroot,\n    caseroot = caseroot,\n    inputdir = inputdir,\n    ocn_grid = grid,\n    ocn_vgrid = vgrid,\n    ocn_topo = topo,\n    project = 'NCGD0011',\n    override = False,\n)\n\n\n\n","type":"content","url":"/notebooks/use-cases/three-boundary#step-2-2-create-the-case","position":13},{"hierarchy":{"lvl1":"Setup the NWA12 domain through CrocoDash","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl2","url":"/notebooks/use-cases/three-boundary#section-3-prepare-ocean-forcing-data","position":14},{"hierarchy":{"lvl1":"Setup the NWA12 domain through CrocoDash","lvl2":"Section 3: Prepare ocean forcing data"},"content":"We need to cut out our ocean forcing. The package expects an initial condition and one time-dependent segment per non-land boundary. Naming convention is \"east_unprocessed\" for segments and \"ic_unprocessed\" for the initial condition.\n\nIn this notebook, we are forcing with the Copernicus Marine ‚ÄúGlorys‚Äù reanalysis dataset. There‚Äôs a function in the CrocoDash package, called configure_forcings, that generates a bash script to download the correct boundary forcing files for your experiment. First, you will need to create an account with Copernicus, and then call copernicusmarine login to set up your login details on your machine. Then you can run the get_glorys_data.sh bash script.","type":"content","url":"/notebooks/use-cases/three-boundary#section-3-prepare-ocean-forcing-data","position":15},{"hierarchy":{"lvl1":"Setup the NWA12 domain through CrocoDash","lvl3":"Step 3.1 Configure Initial Conditions and Forcings","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl3","url":"/notebooks/use-cases/three-boundary#step-3-1-configure-initial-conditions-and-forcings","position":16},{"hierarchy":{"lvl1":"Setup the NWA12 domain through CrocoDash","lvl3":"Step 3.1 Configure Initial Conditions and Forcings","lvl2":"Section 3: Prepare ocean forcing data"},"content":"\n\ncase.configure_forcings(\n    date_range = [\"2020-01-01 00:00:00\", \"2020-02-01 00:00:00\"],\n    boundaries = [\"south\", \"north\",\"east\"], \n    tidal_constituents = ['M2'],\n    tpxo_elevation_filepath = \"<TPXO_H>\",\n    tpxo_velocity_filepath = \"<TPXO_U>\"\n)\n\n\n\n","type":"content","url":"/notebooks/use-cases/three-boundary#step-3-1-configure-initial-conditions-and-forcings","position":17},{"hierarchy":{"lvl1":"Setup the NWA12 domain through CrocoDash","lvl3":"Step 3.2 Run get_glorys_data.sh","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl3","url":"/notebooks/use-cases/three-boundary#step-3-2-run-get-glorys-data-sh","position":18},{"hierarchy":{"lvl1":"Setup the NWA12 domain through CrocoDash","lvl3":"Step 3.2 Run get_glorys_data.sh","lvl2":"Section 3: Prepare ocean forcing data"},"content":"In a terminal session, locate the get_glorys_data.sh script and execute it to download the initial conditions and boundary conditions. Follow the instructions printed by the configure_forcings method above.\n\nTODO: user copernicusmarine python API within CrocoDash, instead of directing users to run it via CLI. Also, on a derecho login node, both CLI and API fails to run due to the computational demand. We also need to address that.\n\n","type":"content","url":"/notebooks/use-cases/three-boundary#step-3-2-run-get-glorys-data-sh","position":19},{"hierarchy":{"lvl1":"Setup the NWA12 domain through CrocoDash","lvl3":"Step 3.3: Process forcing data","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl3","url":"/notebooks/use-cases/three-boundary#step-3-3-process-forcing-data","position":20},{"hierarchy":{"lvl1":"Setup the NWA12 domain through CrocoDash","lvl3":"Step 3.3: Process forcing data","lvl2":"Section 3: Prepare ocean forcing data"},"content":"In this final step, we call the process_forcings method of CrocoDash to cut out and interpolate the initial condition as well as all boundaries. CrocoDash also updates MOM6 runtime parameters and CESM xml variables accordingly.\n\ncase.process_forcings()\n\n\n\n","type":"content","url":"/notebooks/use-cases/three-boundary#step-3-3-process-forcing-data","position":21},{"hierarchy":{"lvl1":"Setup the NWA12 domain through CrocoDash","lvl2":"Section 4: Build and run the case"},"type":"lvl2","url":"/notebooks/use-cases/three-boundary#section-4-build-and-run-the-case","position":22},{"hierarchy":{"lvl1":"Setup the NWA12 domain through CrocoDash","lvl2":"Section 4: Build and run the case"},"content":"After completing the previous steps, you are ready to build and run your CESM case. Begin by navigating to the case root directory specified during the case creation. Before proceeding, review the user_nl_mom file located in the case directory. This file contains MOM6 parameter settings that were automatically generated by CrocoDash. Carefully examine these parameters and make any necessary adjustments to fine-tune the model for your specific requirements. While CrocoDash aims to provide a solid starting point, further tuning and adjustments are typically necessary to improve the model for your use case.\n\nOnce you have reviewed and modified the parameters as needed, you can prepare, build, and execute the case using the following commands:./case.setup\n./case.build\n./case.submit","type":"content","url":"/notebooks/use-cases/three-boundary#section-4-build-and-run-the-case","position":23},{"hierarchy":{"lvl1":"Make a lowres domain for DA dev work!"},"type":"lvl1","url":"/notebooks/use-cases/three-boundary-from-t232","position":0},{"hierarchy":{"lvl1":"Make a lowres domain for DA dev work!"},"content":"A typical workflow of utilizing CrocoDash consists of four main steps:\n\nGenerate a regional MOM6 domain.\n\nCreate the CESM case.\n\nPrepare ocean forcing data.\n\nBuild and run the case.\n\n","type":"content","url":"/notebooks/use-cases/three-boundary-from-t232","position":1},{"hierarchy":{"lvl1":"Make a lowres domain for DA dev work!","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl2","url":"/notebooks/use-cases/three-boundary-from-t232#section-1-generate-a-regional-mom6-domain","position":2},{"hierarchy":{"lvl1":"Make a lowres domain for DA dev work!","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"We begin by defining a regional MOM6 domain using CrocoDash. To do so, we first carve out a subdomain from a 1/12 degree global grid. We then generate the topography by remapping an existing bathymetric dataset to our horizontal grid. Finally, we define a vertical grid.\n\n","type":"content","url":"/notebooks/use-cases/three-boundary-from-t232#section-1-generate-a-regional-mom6-domain","position":3},{"hierarchy":{"lvl1":"Make a lowres domain for DA dev work!","lvl3":"Step 1.1: Horizontal Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl3","url":"/notebooks/use-cases/three-boundary-from-t232#step-1-1-horizontal-grid","position":4},{"hierarchy":{"lvl1":"Make a lowres domain for DA dev work!","lvl3":"Step 1.1: Horizontal Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"Extract a subgrid from a lowres global grid using the subgrid_from_supergrid method:\n\nfrom CrocoDash.grid import Grid\n\ngrid = Grid.subgrid_from_supergrid(\n    path = \"<HGRID_221123>\",  # supergrid\n    llc = (15, -99),  # (l)ower (l)eft (c)orner coords\n    urc = (30, -80.),  # (u)pper (r)ight (c)orner coords\n    name = \"little_nwa12\"\n)\n\n\n\n","type":"content","url":"/notebooks/use-cases/three-boundary-from-t232#step-1-1-horizontal-grid","position":5},{"hierarchy":{"lvl1":"Make a lowres domain for DA dev work!","lvl3":"Step 1.2: Topography","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl3","url":"/notebooks/use-cases/three-boundary-from-t232#step-1-2-topography","position":6},{"hierarchy":{"lvl1":"Make a lowres domain for DA dev work!","lvl3":"Step 1.2: Topography","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"\n\nfrom CrocoDash.topo import Topo\n\ntopo = Topo(\n    grid = grid,\n    min_depth = 9.5,\n)\n\n\n\nbathymetry_path='<GEBCO>'\n\ntopo.interpolate_from_file(\n    file_path = bathymetry_path,\n    longitude_coordinate_name=\"lon\",\n    latitude_coordinate_name=\"lat\",\n    vertical_coordinate_name=\"elevation\"\n)\n\n\n\ntopo.depth.plot()\n\n\n\n# Erase Pacific -> Go to Basinmask, Select basin you want to keep (Atlantic), press the erase disconnected basin button to erase the pacific\n%matplotlib ipympl\nfrom CrocoDash.topo_editor import TopoEditor\ntopo.depth[\"units\"] = \"m\" # Short term bug fix\nTopoEditor(topo)\n\n\n\n\n","type":"content","url":"/notebooks/use-cases/three-boundary-from-t232#step-1-2-topography","position":7},{"hierarchy":{"lvl1":"Make a lowres domain for DA dev work!","lvl3":"Step 1.3: Vertical Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"type":"lvl3","url":"/notebooks/use-cases/three-boundary-from-t232#step-1-3-vertical-grid","position":8},{"hierarchy":{"lvl1":"Make a lowres domain for DA dev work!","lvl3":"Step 1.3: Vertical Grid","lvl2":"SECTION 1: Generate a regional MOM6 domain"},"content":"\n\nfrom CrocoDash.vgrid import VGrid\n\nvgrid  = VGrid.hyperbolic(\n    nk = 75,\n    depth = topo.max_depth,\n    ratio=20.0\n)\n\n\n\n","type":"content","url":"/notebooks/use-cases/three-boundary-from-t232#step-1-3-vertical-grid","position":9},{"hierarchy":{"lvl1":"Make a lowres domain for DA dev work!","lvl2":"SECTION 2: Create the CESM case"},"type":"lvl2","url":"/notebooks/use-cases/three-boundary-from-t232#section-2-create-the-cesm-case","position":10},{"hierarchy":{"lvl1":"Make a lowres domain for DA dev work!","lvl2":"SECTION 2: Create the CESM case"},"content":"After generating the MOM6 domain, the next step is to create a CESM case using CrocoDash. This process is straightforward and involves instantiating the CrocoDash Case object. The Case object requires the following inputs:\n\nCESM Source Directory: A local path to a compatible CESM source copy.\n\nCase Name: A unique name for the CESM case.\n\nInput Directory: The directory where all necessary input files will be written.\n\nMOM6 Domain Objects: The horizontal grid, topography, and vertical grid created in the previous section.\n\nProject ID: (Optional) A project ID, if required by the machine.\n\nfrom pathlib import Path\n\n\n\n# CESM case (experiment) name\ncasename = \"little-nwa-three-boundary-t232\"\n\n# CESM source root (Update this path accordingly!!!)\ncesmroot = \"<CESM>\"\n\n# Place where all your input files go \ninputdir = Path.home()  / \"croc_input\" / casename\n    \n# CESM case directory\ncaseroot =  Path.home() / \"croc_cases\" / casename\n\n\n\n","type":"content","url":"/notebooks/use-cases/three-boundary-from-t232#section-2-create-the-cesm-case","position":11},{"hierarchy":{"lvl1":"Make a lowres domain for DA dev work!","lvl3":"Step 2.2: Create the Case","lvl2":"SECTION 2: Create the CESM case"},"type":"lvl3","url":"/notebooks/use-cases/three-boundary-from-t232#step-2-2-create-the-case","position":12},{"hierarchy":{"lvl1":"Make a lowres domain for DA dev work!","lvl3":"Step 2.2: Create the Case","lvl2":"SECTION 2: Create the CESM case"},"content":"To create the CESM case, instantiate the Case object as shown below. This will automatically set up the CESM case based on the provided inputs: The cesmroot argument specifies the path to your local CESM source directory.\nThe caseroot argument defines the directory where the case will be created. CrocoDash will handle all necessary namelist modifications and XML changes to align with the MOM6 domain objects generated earlier.\n\nfrom CrocoDash.case import Case\n\ncase = Case(\n    cesmroot = cesmroot,\n    caseroot = caseroot,\n    inputdir = inputdir,\n    ocn_grid = grid,\n    ocn_vgrid = vgrid,\n    ocn_topo = topo,\n    project = 'NCGD0011',\n    override = True,\n    ninst = 2,\n)\n\n\n\n","type":"content","url":"/notebooks/use-cases/three-boundary-from-t232#step-2-2-create-the-case","position":13},{"hierarchy":{"lvl1":"Make a lowres domain for DA dev work!","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl2","url":"/notebooks/use-cases/three-boundary-from-t232#section-3-prepare-ocean-forcing-data","position":14},{"hierarchy":{"lvl1":"Make a lowres domain for DA dev work!","lvl2":"Section 3: Prepare ocean forcing data"},"content":"We need to cut out our ocean forcing. The package expects an initial condition and one time-dependent segment per non-land boundary. Naming convention is \"east_unprocessed\" for segments and \"ic_unprocessed\" for the initial condition.\n\nIn this notebook, we are forcing with the Copernicus Marine ‚ÄúGlorys‚Äù reanalysis dataset. There‚Äôs a function in the CrocoDash package, called configure_forcings, that generates a bash script to download the correct boundary forcing files for your experiment. First, you will need to create an account with Copernicus, and then call copernicusmarine login to set up your login details on your machine. Then you can run the get_glorys_data.sh bash script.","type":"content","url":"/notebooks/use-cases/three-boundary-from-t232#section-3-prepare-ocean-forcing-data","position":15},{"hierarchy":{"lvl1":"Make a lowres domain for DA dev work!","lvl3":"Step 3.1 Configure Initial Conditions and Forcings","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl3","url":"/notebooks/use-cases/three-boundary-from-t232#step-3-1-configure-initial-conditions-and-forcings","position":16},{"hierarchy":{"lvl1":"Make a lowres domain for DA dev work!","lvl3":"Step 3.1 Configure Initial Conditions and Forcings","lvl2":"Section 3: Prepare ocean forcing data"},"content":"\n\n\ncase.configure_forcings(\n    date_range = [\"2020-01-01 00:00:00\", \"2020-02-01 00:00:00\"],\n    boundaries = [\"south\", \"north\",\"east\"], \n)\n\n\n\n","type":"content","url":"/notebooks/use-cases/three-boundary-from-t232#step-3-1-configure-initial-conditions-and-forcings","position":17},{"hierarchy":{"lvl1":"Make a lowres domain for DA dev work!","lvl3":"Step 3.2 Run get_glorys_data.sh","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl3","url":"/notebooks/use-cases/three-boundary-from-t232#step-3-2-run-get-glorys-data-sh","position":18},{"hierarchy":{"lvl1":"Make a lowres domain for DA dev work!","lvl3":"Step 3.2 Run get_glorys_data.sh","lvl2":"Section 3: Prepare ocean forcing data"},"content":"In a terminal session, locate the get_glorys_data.sh script and execute it to download the initial conditions and boundary conditions. Follow the instructions printed by the configure_forcings method above.\n\nTODO: user copernicusmarine python API within CrocoDash, instead of directing users to run it via CLI. Also, on a derecho login node, both CLI and API fails to run due to the computational demand. We also need to address that.\n\n","type":"content","url":"/notebooks/use-cases/three-boundary-from-t232#step-3-2-run-get-glorys-data-sh","position":19},{"hierarchy":{"lvl1":"Make a lowres domain for DA dev work!","lvl3":"Step 3.3: Process forcing data","lvl2":"Section 3: Prepare ocean forcing data"},"type":"lvl3","url":"/notebooks/use-cases/three-boundary-from-t232#step-3-3-process-forcing-data","position":20},{"hierarchy":{"lvl1":"Make a lowres domain for DA dev work!","lvl3":"Step 3.3: Process forcing data","lvl2":"Section 3: Prepare ocean forcing data"},"content":"In this final step, we call the process_forcings method of CrocoDash to cut out and interpolate the initial condition as well as all boundaries. CrocoDash also updates MOM6 runtime parameters and CESM xml variables accordingly.\n\ncase.process_forcings()\n\n\n\n# Because this case is so small, AFTER CREATING THE CASE, we change NTASKS to be 4 instead of the default of 128. \nfrom visualCaseGen.custom_widget_types.case_tools import xmlchange\nxmlchange(\"NTASKS\", \"4\")\n\n\n\n","type":"content","url":"/notebooks/use-cases/three-boundary-from-t232#step-3-3-process-forcing-data","position":21},{"hierarchy":{"lvl1":"Make a lowres domain for DA dev work!","lvl2":"Section 4: Build and run the case"},"type":"lvl2","url":"/notebooks/use-cases/three-boundary-from-t232#section-4-build-and-run-the-case","position":22},{"hierarchy":{"lvl1":"Make a lowres domain for DA dev work!","lvl2":"Section 4: Build and run the case"},"content":"After completing the previous steps, you are ready to build and run your CESM case. Begin by navigating to the case root directory specified during the case creation. Before proceeding, review the user_nl_mom file located in the case directory. This file contains MOM6 parameter settings that were automatically generated by CrocoDash. Carefully examine these parameters and make any necessary adjustments to fine-tune the model for your specific requirements. While CrocoDash aims to provide a solid starting point, further tuning and adjustments are typically necessary to improve the model for your use case.\n\nOnce you have reviewed and modified the parameters as needed, you can prepare, build, and execute the case using the following commands:./case.setup\n./case.build\n./case.submit","type":"content","url":"/notebooks/use-cases/three-boundary-from-t232#section-4-build-and-run-the-case","position":23}]}