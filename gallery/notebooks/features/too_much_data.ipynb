{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative OBC generation (for Long CESM-MOM6 runs) - Large Data Workflow\n",
    "\n",
    "Often, generating OBC datasets can be done all in one shot, but in longer and larger cases (like running the Northwest Atlantic for a year) we need to start iterating through the generation. Generating open boundary condition (OBC) data is essential for the entire model runtime but can be time-consuming and resource-intensive. \n",
    "\n",
    "The Large Data Workflow in CrocoDash helps manage this by breaking data access into smaller, more manageable components.\n",
    "# Large Data Workflow Overview\n",
    "\n",
    "The workflow is enabled by setting the `too_much_data` boolean in `case.configure_forcings`. This triggers the copying of a script folder into the case input directory forcing folder \n",
    "and the generation of a configuration file to download the required boundary condition files. An example of this is in CrocoGallery under features/add_data_products.ipynb.\n",
    "Users can trigger the workflow by running driver.py in the forcing folder and adjusting config options in the config file.\n",
    "## Folder Structure\n",
    "\n",
    "- **config.json** – Defines the region-specific requirements and run parameters.  \n",
    "- **README** – Explains the workflow.  \n",
    "- **driver.py** – Executes all scripts needed to obtain OBC data.  \n",
    "- **Code/** – Contains all scripts used in the workflow.  \n",
    "- **raw_data/**, **regridded_data/** – Intermediate storage for workflow steps, preventing the need to rerun all scripts at once.  \n",
    "\n",
    "## Scripts\n",
    "\n",
    "1. **get_data_piecewise** – Retrieves raw, unprocessed data in chunks (size defined by `config[\"params\"][\"step\"]`) and saves it to `config[\"raw_data\"]`.  \n",
    "2. **regrid_data_piecewise** – Processes raw data and stores it in `config[\"regridded_data\"]`.  \n",
    "3. **merge_piecewise_dataset** – Combines regridded data into the final dataset for model input.  \n",
    "\n",
    "## How to Use\n",
    "\n",
    "1. Identify and allocate available computing resources.  \n",
    "2. Adjust the `step` parameter to match resource constraints (default: 5 days).  \n",
    "3. Run each step manually or use `driver.py` as a guide.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Trigger Large Data Workflow\n",
    "This can be done by setting the too_much_data bool to true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case.configure_forcings(\n",
    "    date_range = [\"2020-01-01 00:00:00\", \"2020-01-09 00:00:00\"],\n",
    "    too_much_data = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Run the iterative OBC processor\n",
    "\n",
    "In a terminal session, locate the `large_data_workflow` folder, which is put in the case input directory under the forcing folder, default called \"glorys/large_data_workflow\". Then, execute the `driver.py` to generate boundary conditions. It uses the config.json file to generate the OBCs in a piecewise format before merging. Modify the code as you see fit!\n",
    "\n",
    "Especially consider adjusting the specific function being used in config.json to download the data. For example, On Derecho? Using the RDA reader. On a local computer? Use the python GLORYS api. You can change the function by changing the respective line in config.json.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Process forcing data\n",
    "\n",
    "In this final step, we call the `process_forcings` method of CrocoDash to interpolate the initial condition as well as all boundaries. CrocoDash also updates MOM6 runtime parameters and CESM xml variables accordingly. It will auto skip the OBCs because of the large data workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case.process_forcings()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CrocoDash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
